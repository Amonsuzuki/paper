\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{cite}
\usepackage{geometry}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{booktabs}
\geometry{margin=1in}

\title{Overcoming the Barrier of Orbital-Free Density Functional Theory for Molecular Systems Using Deep Learning}

\author{
He Zhang\textsuperscript{1,2$\dagger\sharp$}, 
Siyuan Liu\textsuperscript{2$\dagger\sharp$}, 
Jiacheng You\textsuperscript{2$\sharp$}, 
Chang Liu\textsuperscript{2*}, 
Shuxin Zheng\textsuperscript{2*},\\
Ziheng Lu\textsuperscript{2}, 
Tong Wang\textsuperscript{2}, 
Nanning Zheng\textsuperscript{1}, 
Bin Shao\textsuperscript{2*}\\
\\
\small \textsuperscript{1}National Key Laboratory of Human-Machine Hybrid Augmented Intelligence,\\
\small National Engineering Research Center for Visual Information and Applications,\\
\small and Institute of Artificial Intelligence and Robotics,\\
\small Xi'an Jiaotong University, Xi'an, China.\\
\small \textsuperscript{2}Microsoft Research AI4Science, Beijing, China.\\
\\
\small \textsuperscript{*}Corresponding authors. E-mails: \{changliu, shuz, binshao\}@microsoft.com\\
\small \textsuperscript{$\dagger$}These authors contributed equally.\\
\small \textsuperscript{$\sharp$}These authors did this work during an internship at Microsoft Research AI4Science.
}

\date{}

\begin{document}

\maketitle

\begin{abstract}
Orbital-free density functional theory (OFDFT) is a quantum chemistry formulation that has a
lower cost scaling than the prevailing Kohn-Sham DFT, which is increasingly desired for contemporary
molecular research. However, its accuracy is limited by the kinetic energy density functional, which is
notoriously hard to approximate for non-periodic molecular systems. Here we propose M-OFDFT, an
OFDFT approach capable of solving molecular systems using a deep learning functional model. We
build the essential non-locality into the model, which is made affordable by the concise density repre-
sentation as expansion coefficients under an atomic basis. With techniques to address unconventional
learning challenges therein, M-OFDFT achieves a comparable accuracy with Kohn-Sham DFT on a
wide range of molecules untouched by OFDFT before. More attractively, M-OFDFT extrapolates well
to molecules much larger than those seen in training, which unleashes the appealing scaling of OFDFT
for studying large molecules including proteins, representing an advancement of the accuracy-efficiency
trade-off frontier in quantum chemistry.
1 Introduction
Density functional theory (DFT) is a powerful quantum chemistry method for solving electronic states,
and hence the energy and properties of molecular systems. It is among the most popular choices owing
to its appropriate accuracy-efficiency trade-off, and has fostered many scientific discoveries [1, 2]. For
solving a system with Nelectrons, the prevailing Kohn-Sham formulation (KSDFT) [3] minimizes the
electronic energy as a functional of Norbital functions {Ï•i(r)}N
i=1, where Ï•idenotes the i-th orbital,
which is a function of the coordinates rof an electron. Although the orbitals allow explicitly calculating
the non-interacting part of kinetic energy, optimizing Nfunctions deviates from the original idea of
DFT [4â€“7] to optimize one function, the (one-body reduced) electron density Ï(r), and hence immediately
increases the cost scaling by an order of N(Fig. 1(a)). This higher complexity is increasingly undesired
for the current research stage where large-scale system simulations for practical applications are in high
demand. For this reason, there is a growing interest in methods following the original spirit of DFT, now
called orbital-free DFT (OFDFT) [8â€“10].
The central task in OFDFT is to approximate the non-interacting part of kinetic energy as a density func-
tional (KEDF), which is denoted as TS[Ï]. Classical approximations are developed based on the uniform
electron gas theory [11â€“15], and have achieved many successes for periodic material systems [16, 17].
But the accuracy is still limited for molecules [18â€“20], mainly because the electron density in molecules
is far from uniform.
For approximating a complicated functional, recent triumphant progress in machine learning creates
new opportunities. By leveraging labeled data, the theoretical mismatch can be compensated. Pio-
neering works [21â€“23] use kernel ridge regression to approximate KEDF and have shown success on
1-dimensional systems. Deep learning models have been recently explored for a broader applicability.
Some works learn to output point-wise kinetic energy density from electron density features [24â€“26] and
Published in Nature Computational Science, 2024. DOI: https://doi.org/10.1038/s43588-024-00605-8
1arXiv:2309.16578v2  [stat.ML]  10 Mar 2024
\end{abstract}


1 Introduction
Density functional theory (DFT) is a powerful quantum chemistry method for solving electronic states,
and hence the energy and properties of molecular systems. It is among the most popular choices owing
to its appropriate accuracy-efficiency trade-off, and has fostered many scientific discoveries [1, 2]. For
solving a system with Nelectrons, the prevailing Kohn-Sham formulation (KSDFT) [3] minimizes the
electronic energy as a functional of Norbital functions {Ï•i(r)}N
i=1, where Ï•idenotes the i-th orbital,
which is a function of the coordinates rof an electron. Although the orbitals allow explicitly calculating
the non-interacting part of kinetic energy, optimizing Nfunctions deviates from the original idea of
DFT [4â€“7] to optimize one function, the (one-body reduced) electron density Ï(r), and hence immediately
increases the cost scaling by an order of N(Fig. 1(a)). This higher complexity is increasingly undesired
for the current research stage where large-scale system simulations for practical applications are in high
demand. For this reason, there is a growing interest in methods following the original spirit of DFT, now
called orbital-free DFT (OFDFT) [8â€“10].
The central task in OFDFT is to approximate the non-interacting part of kinetic energy as a density func-
tional (KEDF), which is denoted as TS[Ï]. Classical approximations are developed based on the uniform
electron gas theory [11â€“15], and have achieved many successes for periodic material systems [16, 17].
But the accuracy is still limited for molecules [18â€“20], mainly because the electron density in molecules
is far from uniform.
For approximating a complicated functional, recent triumphant progress in machine learning creates
new opportunities. By leveraging labeled data, the theoretical mismatch can be compensated. Pio-
neering works [21â€“23] use kernel ridge regression to approximate KEDF and have shown success on
1-dimensional systems. Deep learning models have been recently explored for a broader applicability.
Some works learn to output point-wise kinetic energy density from electron density features [24â€“26] and
Published in Nature Computational Science, 2024. DOI: https://doi.org/10.1038/s43588-024-00605-8
1arXiv:2309.16578v2  [stat.ML]  10 Mar 2024

b
Ã—ğ¿ğ‘‡S,ğœƒGraphormer
ğ¡(2) 
ğ¡(ğ´) â‹® â‹®ğ¡(1)  ğ©
  ğ“œ
 
ğœŒğ« 
=Ïƒğ‘,ğœğ©ğ‘,ğœğœ”ğ‘,ğœğ« 
ğœ”2,2 ğœ”2,1 â‹®ğ©2,1 ğ©2,2 â‹¯ ğ©2,ğ’¯ ğ±(2),ğ‘(2)ğ©1,1 ğ©1,2 â‹¯ ğ©1,ğ’¯ ğ±(1),ğ‘(1)
ğ©ğ´,1 ğ©ğ´,2 â‹¯ ğ©ğ´,ğ’¯ ğ±(ğ´),ğ‘(ğ´)a
â‹¯ c
min
Î¦Ïƒğ‘–ğœ™ğ‘–à· ğ‘‡ğœ™ğ‘–+ğ¸HğœŒÎ¦+ğ¸XCğœŒÎ¦+ğ¸ext[ğœŒÎ¦] KSDFT: ğ‘‚ğ‘3 
OFDFT: ğ‘‚ğ‘2 
min
ğœŒ ğ‘‡SğœŒ+ğ¸HğœŒ+ğ¸XCğœŒ+ğ¸extğœŒ ğ¸â‹†
ğœŒâ‹†
ğŸ
â‹® 
ğœŒ: ğ«
âˆ‡ğ©ğ¸ğœƒ(ğ©ğ‘˜,ğ“œ) ğ©ğ‘˜ 
ğ©ğ‘˜+1 
ğ¸ğœƒğ©,ğ“œ â‰” 
ğ‘‡S,ğœƒğ©,ğ“œ  + ğ¸Hğ©,ğ“œ +ğ¸XCğ©,ğ“œ +ğ¸ext(ğ©,ğ“œ)ğ«â‹® ğœ™1 ğ«
ğ«
ğœ™ğ‘ 
Î¦: 
ğœ™2 
C
OH
ğ¡(1) 
ğ¡(2) 
ğ¡(ğ´) Figure 1: Overview of M-OFDFT. (a) KSDFT solves the properties (for example, the ground-state elec-
tron density Ïâ‹†, the energy Eâ‹†, and the force f) of a molecular structure M:={(x(a), Z(a))}A
a=1with
Nelectrons, where x(a)andZ(a)denote the coordinates and atomic number of the a-th atom out of a
total of Aatoms in the molecule, by optimizing Norbital functions Î¦ :={Ï•i(r)}N
i=1, where Ï•idenotes
thei-th orbital which is a function of the coordinates rof an electron, so that the kinetic energy can be
evaluated directly ( Ë†Tis the kinetic-energy operator). In contrast, OFDFT only needs to optimize one
density function Ï(r)if the kinetic energy density functional (KEDF) TS[Ï]is available, which reduces
the complexity by an order of N.(b)The proposed M-OFDFT uses a deep learning model TS,Î¸(p,M)(Î¸
denotes learnable parameters) to approximate KEDF, which is learned from data. The model incorporates
non-local interaction of density over the space, which is made affordable by inputting a concise represen-
tation of the density (gray shaded region around the molecule): the expansion coefficients pon an atomic
basis{Ï‰Âµ(r)}Âµ, where Ï‰Âµ(r)is the Âµ-th basis function and the index Âµ= (a, Ï„)is composed of the center
atom index aand the pattern index Ï„(for example, the blue and red spheres located bottom-left illustrate
two basis functions of two patterns centered at atom 2 (the carbon)). The coefficients are correspondingly
distributed over the atoms. Non-locality is captured by the attention mechanism which updates features
on one atom by calculation with features on all other atoms, including distant ones (for example, the solid
blue lines represent the update of features h(2)of atom 2 incorporates features on all other atoms). After
updates by Llayers, the final scalar features over atoms are summed up to produce the kinetic energy
value. (c)M-OFDFT solves a molecular structure Mby optimizing the density coefficients pto mini-
mize the electronic energy EÎ¸(p,M), which is constructed by the learned KEDF model and three other
terms that can be directly evaluated. The red and blue hues represent values of electronic energy.
hence are (semi-)local. Others account for the non-local interaction of density at different points [27â€“29].
They enable 3-dimensional calculations, yet are still limited to dozen atoms. Supplementary Section C
gives more details. Moreover, few studies have shown accuracy on molecules much larger than those in
training data. However, such an extrapolation study is imperative to demonstrate the dominating value of
the scaling advantage of an OFDFT method, since molecules in a similar scale as training molecules are
already affordable by the data-generating methods.
In this work, we develop an OFDFT method called M-OFDFT that can handle common Molecules us-
ing a deep learning KEDF model. We attribute the limited applicability of previous works on molecular
systems to the grid-based representation of density as the model input, which is not sufficiently effi-
cient to represent the uneven density in molecules. Even an irregular grid requires unaffordably many
points ( âˆ¼104N) for a non-local calculation, while the non-locality has been found indispensable for ap-
proximating KEDF [30, 14, 8, 31] (Supplementary Section D.4.2); hence, a stringent accuracy-efficiency
trade-off is raised. To afford non-local calculation for approximating KEDF, we adopt an atomic basis
set{Ï‰Âµ(r)}M
Âµ=1, where Ï‰Âµ(r)represents the Âµ-th basis function and Mis the number of basis functions,
to expand the density as Ï(r) =P
ÂµpÂµÏ‰Âµ(r), and take the coefficients pas the model input (Fig. 1(b)).
Each basis function Ï‰Âµ=(a,Ï„)(r)depicts a function pattern Ï„around an atom a, which aligns with the
2

pattern that the electron density in the molecule distributes around atoms. They are even designed to
mimic the nuclear cusp condition [32] for sculpting the sharp density change near a nucleus. Such an
alignment makes an efficient representation of density: typically âˆ¼10Nbasis functions are sufficient,
leading to thousands times fewer dimensions than a grid-based representation, which is especially desired
in a non-local calculation. Moreover, atomic basis functions form a shell structure, facilitating an OFDFT
method to overcome the challenge of recovering the shell structure of density in molecules.
Under this representation, the KEDF model follows the form TS,Î¸(p,M), where Î¸denotes learnable
parameters, and M:={(x(a), Z(a))}A
a=1denotes the molecular structure, comprising the coordinates
and atomic numbers of all atoms, of the target system, which is required in the input for specifying the
locations and types of basis functions. As the coefficients pcan be distributed over atoms according to
the center of the corresponding basis function, the input is a set of nodes each with a location, a type,
and coefficient features, representing the electron density in its locality (Fig. 1(b)). To process such input,
we build a deep learning model based on Graphormer [33, 34], a variant of the Transformer model [35].
It iteratively processes features on all nodes, and adds up the final features over the nodes as the kinetic
energy output. Non-locality is covered by the attention mechanism, which updates features on a node
by first calculating a weight (â€œattentionâ€) for the interaction with every other node using features on the
two nodes and their distance, then adding the features on every other node, each with the above calculated
weight, to the features on this node (Fig. 1(b)). This process accounts for the interaction of density features
in distant localities, hence non-local effect is captured. Details on the model architecture are provided in
Supplementary Section B. After the KEDF model is learned, M-OFDFT solves a given molecular system
by optimizing the density coefficients to minimize the electronic energy as the objective, where the KEDF
model is used to construct the energy (Fig. 1(c)). The optimization result gives ground-state properties
such as energy and electron density. We note that our formulation of KEDF model resembles neural
network potentials (NNPs) [36â€“39], which predicts the ground-state energy from Mend-to-end. Without
the need to optimize density, they predict energy faster, but do not describe electronic state. The M-
OFDFT formulation also exhibits better extrapolation (Results 2.3).
Perhaps unexpectedly, learning a KEDF model is more challenging beyond conventional machine learn-
ing. Since the model is used as an optimization objective, it needs to capture the energy landscape over
the coefficient space for each molecular structure, for which only one datapoint per molecular structure is
far from sufficient. We hence design methods to produce multiple coefficient datapoints, each also with
agradient (with respect to the coefficients) label, for each molecular structure (Methods 4.1). Moreover,
the input coefficients are tensors equivariant to the rotation of the molecule, but the output energy is in-
variant. We employ local frames to guarantee this geometric invariance (Methods 4.2). Finally, the model
needs to fit a physical mechanism which may vary steeply with the input. For expressing large gradients,
we introduce a series of enhancement modules that balances the sensitivity over coefficient dimensions,
rescales the gradient dimension-wise, and offsets the gradient with a reference (Methods 4.3).
We demonstrate the practical utility and advantage in the following aspects. (1)M-OFDFT achieves
chemical accuracy compared to KSDFT on a range of molecular systems in similar scales as those in
training. This is hundreds times more accurate than classical OFDFT. The optimized density shows a clear
shell structure, which is regarded as challenging for an orbital-free approach. (2)M-OFDFT achieves an
attractive extrapolation capability that its per-atom error stays constant or even decreases on increasingly
larger molecules all the way to 10 times (224 atoms) beyond those in training. The absolute error is still
much smaller than classical OFDFT. In contrast, the per-atom error keeps increasing by NNP variants.
M-OFDFT also improves more efficiently on limited data at large scale. (3)With the accuracy and
extrapolation capability, M-OFDFT unleashes the scaling advantage of OFDFT to large-scale molecular
systems. We find that its empirical time complexity is O(N1.46), indeed lower by order- NthanO(N2.49)
of KSDFT. The absolute time is always shorter, achieving a 27.4-fold speedup on the protein B system
(738 atoms). In all, M-OFDFT improves the accuracy-efficiency trade-off frontier in quantum chemistry,
pushing the applicability of OFDFT for solving large-scale molecular science problems.
2 Results
2.1 Workflow of M-OFDFT
OFDFT solves the electronic state of a molecular structure Mby minimizing the electronic energy
as a functional of the electron density Ï, which is decomposed in the same way as KSDFT: E[Ï] =
TS[Ï] +EH[Ï] +EXC[Ï] +Eext[Ï](Supplementary Section A.1), where the classical internal (Hartree)
and external potential energies, EH[Ï]andEext[Ï], have exact expressions, and the exchange-correlation
(XC) functional EXC[Ï]already has accurate approximations. In M-OFDFT, the kinetic energy density
functional (KEDF) TS[Ï]is approximated by the TS,Î¸(p,M)model under the atomic-basis representation
3

introduced above. After the TS,Î¸(p,M)model is learned (Methods 4.1), M-OFDFT solves the electronic
state of a given molecular structure Mthrough the density optimization procedure (Fig. 1(c)), which
minimizes the electronic energy EÎ¸(p,M):
min
p:pâŠ¤w=NEÎ¸(p,M) :=TS,Î¸(p,M) +EH(p,M) +EXC(p,M) +Eext(p,M), (1)
where EH,EextandEXCcan be computed from (p,M)in a conventional way (Supplementary Sec-
tion A.3.2). The constraint on pfulfills a normalized density, where wÂµ:=R
Ï‰Âµ(r) dris the basis
normalization vector. The optimization is solved by gradient descent:
p(k+1):=p(k)âˆ’Îµ
Iâˆ’wwâŠ¤
wâŠ¤w
âˆ‡pEÎ¸(p(k),M), (2)
where Îµis a step size, and the gradient is projected onto the admissible plane in respect to the linear
constraint. Notably, due to directly operating on density, the complexity of M-OFDFT in each iteration
isO(N2)(Supplementary Section A.3.2), which is order- Nless than that O(N3)(with density fitting;
Supplementary Section A.3.1) of KSDFT.
2.2 Performance of M-OFDFT on Molecular Systems
We first evaluate the performance of M-OFDFT on molecules in similar scales but unseen in training. We
generate datasets based on two settings: ethanol structures from the MD17 dataset [40, 41] for studying
conformational space generalization, and molecular structures from the QM9 dataset [42, 43] for studying
chemical space generalization. Each dataset is split into three parts for the training and validation of the
KEDF model, and the test of M-OFDFT. For ease of training, we use the asymptotic PBE-like (APBE)
functional [44] as a base KEDF and let the deep learning model learn the residual (Supplementary Sec-
tion B.4.1).
We evaluate M-OFDFT in terms of the mean absolute error (MAE) from KSDFT results in energy, as well
as in the Hellmann-Feynman (HF) force (Supplementary Section A.5). The results are 0.18 kcal/mol
and 1.18 kcal/mol/ËšA on ethanols, and 0.93 kcal/mol and 2.91 kcal/mol/ËšA on QM9 (Supplementary
Section D.1.1 shows more results). We see M-OFDFT achieves chemical accuracy (1 kcal/mol energy
MAE) in both cases.
To show the advantage of this result, we compare M-OFDFT with classical OFDFT using well-established
KEDFs, including the Thomas-Fermi (TF) KEDF [4, 5] which is exact in the uniform electron gas limit,
its corrections TF+1
9vW (ref. [12]) and TF+vW (ref. [45]) with the von Weizs Â¨acker (vW) KEDF [46], and
the base KEDF APBE (Methods 4.7). We note that different KEDFs may have different absolute energy
biases, so for the energy error we compare the MAE in relative energy. On ethanol structures, the relative
energy is taken with respect to the energy on the equilibrium conformation. On QM9, as each molecule
only has one conformation, we evaluate the relative energy between every pair from the 6,095 isomers of
C7H10O2in the QM9 dataset. These isomers can be seen as different conformations of the same set of
atoms [36]. As shown in Fig. 2(a), M-OFDFT still achieves chemical accuracy on relative energy, and is
two orders more accurate than classical OFDFT.
As a qualitative investigation of M-OFDFT, we visualize the density on a test ethanol structure optimized
by these methods in Fig. 2(b) (Supplementary Section D.1.2 shows more). Radial density by spherical
integral around the oxygen atom is plotted. We find that the M-OFDFT curves coincide with the KS-
DFT curve precisely. Particularly, the two major peaks around 0 ËšA and 1.4 ËšA correspond to the density
of core electrons of the oxygen atom and the bonded carbon atom, while the minor peak in between
reflects the density of electrons in the covalent bonds with the hydrogen atom and the carbon atom. M-
OFDFT successfully recovers this shell structure, which is deemed difficult for OFDFT. In comparison,
the classical OFDFT using the APBE KEDF does not align well with the true density around the covalent
bonds. We further assess the density numerically by Hirshfeld partial charges [47] (Supplementary Sec-
tion D.1.2 presents a visualization) and dipole moment, on which the MAEs of M-OFDFT results on test
ethanol structures are 1.92 Ã—10âˆ’3eand 0.0180 D, which are substantially better than the results 0.155 e
and 0.985 Dof the classical TF+1
9vW OFDFT. These results suggest that M-OFDFT is a working OFDFT
for molecular systems.
To further demonstrate the utility of M-OFDFT, we investigate its potential energy surface (PES). Fig. 2(c)
shows the PES on ethanol over two coordinates: the torsion angle along the H CCO bond and the
OH bond length. We see both curves are sufficiently smooth, and stay closely (within chemical ac-
curacy) with KSDFT results. In comparison, the classical APBE OFDFT fails to maintain chemical
accuracy, and does not produce the correct energy barrier and equilibrium bond length. Supplementary
Section D.1.1 presents more details. We also verify the effectiveness of M-OFDFT for geometry opti-
mization in Supplementary Section D.1.1.
4

a
b
c0.0 0.5 1.0 1.5 2.0
C O
Distance from nucleus (Ã…)01020304050Radial density (Ã…1)KSDFT
APBE
M-OFDFT103104
Ethanol QM9 C7H10O2isomer02040Relative energy MAE (kcal/mol)
Ethanol QM9 C7H10O2isomer020406080HF force MAE (kcal/mol/Ã…)
M-OFDFT APBE TF TF+vW TF+1
9vW
180 120 60 0 60 120
Torsion Angles (Degree)101234Relative energy (kcal/mol)
0.85 0.90 0.95 1.00 1.05 1.10 1.15
O-H Bond Length (Ã…)60402002040Relative energy (kcal/mol)
KSDFT M-OFDFT APBEC
OH
Figure 2: Results of M-OFDFT compared with classical OFDFT on molecular systems. (a) Relative
energy (left) and Hellmann-Feynman (HF) force (right) results in terms of the MAE from KSDFT, with
error bars showing 95\% confidence intervals. Results for ethanol are statistics over 10,000 test structures,
and results for QM9 C7H10O2isomer are statistics over 619 test isomers. (b)Visualization of optimized
density. Each curve plots the integrated density on spheres with varying radii centered at the oxygen
atom in an ethanol structure. (c)PES study on ethanol. (left) PES over various torsion angles along the
HCCO bond; (right) PES over various O H bond lengths. The shaded region denotes the range
within chemical accuracy (1 kcal/mol) with respect to KSDFT.
2.3 Extrapolation of M-OFDFT to Larger-Scale Molecules
To wield the advantage of the lower cost scaling of M-OFDFT for a more meaningful impact, we evaluate
its accuracy on molecular systems with a scale beyond affordable for generating abundant training data.
For running on large molecules, we train the deep learning model targeting the sum of the kinetic and
XC energy to get rid of the demanding calculation on grid (Supplementary Sections B.4.2 and D.3). This
modification does not lead to obvious accuracy lost (Supplementary Section D.1.1).
5

Pretrain FromScratch Finetune
M-OFDFT0.000.020.040.060.080.100.12Per-atom energy MAE (kcal/mol)
-35.4\%
Pretrain FromScratch Finetune
M-NNP-Den
-16.3\%
Pretrain FromScratch Finetune
M-NNP
-2.8\%
M-OFDFT M-NNP-Den M-NNP APBE
Method0.10.20.30.40.6Relative per-atom energy MAE 
(kcal/mol)
2
3
4
5
Maximum length of peptides in training
0.10
0.15
0.20
0.25
0.30Per-atom energy MAE (kcal/mol)
M-NNP
M-NNP-Den
M-OFDFT
16
26
36
46
56
66
76
86
96
Number of heavy atoms in test
0.0
0.5
1.0
1.5
2.0Per-atom energy MAE (kcal/mol)
(0.709N heavy 12.666)0.1920.219
(0.709N heavy 12.387)0.2350.441
(0.474N heavy + 79.855)0.110.492
Trained with 15 heavy atoms
15
20
25
30
Largest number of heavy atoms in training
0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75Per-atom energy MAE (kcal/mol)
Tested with 56-60 heavy atoms
M-NNP M-NNP-Den M-OFDFTa
c
eb
dFigure 3: Extrapolation performance of M-OFDFT compared with other deep learning methods.
Considered are M-NNP and M-NNP-Den, which use deep learning models to predict the ground-state
energy end-to-end. The shades and error bars show 95\% confidence intervals. (a)MAE of per-atom en-
ergy on increasingly larger molecules from the QMugs dataset, using models trained on molecules with
no more than 15 heavy atoms from QM9 and QMugs datasets. Each value is calculated on 50 QMugs
molecules. (b)Energy error on QMugs test molecules ( n=50) with 56-60 heavy atoms, using models
trained on a series of datasets containing increasingly larger QMugs molecules up to 30 heavy atoms. The
horizontal dashed black line marks the performance of M-OFDFT trained on the first dataset. (c)Rel-
ative energy error on chignolin structures ( n=50), using models trained on all peptides (lengths 2-5).
Also shown is the result of the classical OFDFT using APBE. (d)Energy error on chignolin structures
(n=1,000), using models trained on a series of datasets including increasingly longer peptides. (e)Energy
error on chignolin structures ( n=50), using models trained on all peptides without (â€˜Pretrainâ€™) and with
(â€˜Finetuneâ€™) finetuning on 500 chignolin structures. Also marked are error reduction ratios by the fine-
tuned models over models trained from scratch (â€˜FromScratchâ€™) on the 500 chignolin structures only.
To evaluate the extrapolation performance, we compare M-OFDFT with a neural-network-potential coun-
terpart, dubbed M-NNP. It is a natural deep learning variant, which directly predicts the ground-state
energy from M. We also consider a variant dubbed M-NNP-Den that additionally takes the MINimal
Atomic Orbitals (MINAO) [48] initialized density into input for investigating the effect of density feature
on extrapolation. Both variants use the same non-local architecture and training settings as M-OFDFT
for fair comparison (Methods 4.8). Supplementary Section D.2.1 presents comparisons with more recent
NNP architectures, ET [38] and Equiformer [39], which suggest the same conclusion.
6

QMugs We first study the extrapolation on the QMugs dataset [49], containing much larger molecules
than those in QM9 which have no more than 9 heavy atoms. We train the models on QM9 together with
QMugs molecules with no more than 15 heavy atoms, and test the methods on larger QMugs molecules up
to 101 heavy atoms, which are grouped according to the number of heavy atoms into bins of width 5, and
are randomly subsampled to ensure the same number (50) of molecular structures in each bin to eliminate
statistical effects.
The result is shown in Fig. 3(a). We see that the per-atom MAE of M-OFDFT is always orders smaller
than M-NNP and M-NNP-Den in absolute value, even though M-NNP and M-NNP-Den achieve a lower
validation error (Supplementary Table S6). More attractively, the error of M-OFDFT keeps constant and
even decreases (note the negative exponent) when the molecule scale increases, while the errors of M-
NNP and M-NNP-Den keep increasing, even though they use the same non-local architecture capable
of capturing long-range effects, and M-NNP-Den also has a density input. We attribute the qualitatively
better extrapolation to appropriately formulating the machine-learning task. The ground-state energy of a
molecular structure is the result of an intricate, many-body interaction among electrons and nuclei, leading
to a highly challenging function to extrapolate from one region to another. M-OFDFT converts the task
into learning the objective function for the target output. The objective needs to capture only the mech-
anism through which the particles interact, which has a reduced level of complexity, while transferring a
large portion of complexity to the optimization process, which optimization tools can handle effectively
without an extrapolation issue. Similar phenomena have also been observed recently in machine learning
that learning an objective shows better extrapolation than learning an end-to-end map [50, 51].
To further substantiate the extrapolation capability of M-OFDFT, we investigate the magnitude by which
the training molecule scale must be increased for M-NNP and M-NNP-Den to achieve the same level of
performance as M-OFDFT on a given workload of large-scale molecules. We take 50 QMugs molecules
with 50-60 heavy atoms as the extrapolation benchmark, and train the models on a series of equal-sized
datasets that include increasingly larger molecules up to 30 heavy atoms. As shown in Fig. 3(b), M-NNP
and M-NNP-Den require at least twice as large molecules in the training dataset (30 vs.15 heavy atoms)
to achieve a commensurate accuracy (0.068 kcal/mol) as M-OFDFT provides. These extrapolation re-
sults suggest that M-OFDFT can be applied to systems much larger than training to exploit the scaling
advantage, and is more affordable to develop for solving large-scale molecular systems.
Chignolin An increasingly important portion of the demand for large-scale quantum chemistry calcu-
lation comes from biomolecular systems, particularly proteins, which are not touched by OFDFT previ-
ously. We assess the capability of M-OFDFT for protein systems on the chignolin protein (10 residues,
168 atoms after neutralization). We consider the common set-up where it is unaffordable to generate
abundant data for the large target system and hence requires extrapolation. We generate training data on
smaller-scale systems of short peptide structures containing 2 to 5 residues, cropped from 1,000 chignolin
structures selected from ref. [52]. To account for non-covalent effects, non-consecutive fragments are also
used in training, including systems of two dipeptides and systems of one dipeptide and one tripeptide. See
more details in Methods 4.5.4. For this task, we let the model target the total energy for a learning stability
consideration (Supplementary Section B.4.3).
We first train the model on all available peptides, and compare the relative energy error on chignolin
with other methods in Fig. 3(c). Notably, M-OFDFT achieves a substantially lower per-atom error than
the classical OFDFT using the APBE KEDF (0.098 kcal/molvs.0.684 kcal/mol), providing an effective
OFDFT method for biomolecular systems. M-OFDFT also outperforms deep learning variants M-NNP
and M-NNP-Den, indicating a better extrapolation capability. To investigate extrapolation in more detail,
we train the deep learning models on peptides with increasingly larger scale and plot the error on chignolin
in Fig. 3(d) (similar to the setting of Fig. 3(b)). Remarkably, M-OFDFT consistently outperforms end-
to-end energy prediction methods M-NNP and M-NNP-Den across all lengths of training peptides, and
halves the required length for the same level of accuracy. We note the spikes of M-NNP and M-NNP-Den
at peptide length 3 despite extensive hyperparameter tuning, possibly due to that their harder extrapolation
difficulty magnifies the gap between in-scale validation and larger-scale performance in this case.
After being trained on data in accessible scale, which is called â€œpretrainingâ€ in the following context,
a deep learning model for a larger-scale workload can be further improved if a few larger-scale data are
available for finetuning. In this situation, a method capable of good extrapolation could be roughly aligned
with the larger-scale task in advance using accessible data, more efficiently leveraging the limited larger-
scale data, and outperforming the model trained from scratch on these limited data only. To investigate the
benefit of M-OFDFT in this scenario, we build a finetuning dataset on 500 chignolin structures. Results
in Fig. 3(e) show that M-OFDFT achieves the most gain from pretraining, reducing the energy error
by 35.4\% over training from scratch, showing the appeal of extracting a more generalizable rule from
accessible-scale data. With finetuning, M-OFDFT still gives the best absolute accuracy. These results
7

110 150 190 230 270 310 350 390 430 470 510 550 590 630 670
Number of electrons02505007501000Time (s)KSDFT: (0.0240Nelec+0.69)2.49+9.81
M-OFDFT: (0.0701Nelec7.71)1.46+7.21
KSDFT
M-OFDFTFigure 4: Empirical time cost of M-OFDFT compared with KSDFT on molecules ( n=808) at various
scales. Each plotted value is the average of running times on molecules whose number of electrons falls
in the corresponding bin of width 20.
suggest that M-OFDFT could effectively handle as large a molecular system as a protein, even without
abundant training data on the same large scale.
2.4 Empirical Time Complexity of M-OFDFT
After validating the accuracy and extrapolation capability, we now demonstrate the scaling advantage of
M-OFDFT empirically. The time cost for running both methods on increasingly larger molecules from
the QMugs dataset [49] is plotted in Fig. 4. M-OFDFT calculations are run on a 32-core CPU server with
216 GiB memory and one Nvidia A100 GPU with 80 GiB memory, and KSDFT calculations on servers
with 256 GiB memory and 32 Intel Xeon Platinum 8272CL cores with hyperthreading disabled. We see
the absolute running time of M-OFDFT is always shorter than that of KSDFT, achieving up to 6.7-fold
speedup. The empirical complexity of M-OFDFT is O(N1.46), which is indeed at least order- Nless than
the empirical complexity O(N2.49)of KSDFT. Supplementary Section D.3 details the running setup and
per-component cost.
To further wield the advantage, we run M-OFDFT on two molecular systems as large as proteins: (1)
the peripheral subunit-binding domain BBL-H142W (PDB ID: 2WXC ) [53] containing 2,676 electrons
(709 atoms), and (2)the K5I/K39V double mutant of the Albumin binding domain of protein B (PDB
ID:1PRB ) [54] containing 2,750 electrons (738 atoms). Such a scale exceeds the typical workload of
KSDFT [55]. M-OFDFT costs 0.41 h and 0.45 h on the two systems, while using KSDFT costs 10.5 h
and 12.3 h; hence, a 25.6-fold and 27.4-fold speedup is achieved. Supplementary Section D.3 provides
more details.
3 Discussion
We have developed M-OFDFT, a deep learning implementation of orbital-free density functional theory
that works accurately on molecules while maintaining the appealing low cost scaling, hence providing
a powerful tool for exploring complex molecular systems with a higher level of detail and scale. M-
OFDFT represents an attempt to leverage deep learning to overcome longstanding challenges and initiate
resurgence of alternative quantum chemistry formulations, demonstrating the potential to improve the
accuracy-efficiency trade-off.
The presented study is focused on neutral molecules without spin polarization, but the methodology is
not limited to such systems. For example, since the formulation only requires the description of electron
density, it can also support charged and open-shell systems. Supplementary Section D.2.2 shows a pre-
liminary demonstration for charged molecules, and more investigation could be conducted in the future.
The formulation can also be applied to material systems, for which atomic basis is also an effective choice
for representing electronic states [56, 57]. Demonstration for a broader interest in the method includes
running molecular dynamics simulation and constructing quantum embedding [58, 59], which could be
elaborated in the future.
Even though M-OFDFT has demonstrated improved extrapolation by choosing an appropriate formula-
tion to leverage deep learning models for molecular science, extrapolation remains an obstacle to the
universal application of M-OFDFT. Future exploration to improve extrapolation may include leveraging
8

analytical properties of the KEDF. Supplementary Section A.6 discusses the possibility to leverage the
scaling property, and Supplementary Section B.4.1 for leveraging a lower bound. From the deep learn-
ing perspective, it is possible to improve extrapolation using more data and larger model with proper
architecture, as suggested by recent progress in large language model [60, 61] and an application of the
Graphormer architecture [62]. Considering the higher cost of obtaining data compared with conventional
deep learning, active learning could be leveraged to collect more informative data, which can be identified
by, for example, a large disagreement among an ensemble of models [63], or a large (relative) variance es-
timation for the model prediction [64]. These possibilities give room to further increase the applicability
of M-OFDFT.
4 Methods
As designing and learning the KEDF model are more challenging than conventional (deep) machine learn-
ing tasks, we describe methodological details for KEDF model training (Methods 4.1), additional design
for geometric invariance (Methods 4.2) and large gradient capacity (Methods 4.3), and density optimiza-
tion strategies (Methods 4.4) of M-OFDFT. For the empirical evaluation, we provide details for dataset
preparation (Methods 4.5) and generation (Methods 4.6), implementation of classical OFDFT methods
(Methods 4.7) and M-NNP/M-NNP-Den variants (Methods 4.8), and method for fitting the curves in
Fig. 3(a) and Fig. 4 (Methods 4.9).
We first summarize and discuss our major technological innovations here. Instead of a grid-based repre-
sentation, we used coefficients on atomic basis as input density feature, whose much lower dimensionality
allows a non-local architecture for accuracy and extrapolation. We also introduce local frames (Meth-
ods 4.2) to guarantee the invariance with respect to the rotation of the molecule. Some works (for example,
refs. [65, 66]) on learning the XC functional also adopt the coefficient input, but without the molecular
structure input, and hence cannot properly capture inter-atomic density feature interaction. Regarding the
additional challenge for learning an objective, we generated multiple datapoints each also with a gradient
label for each molecular structure to train the model (Methods 4.1). Although the possibility has been
noted by previous works (for example, refs. [21, 67]), none has fully leveraged such abundant data for
training (some only incorporated gradient [28, 68, 26, 69]; ref. [29] also produced multiple datapoints
but by perturbing the external potential). There are other ways to regularize the optimization behavior of
a functional model [70â€“72, 66], but our trials in Supplementary Section D.4.4 show that they are not as
effective. To express intrinsically large gradient, we introduce enhancement modules (Methods 4.3) in
addition to a conventional neural network. With these techniques, M-OFDFT achieves a stable density
optimization process, which is regarded as challenging using a deep learning KEDF model. Some previ-
ous deep learning KEDFs [27, 24, 25] do not support density optimization, and some of the others require
projection onto the training-data manifold in each step [21, 67, 23, 28]. M-OFDFT achieves stable den-
sity optimization using an on-manifold initialization, which is a weaker requirement (Methods 4.4). We
note that some previous studies (for example, ref. [26]) have achieved stable density optimization using a
self-consistent field (SCF) scheme. The applicability of the scheme to M-OFDFT will be investigated in
the future.
4.1 Training the KEDF Model
Although learning the KEDF model TS,Î¸(p,M)can be converted to a supervised machine learning task,
it is more challenging than the conventional form. The essential difference is rooted in the way that the
model is used: instead of as an end-to-end mapping to predict the kinetic energy of (p,M)queries, the
model is used as the objective to optimize the density coefficients pfor a given molecular structure M
(Fig. 1(c)). To eliminate instability and achieve accurate optimization result, the model is required to
capture how to vary with pfor a fixed M, that is, the optimization landscape on the coefficient space.
The conventional data format {M(d),p(d), T(d)
S}d(dindexes training molecular structures) does not ef-
fectively convey such information, since only one labeled pdatapoint is seen for each M. Hence, the
first requirement on training data is multiple coefficient datapoints per structure, following the format
{M(d),{p(d,k), T(d,k)
S}k}d(kindexes the multiple coefficient datapoints). On such data, the model is
trained by minimizing:
X
dX
kTS,Î¸(p(d,k),M(d))âˆ’T(d,k)
S. (3)
After some trials, we found this is still not sufficient. The trained model, although accurately predicts the
kinetic energy value, still decreases the electronic energy in density optimization (Eq. (2)) even starting
from the ground-state density. This indicates the gradient âˆ‡pTS,Î¸(p,M)w.r.t the coefficients is still not
9

accurately recovered. We hence also desire a gradient label for each datapoint, which constitutes data
in the format {M(d),{p(d,k), T(d,k)
S,âˆ‡pT(d,k)
S}k}d. As only the projected gradient matters for density
optimization following Eq. (2), the gradient data is used for training the model by minimizing:
X
dX
k
Iâˆ’w(d)w(d)âŠ¤
w(d)âŠ¤w(d)
âˆ‡pTS,Î¸(p(d,k),M(d))âˆ’ âˆ‡pT(d,k)
S, (4)
where Iis the identity matrix with matching dimension. The gradient label provides additional informa-
tion on the local landscape near each coefficient datapoint. As the model is used in density optimization
only through its gradient, the gradient data directly stabilizes and regularizes density optimization, and
enforces stationary-point condition for correct convergence. Supplementary Section D.4.1 verifies the
improvement empirically through an ablation study.
To generate such multiple-coefficient and gradient-labeled data, we note that it is tractable from running
the conventional KSDFT on each molecular structure M(d), which conducts a self-consistent field (SCF)
iteration. The rationale is that, the task in each SCF step kis to solve a non-interacting fermion system
in an effective one-body potential constructed from previous steps. The ground-state wavefunction solu-
tion is a Slater determinant specified by the Norbital solutions in that step, by which the non-interacting
kinetic energy T(d,k)
S can be directly calculated. The corresponding density coefficients p(d,k)can be
calculated from these orbitals by density fitting [73]; see Supplementary Section A.4.1 for details. For the
gradient label, since p(d,k)represents the ground-state density of the non-interacting system, it minimizes
the energy of the non-interacting system as a function of density coefficient, TS(p,M(d)) +pâŠ¤v(d,k)
eff,
where v(d,k)
eff is the effective potential in SCF step kin vector form under the atomic basis. This indicates
âˆ‡pTS(p(d,k),M(d)) =âˆ’v(d,k)
eff up to the normalization projection. Supplementary Section A.2 elabo-
rates more on the reasoning, and Supplementary Section A.4 provides calculation details, including an
efficient implementation to generate the gradient label.
In our implementation of M-OFDFT, the atomic basis for representing density is taken as the even-
tempered basis set [74] with tempering ratio Î²= 2.5. For generating data, restricted-spin KSDFT is
conducted at the PBE/6-31G(2df,p) level, which is sufficient for the considered systems which are un-
charged, in near-equilibrium conformation, and only involve light atoms (up to fluorine). Here, the basis
sets for expanding electron density and orbitals are different, since the density corresponding to an orbital
state is effectively expanded on the paired orbital basis whose number of basis functions is squared (see
Supplementary Eq. (S28)), so the basis set to expand density needs to be larger than the orbital basis set.
Using a different basis set for density is also the common practice in density fitting, in which context
the basis is called an auxiliary basis. The even-tempered basis set is a common choice in density fitting,
which is finer than other auxiliary basis choices. It achieves a lower density fitting error in our trials, and
could facilitate calculation under other basis by projection onto this finer basis.
4.2 Geometric Invariance
Another challenge beyond conventional machine learning is that the target physical functional exhibits
symmetry w.r.t transformations on the input (p,M={X,Z})arising from the translation and rotation
of the molecule, where XandZ, respectively, denote the coordinates and atomic numbers of the atoms
in the molecule. This is formally referred to as SE(3) -invariance, following â€œ3-dimensional special Eu-
clidean groupâ€ that comprises these transformations. This is because the non-interacting kinetic energy
of electrons does not change with the translation and rotation of the molecule, but the input atomic co-
ordinates Xdo, and the input density coefficients palso change with the rotation. The change of pis
due to that the electron density rotates with the molecule, but the atomic basis functions do not, since
their orientations are aligned with the (global) coordinate system, a.k.a frame. Formally, such input fea-
tures are geometric vectors and tensors that change equivariantly with the translation and/or rotation of
the molecule. Subsequently, the model is expected to have this SE(3) -invariance built-in. This allows
the model to learn the essential dependency of the energy on the density irrespective of geometric vari-
ability, reducing the problem space, and facilitating data efficiency and effective training. The invariance
also enhances generalization and extrapolation performance, as an important physical property is always
guaranteed.
For the invariance w.r.t atomic coordinates X, the neural network model of Graphormer is naturally
SE(3) -invariant, since the model only uses relative distances of atom pairs for later processing, which are
inherently invariant w.r.t the translation and rotation of the molecule. To ensure the invariance of the model
w.r.t the density coefficients p, we introduce a transformation on punder local frames to make invariant
coefficient features. Each local frame is associated to an atom, and specifies the orientation of atomic
10

basis functions on that atom. It is determined by the relative positions among nearby atoms, hence the
basis function orientations rotate with the molecule and the density, making the density coefficients under
the local frame invariant. Specifically, the local frame on the atom located at x(0)
ais determined following
previous works (for example, [75, 76]): the x-axis unit vector Ë†x:= Normalize( x(1)
aâˆ’x(0)
a)is pointed to its
nearest heavy atom located at x(1)
a, then the z-axis is pointed to Ë†z:= Normalize Ë†xÃ—(x(2)
aâˆ’x(0)
a)
, where
x(2)
ais the coordinates of the second-nearest heavy atom not collinear with the nearest one, and finally
the y-axis is pointed to Ë†y:=Ë†zÃ—Ë†xfollowing a right-handed system. See Supplementary Section B.2 for
more details.
Moreover, the local frame approach offers an additional benefit that the coefficient features are stabi-
lized for local molecular substructures, for example, bond or functional group, of the same type. Such
substructures on one molecule may have different orientations relative to the whole molecule, but the elec-
tron density on them are naturally close, up to a rotation. Other invariant implementations, for example,
using an equivariant global frame [77, 78] or processing tensorial input invariantly [79, 80], bind the basis
orientations on different atoms together, so the resulting coefficients on the substructures appear vastly
different. In contrast, using local frames, basis orientations on different atoms are decoupled, and since
they are determined only by nearby atoms, the basis functions rotate from one substructure to another ac-
cordingly. Hence, the resulting density coefficients on the same type of substructures are aligned together,
whose difference only indicates the minor density fluctuation on the same type of substructure but not the
different orientations of the copies. This makes it much easier for the model to identify that such local
density components follow the same pattern and contribute similarly to the energy. Supplementary Sec-
tion B.2 provides an illustrative explanation. We numerically demonstrate the benefit in Supplementary
Figures S4-S5 that using local frame instead of equivariant global frame substantially reduces the vari-
ance of both density coefficients and gradients on atoms of each type. Especially, on most basis functions
of hydrogen, the coefficient and gradient scales are reduced by over 60\%. This substantially stabilizes
the training process and immediately reduces training error, resulting in a considerable improvement of
overall performance as empirically verified in Supplementary Section D.4.3.
4.3 Enhancement Modules for Vast Gradient Range
After reducing the geometric variability of data using local frame, the raw gradient values still show a vast
range, which conventional neural networks are not designed for (for example, ref. [81]) and indeed causes
training difficulties in our trials. This is an intrinsic challenge for learning a physical functional since we
require non-ground-state density in the data, which would increase the energy steeply. The large gradient
range cannot be trivially reduced by conventional data normalization techniques, since its scale is associ-
ated with the scale of energy and coefficient, hence downscaling the gradient would either proportionally
downscale the energy values which requires a higher prediction resolution, or inverse-proportionally up-
scale the coefficients which is also numerically unfriendly to process. To handle this challenge, we intro-
duce a series of enhancement modules to allow expressing a vast gradient range, including dimension-wise
rescaling, a reparameterization of the density coefficients, and an atomic reference module to offset the
large mean of gradient.
Dimension-wise Rescaling We first upgrade data normalization more flexibly to trade-off coefficient-
gradient scales dimension-wise. Considering the number of coefficient dimensions vary from different
molecules, we propose to center and rescale the coefficients using biases Â¯pZ,Ï„and factors Î»Z,Ï„each
specific to one coefficient/gradient dimension Ï„associated with one atom type (that is, chemical element)
Z(instead of one atom). The bias Â¯pZ,Ï„:= mean {p(d,k)
a,Ï„}a:Z(a)=Z, k, d for(Z, Ï„)is the average over
coefficient values in dimension Ï„on all atoms of type Zin all molecular structures in the training dataset.
After centering the coefficients using the bias (which does not affect gradients), the scaling factor Î»Z,Ï„is
determined by upscaling the centered coefficient and simultaneously inverse-proportionally downscaling
the gradient, until the gradient achieves a chosen target scale sgrador the coefficient exceeds a chosen
maximal scale scoeff. In equation:
Î»Z,Ï„:=ï£±
ï£²
ï£³minmean absgradZ,Ï„
sgrad,scoeff
stdcoeff Z,Ï„
,ifmean absgradZ,Ï„> s grad,
1, otherwise ,(5)
where the scales of gradient and coefficient in the (Z, Ï„)-th component are measured by the mean of the
absolute value of the gradient component, mean absgradZ,Ï„:= meanâˆ‡pa,Ï„T(d,k)
S	
a:Z(a)=Z, k, d,
and the standard derivation of the coefficient component, stdcoeff Z,Ï„:= std {p(d,k)
a,Ï„}a:Z(a)=Z, k, d , both
11

collected on the training dataset. Using the rescaling factors, each centered coefficient is rescaled by:
pâ€²
a,Ï„:=Î»Z(a),Ï„pa,Ï„, (6)
and gradient by âˆ‡pa,Ï„Tâ€²
S:=âˆ‡pa,Ï„TS/ Î»Z(a),Ï„(Î»Z,Ï„>1in most cases).
Natural Reparameterization On quite a few dimensions, both the coefficient and gradient scales are
large, making dimension-wise rescaling ineffective. We hence introduce natural reparameterization ap-
plied before rescaling to balance the rescaling difficulties across dimensions hence reduce the worst-case
difficulty. The unbalanced scales come from the different sensitivities of the density function on different
coefficient dimensions: the change of density function from a coefficients change âˆ†pis measured by
the L2-metric in the function space,R
|âˆ†Ï(r)|2dr, which turns out to be âˆ†pâŠ¤Wâˆ†p, in which different
dimensions indeed contribute with different weights since the overlap matrix WÂµÎ½:=R
Ï‰Âµ(r)Ï‰Î½(r) dr
(ÂµandÎ½are indexes of basis functions) therein is generally anisotropic. The reparameterized coefficients
Ëœpare expected to contribute equally across the dimensions:R
|âˆ†Ï(r)|2dr= âˆ†ËœpâŠ¤âˆ†Ëœp. We hence take:
Ëœp:=MâŠ¤p, (7)
where Mis a square matrix satisfying MMâŠ¤=W. See Supplementary Section B.3.2 for more details.
This reparameterization also leads to natural gradient descent [82] in density optimization, which is known
to converge faster than vanilla gradient descent.
Atomic Reference Module Recall that in dimension-wise rescaling, the large bias of coefficients can
be offset by the mean on a dataset, but this does not reduce the bias scale of gradient labels. To further
improve the coefficient-gradient scale trade-off, we introduce an atomic reference module :
TAtomRef (p,M) :=Â¯gâŠ¤
Mp+Â¯TM, (8)
which is linear in the coefficients pand whose output is added to the neural network output as the kinetic
energy value. By this design, the gradient of the atomic reference model âˆ‡pTAtomRef (p,M) =Â¯gMis a
constant, which offsets the target gradient for the neural network to capture, effectively reducing the scale
of gradient labels and facilitating neural network training. The weights Â¯gM:= concat {Â¯gZ(a),Ï„}Ï„, aâˆˆM
and bias Â¯TM:=P
aâˆˆMÂ¯TZ(a)+Â¯Tglobal of the linear model are constructed by tiling and summing the
per-type statistics, which are derived over all atoms of each type in a dataset. The per-type gradient
statistics is defined by Â¯gZ,Ï„:= mean {âˆ‡pa,Ï„T(d,k)
S}a:Z(a)=Z, k, d , which represents the average response
toTSfrom the change of coefficients on an atom of type Z. Per-type bias statistics {Â¯TZ}ZandÂ¯Tglobal are
fit by least squares. See Supplementary Section B.3.3 for more details.
The final KEDF model is constructed from these enhancement modules and the neural network model in
the following way (Supplementary Figure S1(a)): the density coefficients are first transformed under local
frame and processed by natural reparameterization; the processed coefficients, through one branch, are
fed to the atomic reference module to calculate the reference part of output energy, and through another
branch, are processed by dimension-wise rescaling and then input to the neural network model which
produces the rest part of output energy. Comparative results in Supplementary Sections B.3 and D.4.3
highlight the empirical benefits of each module.
4.4 Density Optimization
In the deployment stage, M-OFDFT solves the ground state of a given molecular structure Mby min-
imizing the electronic energy as a function of density coefficients p, where the learned KEDF model
TS,Î¸(p,M)is used to construct the energy function (Fig. 1(a)). As described in Results 2.1, we use gra-
dient descent to optimize p(Eq. (2)), since it is unnatural to formulate the optimization problem into a
self-consistent iteration. Gradient descent has also been used in KSDFT, which bears the merit of being
more stable [83].
A subtlety in density optimization using a learned functional model is that the model may be confronted
with densities far from the training-data manifold (or â€œout of distributionâ€ in machine-learning term),
which may lead to unstable optimization. Such an issue has been observed in previous machine-learning
OFDFT [21, 23], which mitigates the problem by projecting the density onto the training-data manifold in
each optimization step. A similar phenomenon is also observed in M-OFDFT. For example, we find that,
when starting from the MINAO initialization [48] which is common for KSDFT, the density optimization
process leads to an obvious gap from the target KSDFT energy. Supplementary Figure S9 provides an
illustrative case. We note that the initial density by MINAO already lies off the manifold inherently: each
density entry in the training data comes from the eigensolution to an effective one-electron Hamiltonian
matrix, which exactly solves an effective non-interacting fermion system (Supplementary Section A.2),
12

while the MINAO density comes from the superposition of orbitals of each atom in isolation, which is a
different mechanism.
We hence propose using two other initialization methods to resolve the mismatch. The first approach is
to use an established initialization that solves an eigenvalue problem, for which we choose the H Â¨uckel
initialization [84]. In our observation, although the H Â¨uckel density shows a much larger energy error than
MINAO density at initialization, it ultimately indeed leads the optimization process to converge closely
to the target energy. Supplementary Figure S9 provides an illustrative case.
The second choice is to project the MINAO density onto the training-data manifold, which we call Proj-
MINAO. In contrast to previous methods, M-OFDFT conducts optimization on the coefficient space
which varies with molecular structure, so the training-data manifold of coefficient is unknown for an
unseen molecular structure. We hence use another deep learning model âˆ†pÎ¸(p,M)to predict the re-
quired correction to project the input coefficient ptowards the ground-state coefficient pâ‹†of the input
molecular structure M, which is always on the manifold. See Supplementary Section B.5.2 for details.
In our observation, we see ProjMINAO initialization indeed converges the optimization curve close to the
target energy, even better than H Â¨uckel initialization. See Supplementary Figure S9 for an illustrative case.
We also note from a few examples that even though ProjMINAO already closely approximates the ground
state density, density optimization still continues to improve the accuracy. This suggests a potential ad-
vantage over end-to-end ground-state density prediction followed by energy prediction from ground-state
density, which may also encounter extrapolation challenges similar to M-NNP and M-NNP-Den.
Remarkably, M-OFDFT only requires an on-manifold initialization but does not need projection in each
optimization step, suggesting better robustness than previous methods; see Supplementary Section B.5.1
for details. M-OFDFT in Results 2 is conducted using ProjMINAO, although using H Â¨uckel still achieves
a reasonable accuracy; see Supplementary Section D.1.1. Supplementary Section B.5.1 provides curves
in density error and comparison with classical KEDFs.
4.5 Dataset Preparation
To substantiate the efficacy of the proposed method, we conduct evaluations on two distinct molecular
datasets: ethanol and QM9. These datasets are specifically chosen to evaluate the generalization per-
formance of M-OFDFT within both conformation space and chemical space. Furthermore, we seek to
assess the extrapolation capability of M-OFDFT with larger molecular systems, for which two additional
datasets are prepared: QMugs and chignolin. Comprehensive supplementary information and generation
details for each dataset are provided below.
4.5.1 Ethanol
To investigate the generalization ability of M-OFDFT in conformation space, we created a set of 100,000
non-equilibrium ethanol structures, randomly drawn from the MD17 dataset [40, 41]. These geometries
were then randomly partitioned into training, validation and test sets using an 8:1:1 ratio.
4.5.2 QM9
The QM9 dataset [43] serves as a popular benchmark for predicting quantum-chemical properties us-
ing deep learning methods. It comprises equilibrium geometries of approximately 134k small organic
molecules, composed of H,C,O,N, and Fatoms. These molecules represent a subset of all species
with up to nine heavy atoms from the GDB-17 chemical universe dataset [42]. As the dataset contains
only equilibrium geometries, we employ it as a benchmark for evaluating the generalization ability of
M-OFDFT in chemical space. Furthermore, to compare M-OFDFT with classical OFDFT methods, we
include the 6905 isomers of C7H10O2present in the QM9 dataset as a benchmark for assessing M-
OFDFT in conformation space. We continue to randomly split the molecules into training, validation,
and test sets using an 8:1:1 ratio.
4.5.3 QMugs
The QMugs dataset, proposed in ref. [49], comprises over 665k biologically and pharmacologically rel-
evant molecules extracted from the ChEMBL database, totaling approximately 2M conformers. No-
tably, QMugs provides molecular samples that are considerably larger than those in the QM9 and MD17
datasets, with an average of 30.6 and a maximum of 100 heavy atoms per compound. This allows us to
study the extrapolation capabilities of M-OFDFT.
We group QMugs molecules based on the number of heavy atoms into bins with a width of 5, except for
the first bin, which contains molecules with 10-15 heavy atoms. To construct an extrapolation task, we
13

divide the union of QM9 and the first QMugs bin into training and validation sets with a 9:1 ratio, and
test M-OFDFT on 50 molecular structures from each of the other bins with increasing scales. The results
are shown in Fig. 3(a).
For the extrapolation setting in Fig. 3(b) for investigating the amount of additional larger-scale molecular
data required by end-to-end counterparts M-NNP and M-NNP-Den to achieve a comparable extrapolation
performance as M-OFDFT, the target workload of large molecules is fixed, while the affordable training
datasets contain increasingly larger molecules. Specifically, taking the QM9 dataset as the base data
source, the molecular structures from the first four QMugs bins are progressively incorporated to build
four training datasets. All training datasets are prepared with the same size to eliminate statistical effects,
while the composition ratio of each data source is designed to follow their relative composition ratios in
the original joint dataset (QM9 âˆªQMugs). The detailed statistics of data source composition ratios are
summarized in Supplementary Figure S11.
4.5.4 Chignolin
Data Selection To further evaluate the extrapolation ability of M-OFDFT on large-scale biomolecular
molecules, such as proteins, we sampled 1,000 chignolin structures from an extensive molecular dynamic
(MD) simulation [52]. Due to its fast-folding property and short length (TYR-TYR-ASP-PRO-GLU-
THR-GLY-THR-TRP-TYR), chignolin has been widely used in molecular dynamics studies. Follow-
ing Wang et al. [85], we first featurized the backbone conformations into all pairwise Î±-carbon-atom
distances, excluding pairs of nearest neighbor residues. Then, a time-lagged independent component
analysis (TICA) was conducted with a lag time of Ï„lag= 20 ns, projecting the conformations onto a
low-dimensional subspace. The first eight TICA components were clustered into 1,000 groups using the
k-means algorithm. The centroid structure of each cluster was extracted using MDTraj [86] and taken as
the representative structure.
Conformation Neutralization In this study, we focus on isolated molecular systems where all atoms
or functional groups stay neutral and all electrons are paired. However, protein molecules extracted
from MD trajectories are simulated in aqueous solution and some of its functional groups are ionized,
such as the amine group in the N-terminal and the carboxyl group in the C-terminal. In our setting, we
neutralize protein conformations by editing (adding/deleting) hydrogen atoms in ionizable groups using
OpenMM [87], which is expected to have a minimal impact on the original geometry. The hydrogen
definition template was modified to handle hydrogen atoms not defined in standard amino acids, such
as the missing hydrogen atom in the carboxyl group in the C-terminal amino acid. We then performed
local energy optimization on the neutralized conformations using Amber [88], with the Amber-ff14SB
force field and a maximum of 100 optimization cycles. To prevent substantial geometry changes, we
only allowed hydrogen atoms associated with heavy atoms involved in the hydrogen editing process to
be optimized. For neutralized amino acids without standard force field templates, we built force field
parameters using the antechamber and parmchk2 tools in Amber.
Protein Fragmentation To investigate the extrapolation utility of M-OFDFT from â€œlocalâ€ fragments
to â€œglobalâ€ proteins, we created our datasets by cutting each of the 1,000 chignolin structures into protein
fragments (that is, polypeptides) of varying lengths. To construct these fragments, we enumerated all
possible polypeptides with sequence lengths up to five in the chignolin sequence. Specifically, we allow
fragments of length 5 to be composed of two dipeptides or a dipeptide and a tripeptide, that is, dipeptide-
[GAP]-dipeptide and tripeptide-[GAP]-dipeptide. The extracted fragment molecules were capped with
hydrogen atoms instead of larger capping groups to minimize the introduction of substantial geometry
changes. These fragments were then used as training and validation sets with a 9:1 ratio. It should be
noted that when benchmarking the performance of deep learning methods trained with different molecule
scales, the training set with a maximum peptide length of Lpepis comprised of all polypeptides with
sequence lengths less than Lpep.
For the comparison of M-OFDFT with classical KEDFs in Fig. 3(c), we only used 50 chignolin structures
as the test set, since classical KEDFs require grid-based quadrature which is very costly for running on
all the 1,000 chignolin structures.
Data Filtration After preliminary analysis, we found that the range of energy and gradient labels in
fragments datasets are too vast to fit even if equipped with several efficient training techniques (see Sup-
plementary Section B.3). Moreover, most of the vast datapoints come from the first several steps of the
SCF iterations that are far from convergent. To mitigate this issue, we filter out datapoints if the residual
between its energy and the ground-state energy is larger than 500 kcal/mol. This strategy can empirically
improve the optimization robustness and prediction performance.
14

4.6 Data Generation Configurations
KSDFT Calculations All KSDFT calculations were carried out using the PySCF [48] software pack-
age, which is open-sourced and has a convenient Python interface that allows flexible customization, for
example, for implementing the data generation process detailed in Supplementary Section A.4, and im-
plementing M-OFDFT calculation described in Results 2.1 using automatic differentiation packages in
Python. A well-acknowledged pure XC functional (that is, only depends on density but not orbitals),
for example, PBE, is used to make sure the data pairs demonstrate a density functional. Restricted-spin
calculation and the 6-31G(2df,p) basis set were adopted, which is sufficient for the considered molecules
which are uncharged, in near-stable conformations, and only involve light atoms (up to fluorine). To
accelerate the calculations, density fitting with the def2-universal-jfit basis set was enabled in the calcu-
lations for molecules with more than 30 atoms. Grid level was set to 2. Convergence tolerence was set to
1meV . Slight modifications to the PySCF code were made to log the required information (for example,
the molecular orbitals) for SCF intermediate steps. Direct Inversion in Iterative Subspace (DIIS) was
enabled, following the defaults. Per-molecule statistics such as running time and number of atoms were
collected alongside the calculation for benchmarking purposes.
KSDFT Initialization The MINAO initialization method is employed to conduct KSDFT calculation
for data generation, adhering to the default settings. We would like to mention that as explained and
empirically shown in Methods 4.4, the H Â¨uckel initialization achieves a good convergence behavior for
density optimization of M-OFDFT, since it comes from an eigenvalue-problem solution and hence aligns
with the way how the training data are generated (Supplementary Section A.4). It is then motivated to
generate data also using the H Â¨uckel initialization for running KSDFT, in hope to further align the training
data with the use cases (that is, density optimization of M-OFDFT using H Â¨uckel initialization), making the
use cases lie more in-distribution with the training data and expecting better density optimization results.
We tried generating and leveraging such data, but found the generated data entries from different SCF
iterations on the same molecular structure show much larger variance, especially the gradient labels. This
brings more challenges on training stability and effectiveness, which are hard to handle effectively even
with the techniques in Methods 4.3 and Supplementary Section B.3. Although using H Â¨uckel-initialized
data improves the accuracy of H Â¨uckel-initialized M-OFDFT on ethanol, the improvement is not obvious
on QM9, and using H Â¨uckel-initialized data does not improve the accuracy of ProjMINAO-initialized M-
OFDFT on either dataset, due to that the training challenges outweigh the benefits.
Hardware Details All KSDFT calculations and preprocessing (for example, density fitting) procedures
were carried out on a cluster of 700 capable CPU servers. Each server in the cluster has 256 GiB of
memory and 32 Intel Xeon Platinum 8272CL cores with hyperthreading disabled.
Dataset Preprocessing After the SCF runs, extra procedures were done to transform the SCF results
into datasets for training and evaluation. The density coefficients were obtained using the density fitting
procedure detailed in Supplementary Section A.4.1, with the auxiliary basis set being an even-tempered
basis set [74] generated on-the-fly with Î²= 2.5(see Supplementary Eq. (S67)). After that, gradient
and force labels were calculated using the obtained density coefficients following Supplementary Sec-
tion A.4.3 and A.5. Energy labels were calculated following Supplementary Section A.4.2. For large
orbital integrals in the procedure, proper symmetries in the atomic orbital indices were leveraged to save
computation and reduce memory footprint. Local frame transformation and natural reparameterization
(detailed in Supplementary Section B.2 and Supplementary Section B.3.2) were applied to the coeffi-
cients to obtain the model inputs.
4.7 Classical OFDFT Methods Implementation
To evaluate the advantages of M-OFDFT, we select several classical kinetic energy density functionals
(KEDFs) for reference. The Thomas-Fermi (TF) KEDF [4, 5] TTF[Ï] :=3
10(3Ï€2)2
3R
Ï(r)5/3dris the
exact KEDF for uniform electron gas (UEG), that is, when the electron density Ïis constant. Expanding
KEDF around the UEG limit up to the first-order variation gives the KEDF approximation TTF[Ï] +
1
9TvW[Ï](ref. [12]), where TvW[Ï] :=1
8Râˆ¥âˆ‡Ï(r)âˆ¥2
Ï(r)dris the von Weizs Â¨acker KEDF [46] which corrects the
approximation with gradient information of the input density. We also include a variant of this correction
TTF[Ï] +TvW[Ï], which is also considered in ref. [45]. The base APBE KEDF [44] is also tested.
For a fair comparison, these functionals are implemented in the same pipeline and code framework as
M-OFDFT. Similar to the way we use the PBE XC functional and the ABPE KEDF as mentioned in Sup-
plementary Section A.3.2, we also re-implemented the TF, TF+1
9vW, and TF+vW functionals in PySCF
using PyTorch. Their values are evaluated by numerical quadrature on a grid in the real 3-dimensional
space. To solve for the ground-state density, gradient descent is used, for which the gradient of the ki-
netic energy is directly evaluated by automatic differentiation. The same stopping criterion is used. For
15

initialization, we use the MINAO method, which we found better than the H Â¨uckel method. Since the cal-
culation of these classical KEDFs requires quadrature on grid which is substantially more computationally
intensive for large molecules, we only used 50 structures for comparison on chignolin, and omitted the
comparison on larger proteins. For the same reason, we omitted the comparison with other deep learning
OFDFT methods [21, 23, 28] since they use grid to represent density thus also require the costly grid
quadrature, in addition to lack of access to their implementation.
We also attempted running other more recent KEDFs [13â€“15] using the implementation in OFDFT soft-
ware packages, including PROFESS [89], GPAW [90], ATLAS [91] and DFTpy [92]. However, they
are primarily oriented towards periodic material systems and use plane-wave basis. To avoid the dif-
ficulty of expressing sharp density change near nuclei using plane waves, most of the packages use
pseudo-potentials (which have to be local ones), but they did not provide pseudo-potentials for ele-
ments ( HCNOF ) in our concerned systems, and we encountered difficulties in generating reliable lo-
cal pseudo-potentials for these elements and in converting the real Coulomb potential into the required
pseudo-potential format. Although GPAW does not require a pseudo-potential, we found that its OFDFT
calculations on molecular systems are hard to converge, even on small molecules (for example, ethanol)
using the default settings with a looser convergence criterion.
4.8 M-NNP and M-NNP-Den Implementation
Results 2.3 demonstrate that M-OFDFT has the ability to extrapolate to large molecular systems, a valu-
able advantage for quantum chemistry methods. We measure the relevance of this advantage by con-
trasting M-OFDFT with two deep learning-based alternatives, M-NNP and M-NNP-Den, that are aimed
to predict the total energy (ground-state electronic energy plus inter-nuclear energy) from the molecular
structure M. Both alternatives employ the same Graphormer backbone architecture as M-OFDFT, but
they do not have the density projection branch (Supplementary Section B.5.2), and M-NNP does not have
the density coefficient input branch in its NodeEmbedding module (Supplementary Figure S1(c)). The
model hyperparameters are also adjusted to be comparable to those of M-OFDFT.
4.9 Curve Fitting Details
For fitting the curves in Fig. 3(a) and Fig. 4, we restrict the formulas (aNheavy +b)c+dof all curves
in each figure to have the same scale, that is, the same a. Otherwise, the flexibility of awould diminish
the reflection of the exponent con the curvature of the curve on the provided range of Nheavy (that is, a
smaller aallows a larger exponent c). We found that sharing the same ain different curves hardly hinders
the closeness to fitted datapoints. A two-phase fitting strategy is put forward based on this idea: (1) we
jointly fit all curves with a shared a, obtaining the pre-optimized aâ€²; (2) taking aâ€²as an initial guess, we re-
fit all curves independently to obtain the post-optimized aâˆ—using the Trust Region Reflective optimization
algorithm, where we restrict the search space of variable ainto[aâ€²(1âˆ’Î¾),aâ€²(1 +Î¾)], with Î¾= 0.5. The
two-phase fitting strategy approximately keeps all post-optimized aâˆ—in the same scale as well as bring
better fitting accuracy. The fitting process is conducted using the SciPy package in Python.
Inclusion \& Ethics
All collaborators who have fulfilled all the criteria of authorship defined by Nature Portfolio journals have
been listed as authors, and other collaborators have been listed in the Acknowledgements. The research
is not on a location-specific topic, but has included local researchers related to the topic throughout the
research process. Roles, responsibilities and workload plan were agreed among collaborators ahead of
the research. This research was not severely restricted or prohibited in the setting of the researchers, and
does not result in stigmatization, incrimination, discrimination or personal risk to participants.
Data Availability
All molecular structures used in this study can be freely accessed from public sources: ethanol struc-
tures are from the MD17 dataset [40] at http://www.sgdml.org/\#datasets , QM9 [43] molecu-
lar structures are from http://dx.doi.org/10.6084/m9.figshare.978904 , QMugs [49] molecular
structures are from https://doi.org/10.3929/ethz-b-000482129 , and structures of Chignolin, the
BBL-H142W system (PDB ID: 2WXC ), and the protein B system (PDB ID: 1PRB ) are from ref. [52] at
https://www.deshawresearch.com/downloads/download\_trajectory\_science2011.cgi . Ex-
ample evaluation data for reproducing the analyses in this work are available at https://doi.org/10.
6084/m9.figshare.c.6877432 (ref. [93]). Source data are provided with this paper.
16

Code Availability
The code for implementing the proposed methodology is available at https://doi.org/10.5281/
zenodo.10616893 (ref. [94]). Trained neural network model checkpoints are available at https:
//doi.org/10.6084/m9.figshare.c.6877432 (ref. [93]).
References
[1] Jorge M Seminario. Recent developments and applications of modern density functional theory .
Elsevier, 1996. 1
[2] Anubhav Jain, Shyue Ping Ong, Geoffroy Hautier, Wei Chen, William Davidson Richards, Stephen
Dacek, Shreyas Cholia, Dan Gunter, David Skinner, Gerbrand Ceder, and Kristin A. Persson. Com-
mentary: The Materials Project: A materials genome approach to accelerating materials innova-
tion. APL Materials , 1(1):011002, 07 2013. ISSN 2166-532X. doi: 10.1063/1.4812323. URL
https://doi.org/10.1063/1.4812323 . 1
[3] Walter Kohn and Lu Jeu Sham. Self-consistent equations including exchange and correlation ef-
fects. Physical review , 140(4A):A1133, 1965. 1, A.1
[4] Llewellyn H Thomas. The calculation of atomic fields. In Mathematical proceedings of the Cam-
bridge philosophical society , volume 23, pages 542â€“548. Cambridge University Press, 1927. 1,
2.2, 4.7, A.1, B.5.1
[5] Enrico Fermi. Eine statistische methode zur bestimmung einiger eigenschaften des atoms und ihre
anwendung auf die theorie des periodischen systems der elemente. Zeitschrift f Â¨ur Physik , 48(1):
73â€“79, 1928. 2.2, 4.7, B.5.1
[6] John C Slater. A simplification of the Hartree-Fock method. Physical review , 81(3):385, 1951. A.1
[7] Pierre Hohenberg and Walter Kohn. Inhomogeneous electron gas. Physical review , 136(3B):B864,
1964. 1, A.1
[8] Yan Alexander Wang and Emily A Carter. Orbital-free kinetic-energy density functional theory.
Theoretical methods in condensed phase chemistry , 5:117â€“184, 2000. 1, 1, B.1
[9] Valentin V Karasiev, Debajit Chakraborty, and SB Trickey. Progress on new approaches to old
ideas: Orbital-free density functionals. In V olker Bach and Luigi Delle Site, editors, Many-electron
approaches in physics, chemistry and mathematics: a multidisciplinary view , pages 113â€“134.
Springer, 2014.
[10] Bing Huang, Guido Falk von Rudorff, and O Anatole von Lilienfeld. The central role of density
functional theory in the AI age. Science , 381(6654):170â€“175, 2023. 1
[11] C. H. Hodges. Quantum corrections to the Thomasâ€“Fermi approximation â€” the Kirzhnits method.
Canadian Journal of Physics , 51(13):1428â€“1437, 1973. doi: 10.1139/p73-189. URL https:
//doi.org/10.1139/p73-189 . 1
[12] M. Brack, B.K. Jennings, and Y .H. Chu. On the extended Thomas-Fermi approximation to the ki-
netic energy density. Physics Letters B , 65(1):1â€“4, 1976. ISSN 0370-2693. doi: https://doi.org/10.
1016/0370-2693(76)90519-0. URL https://www.sciencedirect.com/science/article/
pii/0370269376905190 . 2.2, 4.7, B.5.1
[13] Lin-Wang Wang and Michael P Teter. Kinetic-energy functional of the electron density. Physical
Review B , 45(23):13196, 1992. 4.7
[14] Yan Alexander Wang, Niranjan Govind, and Emily A Carter. Orbital-free kinetic-energy density
functionals with a density-dependent kernel. Physical Review B , 60(24):16350, 1999. 1, B.1, B.5.1
[15] Chen Huang and Emily A Carter. Nonlocal orbital-free kinetic energy density functional for semi-
conductors. Physical Review B , 81(4):045206, 2010. 1, 4.7
[16] Linda Hung and Emily A. Carter. Accurate simulations of metals at the mesoscale: Explicit
treatment of 1 million atoms with quantum mechanics. Chemical Physics Letters , 475(4):163â€“
170, 2009. ISSN 0009-2614. doi: https://doi.org/10.1016/j.cplett.2009.04.059. URL https:
//www.sciencedirect.com/science/article/pii/S0009261409005041 . 1
17

[17] William C Witt, G Beatriz, Johannes M Dieterich, and Emily A Carter. Orbital-free density func-
tional theory for materials research. Journal of Materials Research , 33(7):777â€“795, 2018. 1
[18] David Garc Â´Ä±a-Aldea and JE Alvarellos. Kinetic energy density study of some representative semilo-
cal kinetic energy functionals. The Journal of chemical physics , 127(14):144109, 2007. 1
[19] Junchao Xia, Chen Huang, Ilgyou Shin, and Emily A Carter. Can orbital-free density functional
theory simulate molecules? The Journal of chemical physics , 136(8):084102, 2012.
[20] Andrew M Teale, Trygve Helgaker, Andreas Savin, Carlo Adamo, B Â´alint Aradi, Alexei V Ar-
buznikov, Paul W Ayers, Evert Jan Baerends, Vincenzo Barone, Patrizia Calaminici, et al. DFT
exchange: sharing perspectives on the workhorse of quantum chemistry and materials science.
Physical chemistry chemical physics , 24(47):28700â€“28781, 2022. 1, B.1
[21] John C Snyder, Matthias Rupp, Katja Hansen, Klaus-Robert M Â¨uller, and Kieron Burke. Finding
density functionals with machine learning. Physical review letters , 108(25):253002, 2012. 1, 4,
4.4, 4.7, B.5.2, B.5.2, C
[22] Li Li, Thomas E Baker, Steven R White, Kieron Burke, et al. Pure density functional for strong cor-
relation and the thermodynamic limit from machine learning. Physical Review B , 94(24):245129,
2016. C
[23] Felix Brockherde, Leslie V ogt, Li Li, Mark E Tuckerman, Kieron Burke, and Klaus-Robert M Â¨uller.
Bypassing the Kohn-Sham equations with machine learning. Nature communications , 8(872),
2017. 1, 4, 4.4, 4.7, B.5.2, B.5.2, C
[24] Junji Seino, Ryo Kageyama, Mikito Fujinami, Yasuhiro Ikabata, and Hiromi Nakai. Semi-local
machine-learned kinetic energy density functional with third-order gradients of electron density.
The Journal of Chemical Physics , 148(24):241705, 2018. 1, 4, C
[25] Junji Seino, Ryo Kageyama, Mikito Fujinami, Yasuhiro Ikabata, and Hiromi Nakai. Semi-local
machine-learned kinetic energy density functional demonstrating smooth potential energy curves.
Chemical Physics Letters , 734:136732, 2019. 4, C, D.1.1
[26] Fumihiro Imoto, Masatoshi Imada, and Atsushi Oshiyama. Order-N orbital-free density-functional
calculations with machine learning of functional derivatives for semiconductors and metals. Phys-
ical Review Research , 3(3):033198, 2021. 1, 4, B.5.1, C, D.4.4
[27] Kun Yao and John Parkhill. Kinetic energy of hydrocarbons as a function of electron density and
convolutional neural networks. Journal of chemical theory and computation , 12(3):1139â€“1147,
2016. 1, 4, C, D.1.1
[28] Ralf Meyer, Manuel Weichselbaum, and Andreas W Hauser. Machine learning approaches to-
ward orbital-free density functional theory: Simultaneous training on the kinetic energy density
functional and its functional derivative. Journal of chemical theory and computation , 16(9):5685â€“
5694, 2020. 4, 4.7, C, D.4.4
[29] R. Remme, T. Kaczun, M. Scheurer, A. Dreuw, and F. A. Hamprecht. KineticNet: Deep learning
a transferable kinetic energy functional for orbital-free density functional theory. The Journal of
Chemical Physics , 159(14):144113, 10 2023. ISSN 0021-9606. doi: 10.1063/5.0158275. URL
https://doi.org/10.1063/5.0158275 . 1, 4, B.5.1, C
[30] P. Garc Â´Ä±a-Gonz Â´alez, J. E. Alvarellos, and E. Chac Â´on. Nonlocal kinetic-energy-density functionals.
Phys. Rev. B , 53:9509â€“9512, Apr 1996. doi: 10.1103/PhysRevB.53.9509. URL https://link.
aps.org/doi/10.1103/PhysRevB.53.9509 . 1, B.1
[31] Wenhui Mi, Alessandro Genova, and Michele Pavanello. Nonlocal kinetic energy functionals by
functional integration. The Journal of Chemical Physics , 148(18):184107, 05 2018. ISSN 0021-
9606. doi: 10.1063/1.5023926. URL https://doi.org/10.1063/1.5023926 . 1, B.1
[32] Tosio Kato. On the eigenfunctions of many-particle systems in quantum mechanics. Communica-
tions on Pure and Applied Mathematics , 10(2):151â€“177, 1957. 1
[33] Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and
Tie-Yan Liu. Do Transformers really perform badly for graph representation? Advances in Neural
Information Processing Systems , 34:28877â€“28888, 2021. 1, B.1, B.1.6, B.1.7
18

[34] Yu Shi, Shuxin Zheng, Guolin Ke, Yifei Shen, Jiacheng You, Jiyan He, Shengjie Luo, Chang Liu,
Di He, and Tie-Yan Liu. Benchmarking Graphormer on large-scale molecular modeling datasets.
arXiv preprint arXiv:2203.04810 , 2022. 1, B.1, B.1, B.1.4
[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information
processing systems , 30:6000â€“6010, 2017. 1, A.3.2, B.1.4, B.1.4
[36] Justin S Smith, Olexandr Isayev, and Adrian E Roitberg. ANI-1: an extensible neural network
potential with DFT accuracy at force field computational cost. Chemical science , 8(4):3192â€“3203,
2017. 1, 2.2
[37] Kristof T Sch Â¨utt, Huziel E Sauceda, P-J Kindermans, Alexandre Tkatchenko, and K-R M Â¨uller.
SchNet â€“ a deep learning architecture for molecules and materials. The Journal of Chemical
Physics , 148(24):241722, 2018. A.5
[38] Philipp Th Â¨olke and Gianni De Fabritiis. Equivariant transformers for neural network based molec-
ular potentials. In International Conference on Learning Representations , 2021. 2.3, D.2.1, S17
[39] Yi-Lun Liao and Tess Smidt. Equiformer: Equivariant graph attention transformer for 3d atom-
istic graphs. In The Eleventh International Conference on Learning Representations , 2023. URL
https://openreview.net/forum?id=KwmPfARgOTD . 1, 2.3, D.2.1, S17
[40] Stefan Chmiela, Alexandre Tkatchenko, Huziel E Sauceda, Igor Poltavsky, Kristof T Sch Â¨utt, and
Klaus-Robert M Â¨uller. Machine learning of accurate energy-conserving molecular force fields. Sci-
ence advances , 3(5):e1603015, 2017. 2.2, 4.5.1, 4.9, A.5, D.1.1
[41] Stefan Chmiela, Huziel E Sauceda, Igor Poltavsky, Klaus-Robert M Â¨uller, and Alexandre
Tkatchenko. sGDML: Constructing accurate and data efficient molecular force fields using ma-
chine learning. Computer Physics Communications , 240:38â€“45, 2019. 2.2, 4.5.1, D.1.1
[42] Lars Ruddigkeit, Ruud Van Deursen, Lorenz C Blum, and Jean-Louis Reymond. Enumeration
of 166 billion organic small molecules in the chemical universe database GDB-17. Journal of
chemical information and modeling , 52(11):2864â€“2875, 2012. 2.2, 4.5.2
[43] Raghunathan Ramakrishnan, Pavlo O Dral, Matthias Rupp, and O Anatole von Lilienfeld. Quan-
tum chemistry structures and properties of 134 kilo molecules. Scientific Data , 1(140022), 2014.
2.2, 4.5.2, 4.9
[44] Lucian A Constantin, E Fabiano, S Laricchia, and F Della Sala. Semiclassical neutral atom as a
reference system in density functional theory. Physical review letters , 106(18):186406, 2011. 2.2,
4.7, A.3.2, B.2, B.4.1, B.5.1
[45] Valentin V Karasiev and Samuel B Trickey. Issues and challenges in orbital-free density functional
calculations. Computer Physics Communications , 183(12):2519â€“2527, 2012. 2.2, 4.7, B.5.1
[46] CF von Weizs Â¨acker. Zur theorie der kernmassen. Zeitschrift f Â¨ur Physik , 96(7):431â€“458, 1935. 2.2,
4.7, B.4.1, D.1.1
[47] F. L. Hirshfeld. Bonded-atom fragments for describing molecular charge densities. Theoretica
chimica acta , 44(2):129â€“138, 1977. doi: 10.1007/BF00549096. URL https://doi.org/10.
1007/BF00549096 . 2.2, D.1.2, D.2.1
[48] Qiming Sun, Timothy C Berkelbach, Nick S Blunt, George H Booth, Sheng Guo, Zhendong
Li, Junzi Liu, James D McClain, Elvira R Sayfutyarova, Sandeep Sharma, et al. PySCF: the
Python-based simulations of chemistry framework. Wiley Interdisciplinary Reviews: Computa-
tional Molecular Science , 8(1):e1340, 2018. 2.3, 4.4, 4.6, A.3.2, B.5.1, D.1.1
[49] Clemens Isert, Kenneth Atz, Jos Â´e Jim Â´enez-Luna, and Gisbert Schneider. QMugs, quantum me-
chanical properties of drug-like molecules. Scientific Data , 9(273), 2022. 2.3, 2.4, 4.5.3, 4.9
[50] Yilun Du and Igor Mordatch. Implicit generation and generalization in energy-based models. arXiv
preprint arXiv:1903.08689 , 2019. 2.3
[51] Yilun Du, Shuang Li, Joshua Tenenbaum, and Igor Mordatch. Learning iterative reasoning
through energy minimization. In International Conference on Machine Learning , pages 5570â€“
5582. PMLR, 2022. 2.3
19

[52] Kresten Lindorff-Larsen, Stefano Piana, Ron O Dror, and David E Shaw. How fast-folding proteins
fold. Science , 334(6055):517â€“520, 2011. 2.3, 4.5.4, 4.9, D.3
[53] Hannes Neuweiler, Timothy D Sharpe, Trevor J Rutherford, Christopher M Johnson, Mark D Allen,
Neil Ferguson, and Alan R Fersht. The folding mechanism of BBL: Plasticity of transition-state
structure observed within an ultrafast folding protein family. Journal of molecular biology , 390(5):
1060â€“1073, 2009. 2.4
[54] Ting Wang, Yongjin Zhu, and Feng Gai. Folding of a three-helix bundle at the folding speed limit.
The Journal of Physical Chemistry B , 108(12):3694â€“3697, 2004. 2.4
[55] Mark J Rayson and Patrick R Briddon. Rapid iterative method for electronic-structure eigenprob-
lems using localised basis functions. Computer Physics Communications , 178(2):128â€“134, 2008.
2.4
[56] Joost VandeV ondele, Matthias Krack, Fawzi Mohamed, Michele Parrinello, Thomas Chassaing,
and J Â¨urg Hutter. QUICKSTEP: Fast and accurate density functional calculations using a mixed
Gaussian and plane waves approach. Computer Physics Communications , 167(2):103â€“128,
2005. ISSN 0010-4655. doi: https://doi.org/10.1016/j.cpc.2004.12.014. URL https://www.
sciencedirect.com/science/article/pii/S0010465505000615 . 3
[57] Hong-Zhou Ye and Timothy C. Berkelbach. Fast periodic Gaussian density fitting by range sep-
aration. The Journal of Chemical Physics , 154(13):131104, 04 2021. doi: 10.1063/5.0046617.
3
[58] Pietro Cortona. Self-consistently determined properties of solids without band-structure calcula-
tions. Physical Review B , 44(16):8454, 1991. 3
[59] N Govind, YA Wang, AJR Da Silva, and EA Carter. Accurate ab initio energetics of extended
systems via explicit correlation embedded in a density functional environment. Chemical physics
letters , 295(1-2):129â€“134, 1998. 3
[60] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. Advances in neural information processing systems , 33:1877â€“1901, 2020. 3
[61] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min,
Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. arXiv preprint
arXiv:2303.18223 , 2023. 3
[62] Shuxin Zheng, Jiyan He, Chang Liu, Yu Shi, Ziheng Lu, Weitao Feng, Fusong Ju, Jiaxi Wang,
Jianwei Zhu, Yaosen Min, Zhang He, Shidi Tang, Hongxia Hao, Peiran Jin, Chi Chen, Frank
NoÂ´e, Haiguang Liu, and Tie-Yan Liu. Towards predicting equilibrium distributions for molecular
systems with deep learning. arXiv preprint arXiv:2306.05445 , 2023. 3, B.1
[63] Linfeng Zhang, De-Ye Lin, Han Wang, Roberto Car, and E Weinan. Active learning of uniformly
accurate interatomic potentials for materials simulation. Physical Review Materials , 3(2):023804,
2019. 3
[64] Johannes K. Krondorfer, Christian W. Binder, and Andreas W. Hauser. Symmetry- and gradient-
enhanced Gaussian process regression for the active learning of potential energy surfaces in porous
materials. The Journal of Chemical Physics , 159(1):014115, 07 2023. ISSN 0021-9606. doi:
10.1063/5.0154989. URL https://doi.org/10.1063/5.0154989 . 3
[65] Sebastian Dick and Marivi Fernandez-Serra. Machine learning accurate exchange and correlation
functionals of the electronic density. Nature communications , 11(1):3509, 2020. 4, B.1
[66] Yixiao Chen, Linfeng Zhang, Han Wang, and Weinan E. DeePKS: A comprehensive data-driven
approach toward chemically accurate density functional theory. Journal of Chemical Theory and
Computation , 17(1):170â€“181, 2021. 4, B.1, D.4.4
[67] John C Snyder, Matthias Rupp, Katja Hansen, Leo Blooston, Klaus-Robert M Â¨uller, and Kieron
Burke. Orbital-free bond breaking via machine learning. The Journal of chemical physics , 139
(22):224104, 2013. 4, B.5.2, B.5.2, C
20

[68] Mikito Fujinami, Ryo Kageyama, Junji Seino, Yasuhiro Ikabata, and Hiromi Nakai. Orbital-free
density functional theory calculation applying semi-local machine-learned kinetic energy density
functional and kinetic potential. Chemical Physics Letters , 748:137358, 2020. 4, B.5.1, C, D.4.4
[69] Pablo del Mazo-Sevillano and Jan Hermann. Variational principle to regularize machine-
learned density functionals: the non-interacting kinetic-energy functional. arXiv preprint
arXiv:2306.17587 , 2023. 4, C, D.4.4
[70] Ryo Nagai, Ryosuke Akashi, and Osamu Sugino. Completing density functional theory by machine
learning hidden messages from molecules. npj Computational Materials , 6(1):1â€“8, 2020. 4, D.4.4
[71] James Kirkpatrick, Brendan McMorrow, David HP Turban, Alexander L Gaunt, James S Spencer,
Alexander GDG Matthews, Annette Obika, Louis Thiry, Meire Fortunato, David Pfau, et al. Push-
ing the frontiers of density functionals by solving the fractional electron problem. Science , 374
(6573):1385â€“1389, 2021. D.4.4
[72] Li Li, Stephan Hoyer, Ryan Pederson, Ruoxi Sun, Ekin D Cubuk, Patrick Riley, Kieron Burke,
et al. Kohn-Sham equations as regularizer: Building prior knowledge into machine-learned physics.
Physical review letters , 126(3):036401, 2021. 4, D.4.4
[73] Brett I. Dunlap. Robust and variational fitting. Phys. Chem. Chem. Phys. , 2:2113â€“2116, 2000. doi:
10.1039/B000027M. URL http://dx.doi.org/10.1039/B000027M . 4.1, A.4.1
[74] Richard D. Bardo and Klaus Ruedenberg. Even-tempered atomic orbitals. VI. optimal orbital
exponents and optimal contractions of Gaussian primitives for hydrogen, carbon, and oxygen in
molecules. The Journal of Chemical Physics , 60(3):918â€“931, 1974. doi: 10.1063/1.1681168.
URL https://doi.org/10.1063/1.1681168 . 4.1, 4.6, B.1.1
[75] Jiequn Han, Linfeng Zhang, Roberto Car, et al. Deep Potential: A general representation of a
many-body potential energy surface. Communications in Computational Physics , 23(3):629â€“639,
2018. 4.2, B.2
[76] He Li, Zun Wang, Nianlong Zou, Meng Ye, Runzhang Xu, Xiaoxun Gong, Wenhui Duan, and Yong
Xu. Deep-learning density functional theory Hamiltonian for efficient ab initio electronic-structure
calculation. Nature Computational Science , 2(6):367â€“377, 2022. 4.2
[77] Feiran Li, Kent Fujiwara, Fumio Okura, and Yasuyuki Matsushita. A closer look at rotation-
invariant deep point cloud analysis. In Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 16218â€“16227, 2021. 4.2
[78] Omri Puny, Matan Atzmon, Edward J. Smith, Ishan Misra, Aditya Grover, Heli Ben-Hamu, and
Yaron Lipman. Frame averaging for invariant and equivariant network design. In International
Conference on Learning Representations , 2022. URL https://openreview.net/forum?id=
zIUyj55nXR . 4.2
[79] Kristof Sch Â¨utt, Oliver Unke, and Michael Gastegger. Equivariant message passing for the pre-
diction of tensorial properties and molecular spectra. In International Conference on Machine
Learning , pages 9377â€“9388. PMLR, 2021. 4.2
[80] Simon Batzner, Albert Musaelian, Lixin Sun, Mario Geiger, Jonathan P Mailoa, Mordechai Korn-
bluth, Nicola Molinari, Tess E Smidt, and Boris Kozinsky. E (3)-equivariant graph neural networks
for data-efficient and accurate interatomic potentials. Nature communications , 13(1):2453, 2022.
4.2
[81] Mahyar Fazlyab, Alexander Robey, Hamed Hassani, Manfred Morari, and George Pappas. Efficient
and accurate estimation of Lipschitz constants for deep neural networks. Advances in Neural
Information Processing Systems , 32:11427â€“11438, 2019. 4.3
[82] Shun-Ichi Amari. Natural gradient works efficiently in learning. Neural computation , 10(2):251â€“
276, 1998. 4.3
[83] Naruki Yoshikawa and Masato Sumita. Automatic differentiation for the direct minimization ap-
proach to the Hartreeâ€“Fock method. The Journal of Physical Chemistry A , 126(45):8487â€“8493,
2022. 4.4
21

[84] Roald Hoffmann. An Extended H Â¨uckel Theory. I. Hydrocarbons. The Journal of Chemical Physics ,
39(6):1397â€“1412, 06 1963. ISSN 0021-9606. doi: 10.1063/1.1734456. URL https://doi.org/
10.1063/1.1734456 . 4.4, B.5.2
[85] Jiang Wang, Simon Olsson, Christoph Wehmeyer, Adri `a PÂ´erez, Nicholas E Charron, Gianni
De Fabritiis, Frank No Â´e, and Cecilia Clementi. Machine learning of coarse-grained molecular
dynamics force fields. ACS central science , 5(5):755â€“767, 2019. 4.5.4
[86] Robert T McGibbon, Kyle A Beauchamp, Matthew P Harrigan, Christoph Klein, Jason M Swails,
Carlos X Hern Â´andez, Christian R Schwantes, Lee-Ping Wang, Thomas J Lane, and Vijay S Pande.
Mdtraj: a modern open library for the analysis of molecular dynamics trajectories. Biophysical
journal , 109(8):1528â€“1532, 2015. 4.5.4
[87] Peter Eastman, Jason Swails, John D Chodera, Robert T McGibbon, Yutong Zhao, Kyle A
Beauchamp, Lee-Ping Wang, Andrew C Simmonett, Matthew P Harrigan, Chaya D Stern, et al.
OpenMM 7: Rapid development of high performance algorithms for molecular dynamics. PLoS
computational biology , 13(7):e1005659, 2017. 4.5.4
[88] David A Case, Tom A Darden, Thomas E Cheatham, Carlos L Simmerling, Junmei Wang, Robert E
Duke, Ray Luo, MRCW Crowley, Ross C Walker, Wei Zhang, et al. Amber 10. Technical report,
University of California, 2008. 4.5.4
[89] Gregory S Ho, Vincent L Lign `eres, and Emily A Carter. Introducing PROFESS: A new program
for orbital-free density functional theory calculations. Computer physics communications , 179(11):
839â€“854, 2008. 4.7
[90] Jussi Enkovaara, Carsten Rostgaard, J JÃ¸rgen Mortensen, Jingzhe Chen, M DuÅ‚ak, Lara Ferrighi,
Jeppe Gavnholt, Christian Glinsvad, V Haikola, HA Hansen, et al. Electronic structure calculations
with GPAW: a real-space implementation of the projector augmented-wave method. Journal of
physics: Condensed matter , 22(25):253202, 2010. 4.7
[91] Wenhui Mi, Xuecheng Shao, Chuanxun Su, Yuanyuan Zhou, Shoutao Zhang, Quan Li, Hui Wang,
Lijun Zhang, Maosheng Miao, Yanchao Wang, et al. ATLAS: A real-space finite-difference im-
plementation of orbital-free density functional theory. Computer Physics Communications , 200:
87â€“95, 2016. 4.7
[92] Xuecheng Shao, Kaili Jiang, Wenhui Mi, Alessandro Genova, and Michele Pavanello. DFTpy:
An efficient and object-oriented platform for orbital-free dft simulations. Wiley Interdisciplinary
Reviews: Computational Molecular Science , 11(1):e1482, 2021. 4.7
[93] He Zhang, Siyuan Liu, Jiacheng You, Chang Liu, Shuxin Zheng, Ziheng Lu, Tong Wang, Nanning
Zheng, and Bin Shao. Overcoming the barrier of orbital-free density functional theory in molecular
systems using deep learning, Feb 2024. URL https://doi.org/10.6084/m9.figshare.c.
6877432 . 4.9
[94] He Zhang, Siyuan Liu, Jiacheng You, Chang Liu, Shuxin Zheng, Ziheng Lu, Tong Wang, Nan-
ning Zheng, and Bin Shao. Overcoming the Barrier of Orbital-Free Density Functional Theory in
Molecular Systems Using Deep Learning, February 2024. URL https://doi.org/10.5281/
zenodo.10616893 . 4.9
[95] Mel Levy. Universal variational functionals of electron densities, first-order density matrices, and
natural spin-orbitals and solution of the v-representability problem. Proceedings of the National
Academy of Sciences , 76(12):6062â€“6065, 1979. A.1
[96] Elliott H Lieb. Density functionals for Coulomb systems. International Journal of Quantum
Chemistry , 24(3):243â€“277, 1983. 2, 3, A.1, B.4.1, D.1.1
[97] Frank W. Bobrowicz and William A. Goddard. The Self-Consistent Field Equations for Generalized
Valence Bond and Open-Shell Hartree-Fock Wave Functions , pages 79â€“127. Springer US, Boston,
MA, 1977. ISBN 978-1-4757-0887-5. doi: 10.1007/978-1-4757-0887-5 4. URL https://doi.
org/10.1007/978-1-4757-0887-5\_4 . A.1
[98] Ira N Levine, Daryle H Busch, and Harrison Shull. Quantum chemistry , volume 6. Pearson Prentice
Hall Upper Saddle River, NJ, 2009. A.1, A.5
[99] SM Blinder. Basic concepts of self-consistent-field theory. American journal of physics , 33(6):
431â€“443, 1965. A.2
22

[100] P. Pulay. Improved scf convergence acceleration. Journal of Computational Chemistry , 3(4):556â€“
560, 1982. doi: https://doi.org/10.1002/jcc.540030413. URL https://onlinelibrary.wiley.
com/doi/abs/10.1002/jcc.540030413 . A.2, A.3.1
[101] Konstantin N. Kudin, Gustavo E. Scuseria, and Eric Canc `es. A black-box self-consistent field
convergence algorithm: One step closer. The Journal of Chemical Physics , 116(19):8255â€“8261,
04 2002. ISSN 0021-9606. doi: 10.1063/1.1470195. URL https://doi.org/10.1063/1.
1470195 . A.2, A.3.1
[102] Michel Dupuis, John Rys, and Harry F. King. Evaluation of molecular integrals over Gaussian
basis functions. The Journal of Chemical Physics , 65(1):111â€“116, 07 1976. ISSN 0021-9606. doi:
10.1063/1.432807. URL https://doi.org/10.1063/1.432807 . A.3.2
[103] J. Rys, M. Dupuis, and H. F. King. Computation of electron repulsion integrals using the rys
quadrature method. Journal of Computational Chemistry , 4(2):154â€“157, 1983. doi: https://doi.
org/10.1002/jcc.540040206. URL https://onlinelibrary.wiley.com/doi/abs/10.1002/
jcc.540040206 . A.3.2
[104] Qiming Sun. Libcint: An efficient general integral library for Gaussian basis functions. Journal of
computational chemistry , 36(22):1664â€“1671, 2015. A.3.2
[105] Atilim Gunes Baydin, Barak A Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind.
Automatic differentiation in machine learning: a survey. Journal of Marchine Learning Research ,
18:1â€“43, 2018. A.3.2
[106] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative
style, high-performance deep learning library. Advances in neural information processing systems ,
32, 2019. A.3.2, B.1, B.1.6
[107] John P Perdew, Kieron Burke, and Matthias Ernzerhof. Generalized gradient approximation made
simple. Physical review letters , 77(18):3865, 1996. A.3.2
[108] Chuin Wei Tan, Chris J. Pickard, and William C. Witt. Automatic differentiation for orbital-free
density functional theory. The Journal of Chemical Physics , 158(12):124801, 03 2023. ISSN
0021-9606. doi: 10.1063/5.0138429. URL https://doi.org/10.1063/5.0138429 . A.3.2
[109] Hans Hellman. Einf Â¨uhrung in die Quantenchemie. Franz Deuticke, Leipzig , 285, 1937. A.5, A.5
[110] R. P. Feynman. Forces in molecules. Phys. Rev. , 56:340â€“343, Aug 1939. doi: 10.1103/PhysRev.
56.340. URL https://link.aps.org/doi/10.1103/PhysRev.56.340 . A.5, A.5
[111] Peter Pulay. Ab initio calculation of force constants and equilibrium geometries in polyatomic
molecules: I. Theory. Molecular Physics , 17(2):197â€“204, 1969. A.5, A.5, A.5
[112] Robert G Parr and Weitao Yang. Density-functional theory of atoms and molecules. 1989. A.6
[113] Jacob Hollingsworth, Li Li, Thomas E Baker, and Kieron Burke. Can exact conditions improve
machine-learned density functionals? The Journal of chemical physics , 148(24):241743, 2018.
A.6, C
[114] Bhupalee Kalita, Li Li, Ryan J McCarty, and Kieron Burke. Learning to approximate density
functionals. Accounts of Chemical Research , 54(4):818â€“826, 2021. A.6, C
[115] Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are
universal approximators. Neural networks , 2(5):359â€“366, 1989. B.1.3
[116] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (GELUs). arXiv preprint
arXiv:1606.08415 , 2016. B.1.3
[117] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450 , 2016. B.1.4
[118] Feng Changyong, Wang Hongyue, Lu Naiji, Chen Tian, He Hua, Lu Ying, et al. Log-
transformation and its implications for data analysis. Shanghai archives of psychiatry , 26(2):105,
2014. B.3
23

[119] Albert Musaelian, Simon Batzner, Anders Johansson, Lixin Sun, Cameron J Owen, Mordechai
Kornbluth, and Boris Kozinsky. Learning local equivariant representations for large-scale atomistic
dynamics. Nature Communications , 14(1):579, 2023. B.3.3
[120] Susi Lehtola. Assessment of initial guesses for self-consistent field calculations. Superposition
of atomic potentials: Simple yet efficient. Journal of chemical theory and computation , 15(3):
1593â€“1604, 2019. B.5.2, D.1.1
[121] Mohammed Alghadeer, Abdulaziz Al-Aswad, and Fahhad H Alharbi. Highly accurate machine
learning model for kinetic energy density functional. Physics Letters A , 414:127621, 2021. C
[122] Shashikant Kumar, Edgar Landinez Borda, Babak Sadigh, Siya Zhu, Sebastian Hamel, Brian Gal-
lagher, Vasily Bulatov, John Klepeis, and Amit Samanta. Accurate parameterization of the kinetic
energy functional. The Journal of Chemical Physics , 156(2):024110, 2022. C
[123] Pavlo Golub and Sergei Manzhos. Kinetic energy densities based on the fourth order gradient
expansion: performance in different classes of materials and improvement via machine learning.
Phys. Chem. Chem. Phys. , 21:378â€“395, 2019. doi: 10.1039/C8CP06433D. URL http://dx.
doi.org/10.1039/C8CP06433D . C
[124] Kevin Ryczko, Sebastian J Wetzel, Roger G Melko, and Isaac Tamblyn. Orbital-free density func-
tional theory with small datasets and deep learning. arXiv preprint arXiv:2104.05408 , 2021. C
[125] Landrum Greg, Tosco Paolo, Kelley Brian, Ric, and Cosgrove David. RDKit: Open-source chem-
informatics. 2013. doi: 10.5281/zenodo.591637. URL https://www.rdkit.org . D.2.2
[126] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.
arXiv preprint arXiv:2004.05150 , 2020. D.3
[127] Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan, and Jianfeng
Gao. Focal self-attention for local-global interactions in vision transformers. arXiv preprint
arXiv:2107.00641 , 2021. D.3
Acknowledgements
We thank Paola Gori Giorgi, William Chuck Witt, Sebastian Ehlert, Zun Wang, Lixue Cheng, Jan Her-
mann and Ziteng Liu for insightful discussions and constructive feedback; Xingheng He and Yaosen Min
for suggestions on protein preprocessing; Han Yang for help with trying other OFDFT software; Yu Shi
for suggestions and feedback on model design and optimization; and Jingyun Bai for help with figure
design. We received no specific funding for this work. He Zhang, Siyuan Liu and Jiacheng You did this
work during an internship at Microsoft Research AI4Science.
Author Contributions
C.L. led the research under the support from N.Z. and B.S. C.L. is the lead contact. C.L., S.Z. and B.S.
conceived the project. S.L., C.L., H.Z. and J.Y . deduced and designed data generation methods, enhance-
ment modules, training pipeline, and density optimization. H.Z., S.Z. and J.Y . designed and implemented
the deep learning model. H.Z. and S.L. conducted the experiments. Z.L. and T.W. contributed to the
experiment design and evaluation protocol. C.L., H.Z., S.L. and S.Z. wrote the paper with inputs from all
authors.
Competing Interests
C.L., S.Z. and B.S. have filed a patent on M-OFDFT (application number: PCT/CN2023/112628). The
other authors declare no competing interests.
24

Supplementary Information
Contents
1 Introduction 1
2 Results 3
2.1 Workflow of M-OFDFT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
2.2 Performance of M-OFDFT on Molecular Systems . . . . . . . . . . . . . . . . . . . . . 4
2.3 Extrapolation of M-OFDFT to Larger-Scale Molecules . . . . . . . . . . . . . . . . . . 5
2.4 Empirical Time Complexity of M-OFDFT . . . . . . . . . . . . . . . . . . . . . . . . . 8
3 Discussion 8
4 Methods 9
4.1 Training the KEDF Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
4.2 Geometric Invariance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
4.3 Enhancement Modules for Vast Gradient Range . . . . . . . . . . . . . . . . . . . . . . 11
4.4 Density Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
4.5 Dataset Preparation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
4.5.1 Ethanol . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
4.5.2 QM9 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
4.5.3 QMugs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
4.5.4 Chignolin . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
4.6 Data Generation Configurations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
4.7 Classical OFDFT Methods Implementation . . . . . . . . . . . . . . . . . . . . . . . . 15
4.8 M-NNP and M-NNP-Den Implementation . . . . . . . . . . . . . . . . . . . . . . . . . 16
4.9 Curve Fitting Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
A Mechanism of Density Functional Theory 27
A.1 Basic Formulation of Density Functional Theory . . . . . . . . . . . . . . . . . . . . . 27
A.2 KSDFT Calculation Produces Labels of KEDF . . . . . . . . . . . . . . . . . . . . . . 30
A.3 Formulation under Atomic Basis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
A.3.1 KSDFT under Atomic Basis . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
A.3.2 OFDFT under Atomic Basis . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
A.4 Label Calculation under Atomic Basis . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
A.4.1 Density Fitting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
A.4.2 Value Label Calculation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
A.4.3 Gradient Label Calculation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
A.5 Force Calculation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
A.6 Scaling Property under Atomic Basis . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
B M-OFDFT Technical Details 44
B.1 Model Specification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
B.1.1 Density Basis and Coefficient Specification . . . . . . . . . . . . . . . . . . . . 46
B.1.2 Gaussian Basis Function (GBF) Module . . . . . . . . . . . . . . . . . . . . . . 46
B.1.3 NodeEmbedding Module . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
B.1.4 Graphormer-3D (G3D) Module . . . . . . . . . . . . . . . . . . . . . . . . . . 48
B.1.5 Output Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
B.1.6 Model Configuration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
B.1.7 Training Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
B.2 Local Frame Module for Geometric Invariance . . . . . . . . . . . . . . . . . . . . . . 51
B.3 Enhancement Modules for Expressing Vast Gradient Range . . . . . . . . . . . . . . . . 52
B.3.1 Dimension-wise Rescaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
B.3.2 Natural Reparameterization . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
B.3.3 Atomic Reference Module . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
B.4 Functional Variants . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
B.4.1 Residual KEDF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
B.4.2 TXC Functional . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
B.4.3 Learning Other Functionals . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
B.5 Density Optimization Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
25

B.5.1 Additional Density Optimization Results . . . . . . . . . . . . . . . . . . . . . 55
B.5.2 Density Initialization Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
B.5.3 Stopping Criterion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
B.5.4 Density Optimization Hyperparameters . . . . . . . . . . . . . . . . . . . . . . 58
C Related Work 58
D Additional Empirical Results 59
D.1 In-Scale Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
D.1.1 Quantitative Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
D.1.2 Visualization of Optimized Densities . . . . . . . . . . . . . . . . . . . . . . . 60
D.2 Extrapolation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
D.2.1 Results on QMugs and Chignolin . . . . . . . . . . . . . . . . . . . . . . . . . 61
D.2.2 Additional Extrapolation Study from QM9 . . . . . . . . . . . . . . . . . . . . 62
D.3 Empirical Time Cost Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
D.4 Ablation Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
D.4.1 Multi-Step Data and Gradient Label . . . . . . . . . . . . . . . . . . . . . . . . 64
D.4.2 Non-locality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
D.4.3 Local Frame and Enhancement Modules . . . . . . . . . . . . . . . . . . . . . . 65
D.4.4 Results Using Other Training Strategies . . . . . . . . . . . . . . . . . . . . . . 66
26

Supplementary Section A Mechanism of Density Functional Theory
In this section, we introduce details in relevant theory on DFT, including the formulation of OFDFT
under atomic basis, and the mechanism and details to use KSDFT to generate value and gradient data for
learning KEDF. Atomic units are used through out the paper. Notations used in the main text and the
supplementary text are listed in Supplementary Table S1.
Supplementary Section A.1 Basic Formulation of Density Functional Theory
For brevity, the following formulation is for spinless fermions therefore only consider spacial states. For
the restricted Kohn-Sham calculation we adopt for data generation, a pair of electrons of opposite spins
share a common spacial orbital, which amounts to duplicate the orbitals in the following formulation.
The mechanism of DFT may be more intuitively introduced under Levyâ€™s constrained search formula-
tion [95]. The N-electron Schr Â¨odinger equation for ground state is equivalent to the following optimiza-
tion problem on N-electron wavefunctions Ïˆ(r(1),Â·Â·Â·,r(N))under the variational principle:1
Eâ‹†= min
Ïˆ:antisym ,âŸ¨Ïˆ|ÏˆâŸ©=NâŸ¨Ïˆ|Ë†T+Ë†Vee+Ë†Vext|ÏˆâŸ©, (S1)
where Ë†T:=âˆ’1
2PN
i=1âˆ‡(i)2is the kinetic operator, Ë†Vee:=P
1â©½i<jâ©½N1
âˆ¥r(i)âˆ’r(j)âˆ¥is the electron-electron
Coulomb interaction (internal potential), and Ë†Vext:=PN
i=1Vext(r(i))comes from a one-body external
potential Vext(r)that commonly arises from the electrostatic field of the nuclei specified by the given
molecular structure M={X,Z}where X:= (x(1),Â·Â·Â·,x(A))andZ:= (Z(1),Â·Â·Â·, Z(A)):
Vext(r) =âˆ’AX
a=1Z(a)
râˆ’x(a). (S2)
Although the optimization problem is exactly defined, directly optimizing the N-electron wavefunction
is very challenging computationally. Specifying the wavefunction Ïˆand evaluating the energy already
require an exponential cost in principle, as Ïˆis a function on R3Nwhose dimension increases with N.
To make an easier optimization problem, it is then desired to optimize a functional of the one-electron
reduced density,
Ï[Ïˆ](r) :=NZ
|Ïˆ(r,r(2),Â·Â·Â·,r(N))|2dr(2)Â·Â·Â·dr(N), (S3)
which has an intuitive physical interpretation of charge density even under a classical view, and more
importantly, the cost to specify a density is constant (with respect to N) in principle, as the density is
a function on R3whose dimension is constant. This is the starting point of density functional theory
(DFT) [4â€“6], and is first formally verified by Hohenberg and Kohn [7].
In terms of the density, the external potential energy, that is, the last term in Eq. (S1), is already an explicit
density functional, since the external potential is one-body:
âŸ¨Ïˆ|Ë†Vext|ÏˆâŸ©=Z
Ï[Ïˆ](r)Vext(r) dr=Eext[Ï[Ïˆ]],where Eext[Ï] :=Z
Ï(r)Vext(r) dr. (S4)
For the other energy terms, using the density as an intermediate, the optimization problem in Eq. (S1) can
be equivalently2carried out in two levels:
Eâ‹†= min
Ï:â©¾0,R
Ï(r) dr=N
min
Ïˆ:antisym ,Ï[Ïˆ]=ÏâŸ¨Ïˆ|Ë†T+Ë†Vee|ÏˆâŸ©
+Eext[Ï] (S5)
= min
Ï:â©¾0,R
Ï(r) dr=N
E[Ï] :=U[Ï] +Eext[Ï]
. (S6)
Here, the result of the first-level optimization problem carrying out a constrained search in Eq. (S5) defines
a density functional,
U[Ï] := min
Ïˆ:antisym ,Ï[Ïˆ]=ÏâŸ¨Ïˆ|Ë†T+Ë†Vee|ÏˆâŸ©, (S7)
1In this paper we only consider real-valued wavefunctions (and subsequently real-valued orbitals), since we only
need to solve for the ground state of stationary Schr Â¨odinger equation without spin-orbit interaction, for which the
Hamiltonian operator Ë†H:=Ë†T+Ë†Vee+Ë†Vextis Hermitian and real.
2The correspondence between the optimization space of Ïand the optimization space of Ïˆto allow this equiva-
lence is analyzed by Lieb [96].
27

Supplementary Table S1: Main notations. These notations are used consistently throughout the main
text and the supplementary text.
Basic concepts
râˆˆR3Electron coordinate
N Number of electrons in a molecular system
Ïˆ(r(1),Â·Â·Â·,r(N)) N-electron wavefunction
âŸ¨f|gâŸ©:=R
f(r)g(r) dr The standard inner product in function space
âŸ¨f|Ë†O|gâŸ©:=R
f(r)(Ë†Og)(r) dr Function-space inner product with operator
(f|g) :=Rf(r)g(râ€²)
âˆ¥râˆ’râ€²âˆ¥drdrâ€²Coulomb integral
Molecular system
xâˆˆR3Atom coordinates
a, bâˆˆ {1,2,Â·Â·Â·, A} Indices for atoms in a molecule
X:={x(a)}A
a=1 Molecular conformation/geometry
Z:={Z(a)}A
a=1 Molecular composition
M:={X,Z} Molecular structure
{M(d)}D
d=1 Molecular structures in a dataset
Density functional theory
Î¦ :={Ï•i(r)}N
i=1 Orbitals
i, jâˆˆ {1,2,Â·Â·Â·, N} Indices for orbitals or electrons
Ïˆ[Î¦](r(1),Â·Â·Â·,r(N)) := det[ Ï•i(r(j))]ij Slater determinant from orbitals Î¦ ={Ï•i(r)}N
i=1
{Î·Î±(r)}B
Î±=1 Orbital basis
Subscripts Î±, Î², Î³, Î´ âˆˆ {1,2,Â·Â·Â·, B}Indices for orbital basis
CÎ±i Orbital coefficients
Î“Î±Î²:=PN
i=1CÎ±iCÎ²i Density matrix
SÎ±Î²:=âŸ¨Î·Î±|Î·Î²âŸ© Overlap matrix of orbital basis
DÎ±Î²,Î³Î´ :=âŸ¨Î·Î±Î·Î²|Î·Î³Î·Î´âŸ© Overlap matrix of paired orbital basis
ËœDÎ±Î²,Î³Î´ := (Î·Î±Î·Î²|Î·Î³Î·Î´) 4-center-2-electron Coulomb integral of orbital basis
Ï(r) (One-electron reduced) density (function)
{Ï‰Âµ(r)}M
Âµ=1 Density basis
Subscripts Âµ, Î½âˆˆ {1,2,Â·Â·Â·, M} Indices for density basis
Subscript Âµ= (a, Ï„) Atom assignment decomposition of basis index
pÂµ Density coefficient
wÂµ:=R
Ï‰Âµ(r) dr Density basis normalization vector
WÂµÎ½:=âŸ¨Ï‰Âµ|Ï‰Î½âŸ© Overlap matrix of density basis
ËœWÂµÎ½:= (Ï‰Âµ|Ï‰Î½) 2-center-2-electron Coulomb integral of density basis
LÂµ,Î±Î²:=âŸ¨Ï‰Âµ|Î·Î±Î·Î²âŸ© Overlap matrix between density basis and orbital basis
F Fock matrix in KSDFT
k Step index for SCF iteration or density optimization process
(â‹†for the converged step)
(Veff)Î±Î²:=âŸ¨Î·Î±|Veff|Î·Î²âŸ© Effective potential matrix under orbital basis
(veff)Âµ:=âŸ¨Ï‰Âµ|VeffâŸ© Effective potential vector under density basis
ÂµâˆˆR Chemical potential
Îµ:= Diag[ Îµ1,Â·Â·Â·, ÎµN] The diagonal NÃ—Nmatrix of orbital energies
U[Ï] Universal functional
EXC[Ï] Exchange-correlation (XC) functional
TS[Ï] Kinetic energy density functional (KEDF)
TS(p,M) KEDF under atomic basis of density
TS,Î¸(p,M) KEDF model/approximation
TAPBE The APBE kinetic functional as the base functional TS,base
TS,res Residual KEDF on top of the base functional TS,base
ETXC Kinetic and XC functional
f Hellmann-Feynman force
28

called the universal functional, as it is independent of system specification (that is, MorVext). It is
composed of the kinetic and internal potential energy of the electrons. The optimization objective is then
formally converted to a density functional E[Ï]as shown in Eq. (S6).
To carry out practical computation, variants of the kinetic and internal potential energy that allow explicit
calculation or have known properties are introduced, to cover the major part of the corresponding energies
inU[Ï]. The internal potential energy is covered by its classical version, that is assuming no correlation,
called the Hartree energy:
EH[Ï] :=1
2ZÏ(r)Ï(râ€²)
âˆ¥râˆ’râ€²âˆ¥drdrâ€². (S8)
The kinetic energy is covered by the kinetic energy density functional (KEDF), which is defined in a
similar way as the universal functional:
TS[Ï] := min
Ïˆ:antisym ,Ï[Ïˆ]=ÏâŸ¨Ïˆ|Ë†T|ÏˆâŸ©. (S9)
The remainder in the universal functional is called the exchange-correlation (XC) functional :
EXC[Ï] :=U[Ï]âˆ’TS[Ï]âˆ’EH[Ï],
which is by definition also a density functional. Under this decomposition, the density optimization
problem in Eq. (S6) becomes:
Eâ‹†= min
Ï:â©¾0,R
Ï(r) dr=N
E[Ï] =TS[Ï] +EH[Ï] +EXC[Ï] +Eext[Ï]| {z }
=:Eeff[Ï]
. (S10)
Here Eeff[Ï]is defined for future convenience and denoted after the effective-potential interpretation of
its variation detailed later. Using carefully designed explicit expressions or machine-learning models
to approximate the TS[Ï]andEXC[Ï]functionals, practical computation can be conducted. This is the
formulation of orbital-free density functional theory (OFDFT). Indeed, the object to be optimized is the
electron density, which is one function on the constant-dimensional space of R3, and hence greatly reduces
computation complexity over the original variational problem Eq. (S1). Under properly designed TS[Ï]
andEXC[Ï]approximations, the complexity is favorably O(N2)under atomic basis.
Considering the KEDF TS[Ï]is more challenging to approximate than the XC functional EXC[Ï], Kohn
and Sham [3] leverage an equivalent formulation of KEDF to allow its accurate calculation, at the cost of
increasing the complexity. The alternative formulation optimizes determinantal wavefunctions. A deter-
minantal wavefunction for Nelectrons is specified by None-electron wavefunctions Î¦ :={Ï•i(r)}N
i=1,
called orbitals, following the form:
Ïˆ[Î¦](r(1),Â·Â·Â·,r(N)) :=1âˆš
N!det[Ï•i(r(j))]ij,given orbitals Î¦ :={Ï•i(r)}N
i=1.
The equivalent optimization problem in parallel with Eq. (S9) is:3
TS[Ï] = min
{Ï•i}N
i=1:Ï[Î¦]=ÏâŸ¨Ïˆ[Î¦]|Ë†T|Ïˆ[Î¦]âŸ©= min
{Ï•i}N
i=1:orthonormal ,
Ï[Î¦]=ÏNX
i=1âŸ¨Ï•i|Ë†T|Ï•iâŸ©, (S11)
where Ï[Î¦](r) :=NX
i=1|Ï•i(r)|2,given orthonormal orbitals Î¦ :={Ï•i(r)}N
i=1. (S12)
In Eq. (S11), the second equality holds since the density normalizes to N, and a set of (non-
collinear) functions can always be orthogonalized, for example, using the Gram-Schmidt process ([97,
Sec. 3.1.4]; [98, Sec. 14.3]). This equivalence can be understood from the interpretation of TS[Ï]as the
non-interacting portion of kinetic energy. Indeed, for a non-interacting system, there are only kinetic
energy and external potential energy (that is, taking Ë†Vee= 0), so the two-level optimization in parallel
3The equivalence on defining TS[Ï]by Eq. (S9) and Eq. (S11) is known to be guaranteed if the density Ïcomes
from the ground state of a non-interacting system which is non-degenerate [96, Thm. 4.6]. Even there exists a density
Ïthat makes the determinantally-defined TS[Ï]by Eq. (S11) different [96, Thm. 4.8], the determinantally-defined
TS[Ï]still gives the right ground-state energy in density optimization (up to a closure) [96, Thm. 4.9].
29

with Eq. (S5) becomes:
Eâ‹†= min
Ïˆ:antisym ,âŸ¨Ïˆ|ÏˆâŸ©=1âŸ¨Ïˆ|Ë†T+Ë†Vext|ÏˆâŸ©
= min
Ï:â©¾0,R
Ï(r) dr=N
min
Ïˆ:antisym ,Ï[Ïˆ]=ÏâŸ¨Ïˆ|Ë†T|ÏˆâŸ©
+Eext[Ï]
= min
Ï:â©¾0,R
Ï(r) dr=NTS[Ï] +Eext[Ï], (S13)
from which we see that TS[Ï]is the ground-state kinetic energy of the non-interacting system whose
ground-state density is Ï. On the other hand, it is known that the ground-state wavefunction of a non-
interacting system is commonly determinantal (at least when the ground state is non-degenerate [96,
Thm. 4.6]). Hence the optimization can be rewritten as:
Eâ‹†= min
{Ï•i}N
i=1âŸ¨Ïˆ[Î¦]|Ë†T+Ë†Vext|Ïˆ[Î¦]âŸ©
= min
Ï:â©¾0,R
Ï(r) dr=N
min
{Ï•i}N
i=1:Ï[Î¦]=ÏâŸ¨Ïˆ[Î¦]|Ë†T|Ïˆ[Î¦]âŸ©
+Eext[Ï]
= min
Ï:â©¾0,R
Ï(r) dr=N
min
{Ï•i}N
i=1:orthonormal ,
Ï[Î¦]=ÏNX
i=1âŸ¨Ï•i|Ë†T|Ï•iâŸ©
+Eext[Ï], (S14)
which indicates Eq. (S11).
Back to the main problem, leveraging this knowledge about TS[Ï]in the variational problem Eq. (S10)
gives:
Eâ‹†= min
Ï:â©¾0,R
Ï(r) dr=N
min
{Ï•i}N
i=1:orthonormal ,
Ï[Î¦]=ÏNX
i=1âŸ¨Ï•i|Ë†T|Ï•iâŸ©
+EH[Ï] +EXC[Ï] +Eext[Ï],
which can be converted into directly optimizing the orbitals in a single-level optimization:
Eâ‹†= min
{Ï•i}N
i=1:orthonormal
E[Î¦] :=NX
i=1âŸ¨Ï•i|Ë†T|Ï•iâŸ©+EH[Ï[Î¦]] +EXC[Ï[Î¦]] +Eext[Ï[Î¦]]| {z }
Eeff[Ï[Î¦]]
. (S15)
This is the formulation of Kohn-Sham density functional theory (KSDFT). With decades of develop-
ment of XC functional approximations, KSDFT has achieved remarkable success and becomes among
the most popular quantum chemistry method. In its formulation, the object to be optimized is a set of
orbitals {Ï•i(r)}N
i=1, which are Nfunctions on R3. This is still substantially cheaper than optimizing an
N-electron wavefunction on R3N, but has a complexity at least O(N)times more than OFDFT, due to
Ntimes more R3functions to optimize. Under atomic basis, KSDFT has a complexity at least O(N3)
(using density fitting) without further approximations. In this triumphant era of deep machine learning,
approximating a complicated functional is not as challenging as before. Powerful deep learning mod-
els create the opportunity to approximate KEDF accurately enough to match successful XC functional
approximations. This would enable accurate and practical OFDFT calculation, unleashing its power of
lower complexity to push the accuracy-efficiency trade-off in quantum chemistry.
Supplementary Section A.2 KSDFT Calculation Produces Labels of KEDF
We now explain why a KSDFT calculation procedure could provide value and gradient labels of KEDF.
Computation details under atomic basis are postponed in Supplementary Section A.4. We start by describ-
ing the typical algorithm to solve the optimization problem in KSDFT. To determine the optimal solution
of orbitals Î¦ :={Ï•i(r)}N
i=1, the variation of the energy functional E[Î¦]in Eq. (S15) with respect to each
orbital Ï•iis required:
Î´E[Î¦]
Î´Ï•i(r) =Î´PN
j=1(âˆ’1/2)âŸ¨Ï•j|âˆ‡2|Ï•jâŸ©
Î´Ï•i(r) +ZÎ´Eeff[Ï[Î¦]]
Î´Ï(râ€²)Î´Ï[Î¦](râ€²)
Î´Ï•i(r)drâ€²
= 2Ë†TÏ•i(r) + 2Veff[Ï[Î¦]](r)Ï•i(r), (S16)
where Veff[Ï](r) :=Î´Eeff[Ï]
Î´Ï(r) =ZÏ(râ€²)
âˆ¥râ€²âˆ’râˆ¥drâ€²
|{z }
=:VH[Ï](r)+Î´EXC[Ï]
Î´Ï(r)
|{z}
=:VXC[Ï](r)+Vext(r). (S17)
30

The term Veff[Ï[Î¦]]arises as a variation with respect to the density Ïsince the orbitals affect the energy com-
ponent Eeffapart from TS(defined in Eq. (S10)) only through the density Ï[Î¦]they define. By Eq. (S12)
we haveÎ´Ï[Î¦](râ€²)
Î´Ï•i(r)= 2Ï•i(r)Î´(râˆ’râ€²), which then gives Eq. (S16). Combining Eq. (S16) with the variation
of the orthonormal constraint yields the optimality equation for the problem Eq. (S15):
Ë†F[Ï[Î¦]]Ï•i:=Ë†TÏ•i+Veff[Ï[Î¦]]Ï•i=ÎµiÏ•i,âˆ€i= 1,Â·Â·Â·N. (S18)
In the derivation, only the Lagrange multipliers Îµifor the normalization constraints âŸ¨Ï•i|Ï•iâŸ©= 1 are
imposed, since from the resulting equations (S18), {Ï•i}N
i=1are eigenstates of an Hermitian operator
Ë†F[Ï[Î¦]]called the Fock operator, and hence are naturally orthogonal in the general case of non-degeneracy.
These equations resemble the Schr Â¨odinger equation for Nnon-interacting fermions, where Veff[Ï[Î¦]](r),
as a function on R3, acts as an effective one-body external potential, hence the name.
Note that Veff[Ï[Î¦]]is unknown beforehand, as itself depends on the solution of orbitals. Hence a fixed-
point iteration is employed: starting from a set of initial orbitals Î¦(0):={Ï•(0)
i}N
i=1, construct the Fock
operator using results in previous iterations,
Ë†F(k):=Ë†T+V(k)
eff, (S19)
where V(k)
effis taken as Veff[Ï[Î¦(kâˆ’1)]]following this derivation, that is, Ë†F(k)=Ë†F[Ï[Î¦(kâˆ’1)]], and solve the
corresponding eigenvalue problem for the orbitals in the current iteration:
Ë†F(k)Ï•(k)
i=Îµ(k)
iÏ•(k)
i,âˆ€i= 1,Â·Â·Â·, N. (S20)
The iteration stops until â€œself-consistencyâ€ is achieved, that is, the eigenstate solution Î¦(k):={Ï•(k)
i}i
in the current step coincides (up to an acceptable error) with the orbitals Î¦(kâˆ’1):={Ï•(kâˆ’1)
i}N
i=1in the
previous step that define Ë†F(k). This is the self-consistent field (SCF) method [99].
An important fact of SCF is that, in each iteration k, the solution {Ï•(k)
i}N
i=1exactly defines the ground-
state of a non-interacting system of Nfermions moving in the effective one-body potential V(k)
effas the
external potential Vext. Indeed, the variational problem Eq. (S14) that describes the non-interacting system
can be reformulated into a single-level optimization as:
Eâ‹†= min
{Ï•i}N
i=1:orthonormalNX
i=1âŸ¨Ï•i|Ë†T|Ï•iâŸ©+Z
Ï[Î¦](r)V(k)
eff(r) dr, (S21)
whose variation coincides with Eq. (S20) thus solved by {Ï•(k)
i}N
i=1. This reveals the relation of SCF
solution to the KEDF: this solution of orbitals {Ï•(k)
i}N
i=1achieves the minimum non-interacting kinetic
energyPN
i=1âŸ¨Ïˆ[Î¦]|Ë†T|Ïˆ[Î¦]âŸ©among all orthonormal orbitals that lead to the same density Ï[Î¦(k)](otherwise
Eq. (S21) can be further minimized; can also be seen from the equivalence to Eq. (S14)); by the alternative
form of KEDF Eq. (S11) as non-interacting ground-state kinetic energy, we thus have:
TS[Ï[Î¦(k)]] =âŸ¨Ïˆ[Î¦(k)]|Ë†T|Ïˆ[Î¦(k)]âŸ©=NX
i=1âŸ¨Ï•(k)
i|Ë†T|Ï•(k)
iâŸ©. (S22)
This indicates that every SCF iteration produces a label for TS. Moreover, as the non-interacting vari-
ation problem Eq. (S21) is equivalent to its two-level optimization form Eq. (S14), which is in turn
equivalent to the density optimization form using KEDF Eq. (S13) (which explains the alternative KEDF
form Eq. (S11)), the density Ï[Î¦(k)]from the solution Î¦(k):={Ï•(k)
i}N
i=1of each SCF iteration minimizes
Eq. (S13). Therefore, it satisfies the variation equation (Euler equation) of Eq. (S13) (taking VextasV(k)
eff)
subject to the normalization constraint with Lagrange multiplier (chemical potential) Âµ(k):
Î´TS[Ï[Î¦(k)]]
Î´Ï+V(k)
eff=Âµ(k). (S23)
The variation of KEDFÎ´TS
Î´Ïis related to the gradient with respect to density coefficients when the density
Ïis expanded on a basis (see Supplementary Section A.4.3). Hence, every SCF iteration also produces a
label for the gradient of TS, up to a projection .
31

It is worth noting that these arguments still hold when the effective potential V(k)
effin SCF iteration kis
notVeff[Ï[Î¦(kâˆ’1)]], since the deductions from Eq. (S21) to Eq. (S23) only require V(k)
effto be a one-body
potential. This allows more flexible data generation process since in common DFT calculation settings,
V(k)
effindeed deviates from Veff[Ï[Î¦(kâˆ’1)]]for more stable and faster convergence, for example when using
the â€œdirect inversion in the iterative subspaceâ€ (DIIS) method [100, 101]. This also indicates that even
when the XC functional used in data generation is not accurate, the generated value and gradient labels
forTS[Ï]are still exact, since the XC functional still gives an effective one-body potential to define the
non-interacting system Eq. (S20) or Eq. (S21), as long as it is pure (that is, only depends on density
features). In this sense, data generation for KEDF is easier than that for the XC functional.
Supplementary Section A.3 Formulation under Atomic Basis
For practical calculation, KSDFT typically uses an atomic basis {Î·Î±(r)}B
Î±=1to expand the orbitals for
conducting the SCF iteration in Eq. (S20) for molecular systems. The expansion gives:
Ï•i(r) =X
Î±CÎ±iÎ·Î±(r), (S24)
which converts solving for eigenfunctions into the common problem of solving for eigenvectors of a
matrix. On the other hand, as emphasized in Introduction 1 and Results 2.1, we also hope to represent the
density on an atomic basis {Ï‰Âµ(r)}M
Âµ=1for efficient OFDFT implementation,
Ï(r) =X
ÂµpÂµÏ‰Âµ(r). (S25)
The left-hand-sides of Eq. (S24) and Eq. (S25) may also be denoted as Ï•i,CorÎ¦CandÏpto highlight the
dependency on the coefficients. Note that both the orbital basis {Î·Î±(r)}B
Î±=1and density basis {Ï‰Âµ(r)}M
Âµ=1
depend on the molecular structure M, as the location and type of each basis function is determined by
the coordinates x(a)and atomic number Z(a)of the corresponding atom. Nevertheless, the development
in this subsection is for one given molecular system M, so we omit its appearance for density or orbital
representation. Typically, the numbers of basis BandMincrease linearly with the number of electrons
N, that is, O(B) =O(M) =O(N).
Supplementary Section A.3.1 KSDFT under Atomic Basis
For the SCF iteration in Eq. (S20), using the expansion of orbitals in Eq. (S24), it becomes:P
Î²C(k)
Î²iË†F(k)Î·Î²(r) =Îµ(k)
iP
Î²C(k)
Î²iÎ·Î²(r),âˆ€i= 1,Â·Â·Â·, N. Integrating each function equation with
basis function Î·Î±(r)gives:P
Î²C(k)
Î²iâŸ¨Î·Î±|Ë†F(k)|Î·Î²âŸ©=Îµ(k)
iP
Î²C(k)
Î²iâŸ¨Î·Î±|Î·Î²âŸ©, which can then be formu-
lated as a generalized eigenvalue problem in matrix form:
F(k)C(k)=SC(k)Îµ(k), (S26)
where F(k)
Î±Î²:=âŸ¨Î·Î±|Ë†F(k)|Î·Î²âŸ©Eq. (S19)= âŸ¨Î·Î±|Ë†T|Î·Î²âŸ©|{z}
=âˆ’1
2R
Î·Î±(r)âˆ‡2Î·Î²(r) dr=:TÎ±Î²+âŸ¨Î·Î±|V(k)
eff|Î·Î²âŸ©|{z}
=:(V(k)
eff)Î±Î², (S27)
SÎ±Î²:=âŸ¨Î·Î±|Î·Î²âŸ©,Îµ(k):=ï£«
ï£­Îµ(k)
1
...
Îµ(k)
Nï£¶
ï£¸.
Here,F(k)is called the Fock matrix, and Sis the overlap matrix of the orbital basis.
To show the expression of the Fock matrix, we first give the expression of the density defined by the
orbital coefficients from Eq. (S12) and Eq. (S24):
ÏC(r) :=Ï[Î¦C](r) =NX
i=1X
Î±CÎ±iÎ·Î±(r)2
=NX
i=1X
Î±Î²CÎ±iCÎ²iÎ·Î±(r)Î·Î²(r)
=X
Î±Î²Î“Î±Î²Î·Î±(r)Î·Î²(r), (S28)
where we have defined the density matrix corresponding to the orbital coefficients:
Î“:=CCâŠ¤,Î“Î±Î²:=NX
i=1CÎ±iCÎ²i. (S29)
32

Note that Eq. (S12) requires orthonormal orbitals, and hence Eq. (S28) requires Cto satisfy the corre-
sponding orthonormality constraint shown in Eq. (S39) below. Orbital coefficient solutions C(k)in SCF
iterations satisfy this constraint as explained later.
In the derivation of SCF iteration, V(k)
effis taken as Veff[Ï[Î¦(kâˆ’1)]], that is, Ë†F(k)=Ë†F[Ï[Î¦(kâˆ’1)]]. This allows
explicit calculation of V(k)
effasVeffC(kâˆ’1)based on Eq. (S17) and F(k)asFC(kâˆ’1), for which we introduce
the following series of definitions:
FC:= [âŸ¨Î·Î±|Ë†F[ÏC]|Î·Î²âŸ©]Î±Î²=T+VeffC(Tdefined in Eq. (S27)) ,
VeffC:= [âŸ¨Î·Î±|Veff[ÏC]|Î·Î²âŸ©]Î±Î²=VHC+VXCC+Vext, (S30)
where (VHC)Î±Î²:=âŸ¨Î·Î±|VH[ÏC]|Î·Î²âŸ©Eqs. (S17, S28)=ZZ
Î·Î±(r)Î·Î²(r)P
Î³Î´Î“Î³Î´Î·Î³(râ€²)Î·Î´(râ€²)
âˆ¥râˆ’râ€²âˆ¥drâ€²dr
=X
Î³Î´ËœDÎ±Î²,Î³Î´Î“Î³Î´= (ËœDÂ¯Î“)Î±Î², (S31)
where ËœDÎ±Î²,Î³Î´ := (Î·Î±Î·Î²|Î·Î³Î·Î´),Â¯Î“is the vector of flattened Î“, (S32)
(VXCC)Î±Î²:=âŸ¨Î·Î±|VXC[ÏC]|Î·Î²âŸ©=Z
VXC[ÏC](r)Î·Î±(r)Î·Î²(r) dr,
(Vext)Î±Î²:=âŸ¨Î·Î±|Vext|Î·Î²âŸ©=âˆ’AX
a=1Z(a)ZÎ·Î±(r)Î·Î²(r)râˆ’x(a)dr.
In defining ËœD, we have used the notation of Coulomb integral for brevity:
(f|g) :=Zf(r)g(râ€²)
âˆ¥râˆ’râ€²âˆ¥drdrâ€². (S33)
In practice, integrals in S,T,Vext, and the Coulomb integral ËœDcan be calculated analytically under
Gaussian-Type Orbitals (GTO) as the basis {Î·Î±}B
Î±=1. The integral in VXCCis conducted numerically on
a quadrature grid, as typically used in a DFT calculation. After convergence, the electronic energy can be
calculated from the orbital coefficients Cby:
E(C) :=E[Î¦C]Eq. (S15)=TS(C) +EH(C) +EXC(C) +Eext(C)| {z }
=:Eeff(C), (S34)
where TS(C) :=NX
i=1âŸ¨Ï•i,C|Ë†T|Ï•i,CâŸ©=X
Î±Î²Î“Î±Î²TÎ±Î²=Â¯Î“âŠ¤Â¯T, (S35)
EH(C) :=EH[ÏC] =1
2X
Î±Î²Î³Î´Î“Î±Î²ËœDÎ±Î²,Î³Î´Î“Î³Î´=1
2Â¯Î“âŠ¤ËœDÂ¯Î“, (S36)
EXC(C) :=EXC[ÏC] =EXChX
Î±Î²Î“Î±Î²Î·Î±Î·Î²i
, (S37)
Eext(C) :=Eext[ÏC] =X
Î±Î²Î“Î±Î²(Vext)Î±Î²=Â¯Î“âŠ¤Â¯Vext, (S38)
where Â¯Î“,Â¯T,Â¯Vextare the vectors of flattened Î“,T,Vext, respectively. The term EXC[ÏC]is again calcu-
lated by numerically integrating the defined density by Con a quadrature grid.
Computational Complexity Note that the construction of VHCfrom Eq. (S31) and the evaluation of
EH(C)from Eq. (S36) require O(B4) =O(N4)complexity. Even when using density fitting which
decreases the complexity to O(N2), the complexity in each SCF iteration of KSDFT is O(N3)since the
complexity of density fitting itself is O(N3)(see Supplementary Section A.4.1).
Orbital Orthonormality Under the atomic basis, orbital orthonormality âŸ¨Ï•i|Ï•jâŸ©=Î´ijbecomesP
Î±Î²CÎ±iCÎ²jâŸ¨Î·Î±|Î·Î²âŸ©=Î´ij, or in matrix form,
CâŠ¤SC=I. (S39)
As mentioned after Eq. (S18), only the normalization constraints need to be taken care of, as the or-
bitals are eigenstates of an Hermitian operator and hence are already orthogonal if non-degenerate. This
33

property transmits to the matrix form of the problem:4
CâŠ¤
:iSC:i= 1,âˆ€i= 1,Â·Â·Â·, N. (S40)
Fulfilling the orthonormality constraint then only needs to normalize each eigenvector ËœC(k)
:iof the problem
in Eq. (S26) to form C(k); explicitly, C(k)
:i=ËœC(k)
:i/q
ËœC(k)
:iâŠ¤SËœC(k)
:i.
Relation to Direct Gradient Derivation The matrix form of the optimality equation under basis hence
the SCF iteration problem Eq. (S26) can also be derived directly from Eq. (S15) by taking the gradient of
the energy function of coefficients: E(C) := E[Î¦C] =E
{P
Î±CÎ±iÎ·Î±}N
i=1
. Its gradient is related to
the variation of the functional of orbitals by integral with the basis:
 
âˆ‡CE(C)
Î±i=ZÎ´E[Î¦C]
Î´Ï•i(r) 
âˆ‡CÏ•i,C(r)
Î±idr=ZÎ´E[Î¦C]
Î´Ï•i(r)Î·Î±(r) dr. (S41)
The variation is given by Eq. (S16), which is 2Ë†F[ÏÎ¦C]Ï•i,C(r) = 2P
Î²CÎ²iË†F[ÏÎ¦C]Î·Î²(r), which turns the
gradient into matrix form:
âˆ‡CE(C) = 2FCC. (S42)
For the orbital orthonormality constraint, as mentioned, only the normalization constraints require explicit
treatment. By introducing Lagrange multiplier Îµifor each constraint in Eq. (S40) and taking the gradient
for the corresponding Lagrange term gives âˆ‡CPN
i=1Îµi 
CâŠ¤
:iSC:iâˆ’1
= 2SCÎµ. This leads to the
optimality equation in matrix form:
FCC=SCÎµ. (S43)
By constructing the corresponding fixed-point iteration, Eq. (S26) is derived.
Accelerating and Stabilizing SCF Iteration As mentioned, F(k)andV(k)
effmay be taken differently
fromFC(kâˆ’1)andVeffC(kâˆ’1)for more stable and faster convergence. The direct inversion in the iterative
subspace (DIIS) method [100, 101] is a popular choice for this. In DIIS, the Fock matrix F(k)in the
eigenvalue problem Eq. (S20) for each SCF iteration kis taken as a weighted mixing of the vanilla Fock
matrices FC(kâ€²), kâ€²< kin previous steps:
F(k):=kâˆ’1X
kâ€²=0Ï€(k)
kâ€²FC(kâ€²),
where {Ï€(k)
kâ€²}kâˆ’1
kâ€²=0are the weights that are positive and normalizedPkâˆ’1
kâ€²=0Ï€(k)
kâ€²= 1. Due to the normal-
ization, the kinetic part Tof the matrix remains the same, so it agrees with the form in Eq. (S27) (or
Eq. (S19) in operator form), where:
V(k)
eff:=kâˆ’1X
kâ€²=0Ï€(k)
kâ€²VeffC(kâ€²). (S44)
Supplementary Section A.3.2 OFDFT under Atomic Basis
To solve the optimization problem of OFDFT in Eq. (S10), it is unnatural to construct a fixed-point SCF
iteration process from its variation in Eq. (S23). Hence, direct gradient-based density optimization is
conducted. For this, the energy functional of density function in Eq. (S10) needs to be converted into a
4This can also be directly verified in the matrix form: for iÌ¸=j,(CâŠ¤SC)ij=CâŠ¤
:iSC :jEq. (S26)=
CâŠ¤
:i1
ÎµjFC :jFis Hermitian=1
Îµj(FC :i)âŠ¤C:jEq. (S26)=Îµi
Îµj(SC :i)âŠ¤C:jSis Hermitian=Îµi
Îµj(CâŠ¤SC)ij, which indicates 
1âˆ’
Îµi
Îµj
(CâŠ¤SC)ij= 0, thus (CâŠ¤SC)ij= 0assuming non-degeneracy ÎµiÌ¸=Îµj.
34

function of density coefficients using the basis expansion of density function in Eq. (S25):
E(p) :=E[Ïp] =EhX
ÂµpÂµÏ‰Âµi
=TS(p) +EH(p) +EXC(p) +Eext(p)| {z }
=Eeff(p):=Eeff[Ïp], (S45)
where TS(p) :=TS[Ïp] =TShX
ÂµpÂµÏ‰Âµi
, (S46)
EH(p) :=EH[Ïp] =1
2ZZP
ÂµpÂµÏ‰Âµ(r)P
Î½pÎ½Ï‰Î½(râ€²)
âˆ¥râˆ’râ€²âˆ¥drdrâ€²=1
2pâŠ¤ËœWp, (S47)
EXC(p) :=EXC[Ïp] =EXChX
ÂµpÂµÏ‰Âµi
, (S48)
Eext(p) :=Eext[Ïp] =ZX
ÂµpÂµÏ‰Âµ(r)Vext(r) dr=pâŠ¤vext, (S49)
where ËœWÂµÎ½:= (Ï‰Âµ|Ï‰Î½),(vext)Âµ:=âŸ¨Ï‰Âµ|VextâŸ©.
The Coulomb integral notation (Ï‰Âµ|Ï‰Î½)is defined in Eq. (S33). Recall that we have omitted the depen-
dency of density basis {Ï‰Âµ}Âµhence of the functions for example TS(p)on the molecular structure M
in this subsection. Integrals for ËœWandvextcan be calculated directly [102, 103] under Gaussian-Type
Orbitals (GTO) as the basis {Ï‰Âµ}Âµ, using software libraries for example libcint [104] in PySCF [48].
The term EXC[Ïp]is calculated by numerically integrating the defined density Ïpon a quadrature grid as
typically used in a DFT calculation. In our M-OFDFT, TS(p)is calculated directly from the coefficient p
using the deep learning model TS,Î¸(p,M).
To carry out direct optimization using a learned KEDF model TS,Î¸(p), the gradient of the electronic
energy in Eq. (S45) is required, which is given by:
âˆ‡pEÎ¸(p) =âˆ‡pTS,Î¸(p) +ËœWp+âˆ‡pEXC(p) +vext| {z }
âˆ‡pEeff(p). (S50)
This gradient is then used to update the density coefficient after projected onto the linear subspace of
normalized densities, following Eq. (2).
Relation to Derivation as Integral of the Variation with Basis The gradient âˆ‡pE(p)can also be
derived by the relation between gradient and variation that we have already seen in Eq. (S41):
 
âˆ‡pE(p)
Âµ= 
âˆ‡pE[Ïp]
Âµ=ZÎ´E[Ïp]
Î´Ï(r) 
âˆ‡pÏp(r)
Âµdr=ZÎ´E[Ïp]
Î´Ï(r)Ï‰Âµ(r) dr. (S51)
Integrating the variations given in Eq. (S17) with the basis functions {Ï‰Âµ}Âµrecovers the Hartree energy
gradient ËœWp and external energy gradient vextin Eq. (S50). The formula also applies to the gradient of
the kinetic energy âˆ‡pTS(p)and the gradient of the XC energy âˆ‡pEXC(p).
Automatic Differentiation Implementation for Calculating the Gradient In the implementation of
M-OFDFT, the gradient of the KEDF model âˆ‡pTS,Î¸(p,M)is evaluated directly using automatic differ-
entiation [105], which can be conveniently done if implementing the model using common deep learning
programming frameworks, for example, PyTorch [106]. To calculate âˆ‡pEXC(p)conveniently, we also
re-implemented the PBE XC functional [107] in PySCF using PyTorch and evaluate its gradient also by
automatic differentiation. For material systems, automatic differentiation implementation of OFDFT is
also developed recently [108]. When using the residual version TS,res,Î¸of KEDF model, which is detailed
in Eq. (S78) in Supplementary Section B.4 later, the base KEDF (taken as the APBE KEDF [44]) is also
implemented in this way.
Computational Complexity As will be detailed in Supplementary Section B, the Transformer-
based [35] KEDF model for our M-OFDFT has a quadratic complexity O(A2) =O(N2). The PBE
functional [107] for EXCand the APBE functional [44] for the base KEDF are at the GGA level (gen-
eralized gradient approximation), so evaluating the energies amounts to calculating the density features
withO(M)cost on each grid point, in total with O(MN grid)cost where Ngridis the number of grid
points, and then conducting the quadrature with O(Ngrid)cost. The complexity for these energies is thus
O(MN grid)which is also quadratic O(N2)since Ngrid=O(N)(though with a large prefactor). Evaluat-
ing the gradient using automatic differentiation is in the same order of evaluating the function, hence also
hasO(N2)complexity. Evaluating EH(p)andEext(p)using Eq. (S47) and Eq. (S49) and their gradi-
ents using Eq. (S50) require O(M2) =O(N2)complexity. Therefore, the complexity in M-OFDFT has
35

a quadratic complexity O(N2), which is indeed lower than that of KSDFT (detailed in Supplementary
Section A.3.1 above).
Besides the advantage in asymptotic complexity, the fact that M-OFDFT is implemented in PyTorch(see
Supplementary Section B.1.6) enables it to leverage GPUs efficiently. These factors jointly facilitate the
much higher throughput of M-OFDFT than KSDFT.
Supplementary Section A.4 Label Calculation under Atomic Basis
This subsection details the calculation of the data tuple (p(k), T(k)
S,âˆ‡pT(k)
S)for learning a KEDF model
from the orbital coefficients C(k)of the orbital solution Î¦(k):={Ï•(k)
i}N
i=1in each SCF iteration k.
Following the previous subsection, we omit the appearance of Mfor density or orbital representation
(for example, in TS(p,M)) and omit the index dfor different molecular systems. We insist keeping the
kindex to reflect that the deduction is based on the solution in an SCF iteration but does not apply to
arbitrary orbital coefficients C.
Supplementary Section A.4.1 Density Fitting
We start with calculating the density coefficient p(k)under the density basis {Ï‰Âµ(r)}M
Âµ=1for representing
the density defined by the orbital coefficient solution C(k). This process is called density fitting [73],
which is also used in KSDFT for acceleration, in which context the atomic basis for the density is also
called auxiliary basis . The density coefficient p(k)needs to fit represented density Ïp(k)by Eq. (S25)
to the density ÏC(k)defined by Eq. (S28). Noting that Eq. (S28) essentially expands the density onto
the paired basis {Î·Î±(r)Î·Î²(r)}Î±Î²with coefficient as the vector Â¯Î“(k)of flattened Î“(k), this is the classical
coordinate transformation problem from the paired basis to the density basis. The classical approach is
by minimizing the standard L2-norm of the residual density:ZÏp(k)(r)âˆ’ÏC(k)(r)2dr=p(k)âŠ¤Wp(k)âˆ’2p(k)âŠ¤LÂ¯Î“(k)+Â¯Î“(k)âŠ¤DÂ¯Î“(k),
where WÂµÎ½:=âŸ¨Ï‰Âµ|Ï‰Î½âŸ©,LÂµ,Î±Î² :=âŸ¨Ï‰Âµ|Î·Î±Î·Î²âŸ©, andDÎ±Î²,Î³Î´ :=âŸ¨Î·Î±Î·Î²|Î·Î³Î·Î´âŸ©are the overlap matrices
of the density basis, between the density basis and the paired basis, and of the paired basis, respectively.
Noting that this is an quadratic form of p(k), we know the solution is p(k)=Wâˆ’1LÂ¯Î“(k).
However, the standard L2-metric on the density function may not be the most favorable metric for density
fitting. Instead, energy is the directly concerned quantity. The kinetic energy is the most desired metric,
since this would minimize the mismatch of the fitted density p(k)to the kinetic energy label T(k)
S. But
there is no explicit expression to calculate the kinetic energy from density coefficient. We hence turn to
using the Hartree energy and the external energy as the metric. (Using the XC energy requires an arbitrary
choice of a functional approximation, and the calculation is more costly.)
For the Hartree energy as defined in Eq. (S8), noting that it is quadratic in density, we fit pby minimizing
the (2Ã—) Hartree energy arising from the residual density:
2EH[Ïp(k)âˆ’ÏC(k)] =ZZ 
Ïp(k)(r)âˆ’ÏC(k)(r) 
Ïp(k)(râ€²)âˆ’ÏC(k)(râ€²)
âˆ¥râˆ’râ€²âˆ¥drdrâ€²
=p(k)âŠ¤ËœWp(k)âˆ’2p(k)âŠ¤ËœLÂ¯Î“(k)+Â¯Î“(k)âŠ¤ËœDÂ¯Î“(k),
where symbols with tilde are the corresponding overlap matrices under integral kernel1
âˆ¥râˆ’râ€²âˆ¥, which,
using the symbol of Coulomb integral defined in Eq. (S33), are ËœWÂµÎ½:= (Ï‰Âµ|Ï‰Î½),ËœLÂµ,Î±Î²:= (Ï‰Âµ|Î·Î±Î·Î²),
andËœDÎ±Î²,Î³Î´ := (Î·Î±Î·Î²|Î·Î³Î·Î´)as already defined in Eq. (S32). As a quadratic form, the solution is p(k)=
ËœWâˆ’1ËœLÂ¯Î“(k). This result can be understood as if the Hartree energy (Coulomb integral) defines a metric
on the space of density functions.
For the external energy as defined in Eq. (S4), as it is linear in density, we fit pby directly minimizing the
difference between the defined external energies:
 
Eext[Ïp(k)]âˆ’Eext[ÏC(k)]2= 
Eext(p(k))âˆ’Eext(C(k))2Eqs. (S49, S38)= ( p(k)âŠ¤vextâˆ’Â¯Î“(k)âŠ¤Â¯Vext)2.
To combine the two metrics, the final optimization problem is a combined least squares problem:
p(k)= argmin
ppâŠ¤ËœWpâˆ’2pâŠ¤ËœLÂ¯Î“(k)+Â¯Î“(k)âŠ¤ËœDÂ¯Î“(k)+ (pâŠ¤vextâˆ’Â¯Î“(k)âŠ¤Â¯Vext)2,
36

which corresponds to the over-determined linear equations in matrix form:
Wp(k)=b(k),where W:=ËœW
vâŠ¤
ext
,b(k):= ËœLÂ¯Î“(k)
Â¯Î“(k)âŠ¤Â¯Vext!
.
This is directly solved using least-squares solvers. In this conversion, we did not explicitly consider the
normalization constraint, p(k)âŠ¤w=N, since it is already satisfied with a high accuracy, due to the close
fit to the original density.
Supplementary Section A.4.2 Value Label Calculation
The label for the value of KEDF can be calculated from Eq. (S22) by leveraging the expression Eq. (S35)
under atomic basis:
TS(C(k)) =X
Î±Î²Î“(k)
Î±Î²TÎ±Î²=Â¯Î“(k)âŠ¤Â¯T,
where Î“(k)=C(k)C(k)âŠ¤from Eq. (S29), and Â¯Î“(k)is the vector by flattening. The corresponding density
coefficient p(k)is calculated from C(k)using density fitting as detailed above. A subtlety arises since
the fitted density p(k)may differ a little from the original density defined by C(k)due to finite-basis
incompleteness, so TS(C(k))may not be the best kinetic energy label for p(k). Indeed, as mentioned,
in density fitting, we do not have a way to directly find the density coefficient p(k)that achieves the
kinetic energy closest to TS(C(k)). Instead, p(k)is fitted to match the Hartree and external energy. We
hence assume that the electronic energy is less affected by density-fitting error than the kinetic energy,
and instead of taking TS(p(k))â‰ˆTS(C(k)), we take E(p(k))â‰ˆE(C(k)), which means TS(p(k)) +
Eeff(p(k))â‰ˆTS(C(k)) +Eeff(C(k)). Hence the KEDF value label for p(k)is taken as:
T(k)
S:=TS(C(k)) +Eeff(C(k))âˆ’Eeff(p(k)),
where Eeff(C(k)) =1
2Â¯Î“(k)âŠ¤ËœDÂ¯Î“(k)
|{z }
EH(C(k))+EXC(C(k)) +Â¯Î“(k)âŠ¤Â¯Vext|{z}
Eext(C(k)),
Eeff(p(k)) =1
2p(k)âŠ¤ËœWp(k)
|{z }
EH(p(k))+EXC(p(k)) +p(k)âŠ¤vext|{z}
Eext(p(k)),
following definitions and expressions of Eqs. (S34-S38) and Eqs. (S45-S49). Labels for the two variants of
functional models detailed in Supplementary Section B.4 can be calculated accordingly. For the residual
version of KEDF TS,res, the label is correspondingly modified using values at p(k):
T(k)
S,res:=T(k)
Sâˆ’TAPBE(p(k)).
For the version ETXCthat learns the sum of KEDF and the XC energy, the corresponding label is: T(k)
S+
EXC(p(k)) =TS(C(k))+ 
EH(C(k))+EXC(C(k))+Eext(C(k))
âˆ’ 
EH(p(k))+EXC(p(k))+Eext(p(k))
+
EXC(p(k)) =TS(C(k))+EXC(C(k))+ 
EH(C(k))âˆ’EH(p(k))+Eext(C(k))âˆ’Eext(p(k))
, where the last
term can be omitted since the Hartree energy difference and the external energy difference are minimized
byp(k)in density fitting. We therefore take the label as:
E(k)
TXC:=TS(C(k)) +EXC(C(k)).
Supplementary Section A.4.3 Gradient Label Calculation
Under an atomic basis, the kinetic energy functional of density is converted into a function of density co-
efficient TS(p) :=TS[Ïp]following Eq. (S46). For its gradient âˆ‡pTS(p), following the fact in Eq. (S51),
it is related to the variation of the functional TS[Ï]by integral with the basis:
 
âˆ‡pTS(p)
Âµ=ZÎ´TS[Ïp]
Î´Ï(r) 
âˆ‡pÏp(r)
Âµdr=ZÎ´TS[Ïp]
Î´Ï(r)Ï‰Âµ(r) dr.
The variation corresponding to a known density is given by Eq. (S23) above, which comes from the
solution of orbitals in a KSDFT SCF iteration. If omitting the error in density fitting and approximating
37

Î´
Î´ÏTS[Ïp(k)]withÎ´
Î´ÏTS[ÏC(k)] =Î´
Î´ÏTS[Ï[Î¦(k)]], then the gradient can be accessed by integrating both sides
of Eq. (S23) with the density basis:
âˆ‡pTS(p(k)) +v(k)
eff=Âµ(k)w,
where (v(k)
eff)Âµ:=âŸ¨Ï‰Âµ|V(k)
effâŸ©=Z
V(k)
eff(r)Ï‰Âµ(r) dr,wÂµ:=Z
Ï‰Âµ(r) dr. (S52)
In practice, the chemical potential Âµ(k)is not needed, since the gradient matters in its projection on the
tangent space of normalized densities in order to keep the density normalized in density optimization
(see Eq. (2)). The space of normalized densities is a linear space {p|pâŠ¤w=N}sinceR
Ïp(r) dr=P
ÂµpÂµR
Ï‰Âµ(r) dr=N, so it coincides with its tangent space. The projection onto the tangent space is
achieved by applying Iâˆ’wwâŠ¤
wâŠ¤w, which gives:

Iâˆ’wwâŠ¤
wâŠ¤w
âˆ‡pTS(p(k)) =âˆ’
Iâˆ’wwâŠ¤
wâŠ¤w
v(k)
eff. (S53)
Due to the same reason, the gradient loss function Eq. (4) also only matches the projected gradient of the
model to the projected gradient label.
The remaining task is to evaluate v(k)
effin Eq. (S52). Considering the complication that V(k)
eff(r)may
not be taken as the explicit-form effective potential Veff[Ï[Î¦(kâˆ’1)]](r) = Veff[ÏC(kâˆ’1)](Eq. (S17) and
Eq. (S28)), such as when DIIS (Eq. (S44)) is used in SCF iteration, evaluating v(k)
effcan be done by
leveraging the orbital-basis representation V(k)
effalready available in the SCF problem (Eq. (S26)), which
is defined in Eq. (S27) as the integral with paired orbital basis: (V(k)
eff)Î±Î²:=R
V(k)
eff(r)Î·Î±(r)Î·Î²(r) dr.
Using the expansion coefficients Kof the density basis onto the paired orbital basis, that is Ï‰Âµ(r) =P
Î±Î²KÂµ,Î±Î²Î·Î±(r)Î·Î²(r), we can conduct the conversion by (v(k)
eff)Âµ=P
Î±Î²KÂµ,Î±Î²(V(k)
eff)Î±Î².
However, solving for Kis unaffordable: using least squares, this amounts to solving DK=L(or solving
ËœDK=ËœL). Since D(orËœD) has shape B2Ã—B2, the complexity is O(MB4) =O(N5). Even it is only
called once for one molecular structure, the cost is still intractable even for medium-sized molecules.
Moreover, the approximationÎ´
Î´ÏTS[Ïp(k)]â‰ˆÎ´
Î´ÏTS[ÏC(k)]does not guarantee the optimality of density
optimization using the learned KEDF model on the same molecular structure as explained later.
We hence turn to another approximation, and use a more direct way to calculate the gradient. The approx-
imation is that Eq. (S23) also holds for the fitted density Ïp(k):
Î´TS[Ïp(k)]
Î´Ï+Veff{p(kâ€²)}kâ€²<k=Âµâ€²(k), (S54)
where Veff{p(kâ€²)}kâ€²<kis the V(k)
effconstructed from fitted density coefficients in previous SCF iterations,
instead of orbital coefficient solutions in previous SCF iterations that the V(k)
effin Eq. (S23) uses. The
chemical potential Âµâ€²(k)may be different, but due to the above argument for Eq. (S53), it is not used.
The approximation holds if density fitting error can be omitted, for example when using a large basis set.
Following the procedure above, the corresponding kinetic-energy gradient after projection is given by:

Iâˆ’wwâŠ¤
wâŠ¤w
âˆ‡pTS(p(k)) =âˆ’
Iâˆ’wwâŠ¤
wâŠ¤w
veff{p(kâ€²)}kâ€²<k, (S55)
where 
veff{p(kâ€²)}kâ€²<k
Âµ:=
Ï‰ÂµVeff{p(kâ€²)}kâ€²<k
=Z
Veff{p(kâ€²)}kâ€²<k(r)Ï‰Âµ(r) dr, (S56)
correspondingly. Calculating veff{p(kâ€²)}kâ€²<krequires a direct approach. This can be done following the re-
lation between the known functions Veff[Ï[Î¦(kâ€²)]]and the constructed V(k)
effin the SCF iteration. In DIIS, this
relation can be drawn from the construction in Eq. (S44) by noting the definitions Eq. (S27) and Eq. (S30)
of the matrices, which is a weighted average: V(k)
eff=Pkâˆ’1
kâ€²=0Ï€(k)
kâ€²Veff[ÏC(kâ€²)]. Following this pattern, the
required effective potential in Eq. (S54) is constructed as: Veff{p(kâ€²)}kâ€²<k=Pkâˆ’1
kâ€²=0Ï€(k)
kâ€²Veff[Ïp(kâ€²)]. The
weights {Ï€(k)
kâ€²}kâˆ’1
kâ€²=0are taken as the same as those computed in the SCF iteration. Its vector form under
the density basis is given by:
veff{p(kâ€²)}kâ€²<k=kâˆ’1X
kâ€²=0Ï€(k)
kâ€²veffp(kâ€²),where 
veffp
Âµ:=âŸ¨Ï‰Âµ|Veff[Ïp]âŸ©=Z
Veff[Ïp](r)Ï‰Âµ(r) dr.(S57)
38

Calculation of each veffp(k)can be carried out directly following Eq. (S17) that gives Veff[Ïp]explicitly.
In our implementation, each veffp(k)is conveniently calculated using our automatic differentiation imple-
mentation mentioned in Supplementary Section A.3.2, since we notice the fact that:
veffp=âˆ‡pEeff(p), (S58)
where Eeff(p) :=Eeff[Ïp]is defined in Eq. (S45) and its gradient âˆ‡pEeff(p)is given by Eq. (S50). This
fact is again due to the relation between gradient and variation revealed in Eq. (S51) and noting that Veff[Ï]
is defined as the variationÎ´Eeff[Ï]
Î´Ïof the effective energy functional in Eq. (S17). As analyzed at the end of
Supplementary Section A.3.2, evaluating the gradient has the same complexity as evaluating the energy
Eeff(p), which is O(M2) +O(MN grid) =O(N2), which is much lower than the O(N5)complexity
above.
To sum up, {veffp(kâ€²)}kâ€²are first calculated using automatic differentiation following Eq. (S58), which are
used to construct veff{p(kâ€²)}kâ€²<kfollowing Eq. (S57), then the gradient âˆ‡pTS(p(k))is given by Eq. (S55)
up to a projection. Since the loss function Eq. (4) for gradient supervision explicitly projects the gradient
error, the gradient label itself does not have to be projected before evaluating the loss (that is, the loss
is the same whether the gradient label is projected; since projection is idempotent). We hence take the
gradient label âˆ‡pT(k)
Sdirectly as the density-constructed DIIS effective potential vector:
âˆ‡pT(k)
S:=âˆ’veff{p(kâ€²)}kâ€²<k.
For the residual version of KEDF TS,resand the version ETXCthat also includes XC energy as detailed in
Supplementary Section B.4, the labels are produced accordingly:
âˆ‡pT(k)
S,res:=âˆ‡pT(k)
Sâˆ’ âˆ‡pTAPBE(p(k)),âˆ‡pE(k)
TXC:=âˆ‡pT(k)
S+âˆ‡pEXC(p(k)),
where âˆ‡pTAPBE(p(k))andâˆ‡pEXC(p(k))are also calculated using automatic differentiation.
Apart from the convenient and efficient calculation using automatic differentiation, this choice of gradient
label could also train a KEDF model that leads to the correct optimal density, since the labeling approach
is compatible with the density optimization procedure of M-OFDFT shown in Eq. (2) and Eq. (S50).
More specifically, upon the convergence of SCF iteration for which we mark quantities with â€œ â‹†â€, the
DIIS effective potential Vâ‹†
effis converged to the single-step, explicit-form effective potential Veff[ÏCâ‹†]
(Eq. (S17) and Eq. (S28)) by design. Correspondingly, the density-constructed DIIS effective potential
vector veff{p(kâ€²)}kâ€²<â‹†(Eq. (S56)) at convergence coincides with veffpâ‹†(Eq. (S57)), which is âˆ‡pEeff(pâ‹†)
by Eq. (S58). This gives a gradient label to the KEDF model through Eq. (S55), which enforces the model
to satisfy:

Iâˆ’wwâŠ¤
wâŠ¤w 
âˆ‡pTS,Î¸(pâ‹†) +âˆ‡pEeff(pâ‹†)Eq. (S50)=
Iâˆ’wwâŠ¤
wâŠ¤w
âˆ‡pEÎ¸(pâ‹†) = 0 ,
which in turn enforces the density optimization process Eq. (2) to converge to pâ‹†, the true ground-state
density coefficient. The optimality of density optimization using the learned KEDF model can then be
expected.
Supplementary Section A.5 Force Calculation
The force experienced by atoms in a molecular structure is an important quantity as it is directly required
for geometry optimization and molecular dynamics simulation. It is also used as a metric to evaluate the
results of M-OFDFT (Results 2.2, Supplementary Section D.2.1). There are different ways to calculate
the force in both KSDFT and OFDFT, and the results may differ. Here we describe two common ways for
force calculation, which are the Hellmann-Feynman (HF) force [109, 110] and the analytical force [111].
Evaluation protocol using force for M-OFDFT and M-NNP/M-NNP-Den against KSDFT is detailed at
the end.
Hellmann-Feynman Force Force is the negative gradient of the total energy of a molecule as a function
Etot(X)of atom coordinates X={x(a)}A
a=1(molecular conformation). We omit the dependency on the
molecular composition Zfor brevity. The total energy Etot(X) := Eâ‹†
X+Enuc(X)comprises both the
electronic energy Eâ‹†
Xin electronic ground state (including interaction with the nuclei), and also the energy
from inter-nuclear interaction:
Enuc(X) :=1
2X
a,b=1,Â·Â·Â·,A,
aÌ¸=bZ(a)Z(b)
âˆ¥x(a)âˆ’x(b)âˆ¥, (S59)
39

which gives the inter-nuclear part of the force,
âˆ’âˆ‡x(a)Enuc(X) =âˆ’Z(a)X
b:Ì¸=aZ(b)x(b)âˆ’x(a)
âˆ¥x(b)âˆ’x(a)âˆ¥3. (S60)
The electronic energy Eâ‹†
Xis the minimum after a variational optimization process for solving the elec-
tronic ground state of the molecule in conformation X. In the most fundamental form, Eâ‹†
Xis determined
by the variational problem on N-electron wavefunctions as shown in Eq. (S1). The Hamiltonian oper-
ator therein Ë†HX=Ë†T+Ë†Vee+Ë†Vext,Xdepends on the conformation Xthrough Vext,X, which is given in
Eq. (S2). The ground-state wavefunction Ïˆâ‹†
Xand energy Eâ‹†
X=âŸ¨Ïˆâ‹†
X|Ë†HX|Ïˆâ‹†
XâŸ©hence also depend on X.
The gradient of Eâ‹†
Xcan then be reformed as: âˆ‡XEâ‹†
X=
âˆ‡XâŸ¨Ïˆâ‹†
X|Ë†HX|Ïˆâ‹†
XâŸ©=âŸ¨âˆ‡XÏˆâ‹†
X|Ë†HX|Ïˆâ‹†
XâŸ©+âŸ¨Ïˆâ‹†
X|âˆ‡XË†HX|Ïˆâ‹†
XâŸ©+âŸ¨Ïˆâ‹†
X|Ë†HX|âˆ‡XÏˆâ‹†
XâŸ©
(*)=Eâ‹†
XâŸ¨âˆ‡XÏˆâ‹†
X|Ïˆâ‹†
XâŸ©+âŸ¨Ïˆâ‹†
X|âˆ‡XË†HX|Ïˆâ‹†
XâŸ©+Eâ‹†
XâŸ¨Ïˆâ‹†
X|âˆ‡XÏˆâ‹†
XâŸ©
=âŸ¨Ïˆâ‹†
X|âˆ‡XË†HX|Ïˆâ‹†
XâŸ©+Eâ‹†
Xâˆ‡XâŸ¨Ïˆâ‹†
X|Ïˆâ‹†
XâŸ©
(\#)=âŸ¨Ïˆâ‹†
X|âˆ‡XË†HX|Ïˆâ‹†
XâŸ©, (S61)
where the equality (*) is due to that Ïˆâ‹†
Xis an eigenstate of the Hermitian operator Ë†HXwith real eigenvalue
Eâ‹†
X, and the equality (\#) is due to that the wavefunction is normalized âŸ¨Ïˆâ‹†
X|Ïˆâ‹†
XâŸ©= 1for all X. Eq. (S61)
is the Hellmann-Feynman (HF) theorem [109, 110]. To continue the calculation, the gradient of the
Hamiltonian operator in the expression can be derived as âˆ‡x(a)Ë†HX=âˆ‡x(a)Ë†Vext,X, and by noting that
Ë†Vext,Xis multiplicative and one-body as shown in Eq. (S2), we have âˆ‡x(a)Vext,X(r) =âˆ’Z(a)râˆ’x(a)
âˆ¥râˆ’x(a)âˆ¥3,
and subsequently, the electronic force can be calculated as:
âˆ’âˆ‡x(a)Eâ‹†
X=âˆ’âŸ¨Ïˆâ‹†
X|âˆ‡x(a)Ë†HX|Ïˆâ‹†
XâŸ©=Z(a)Z
Ïâ‹†
X(r)râˆ’x(a)
âˆ¥râˆ’x(a)âˆ¥3dr=:f(a)
X, (S62)
where Ïâ‹†
X(r) := Ï[Ïˆâ‹†
X](r)defined in Eq. (S3). This is the Hellmann-Feynman (HF) force fX. This ex-
pression coincides with the electrostatic force under a classical view, indicating â€œthere are no â€˜mysterious
quantum-mechanical forcesâ€™ acting in moleculesâ€ [98]. From the expression, evaluating the HF force only
requires a good approximation to the ground-state electron density Ïâ‹†
X(r), which is available in various
quantum chemistry methods, including ÏCâ‹†
Xin KSDFT given by Eq. (S28) and Ïpâ‹†
Xin M-OFDFT given
by Eq. (S25). The total force on the nuclei is the sum with the inter-nuclear force in Eq. (S60).
Analytical Force However, when using the atomic basis, there emerges approximation error in the
function representation, since the basis is incomplete. Consequently, the conditions of the HF theorem
do not hold exactly. This makes the HF force in Eq. (S62) only an approximation to the true electronic
forceâˆ’âˆ‡x(a)Eâ‹†
X, and other approximations are possible. For example, âˆ’âˆ‡x(a)Eâ‹†
Xcan also be estimated
by directly taking the analytical gradient of the electronic energy Eâ‹†
Xexpressed under the atomic basis
with the optimal coefficients [111]. This way of estimating the electronic force is hence called analytical
forcefana,X. For KSDFT, Eâ‹†
X=EX(Câ‹†
X), where EX(C)is given by Eqs. (S34-S38) (note that the basis
functions Î·Î±,Î·Î²and matrices T,ËœDandVextall depend on X), andCâ‹†
Xis the optimal orbital coefficients.
The corresponding analytical force on an atom acan be expanded as:
f(a)
ana,X:=âˆ’âˆ‡x(a)EX(Câ‹†
X) =âˆ’(âˆ‡x(a)EX)(Câ‹†
X)âˆ’tr
(âˆ‡x(a)Câ‹†
X)âŠ¤âˆ‡CEX(C)
C=Câ‹†
X
, (S63)
where (âˆ‡x(a)EX)takes the gradient with a fixed C, and âˆ‡x(a)Câ‹†
Xis the Jacobian,
(âˆ‡x(a)Câ‹†
X)Î±i,Î¾ :=âˆ‚(Câ‹†
X)Î±i/âˆ‚x(a)
Î¾in which Î¾âˆˆ {1,2,3}indices the three spacial compo-
nents, and matrix operations, including transpose, matrix multiplication and trace, act on indices
Î±andi. When Câ‹†
Xis indeed accurately optimized, self-consistency in Eq. (S43) and orthonor-
mality in Eq. (S39) are satisfied. By also noting Eq. (S42), the second term in Eq. (S63)
vanishes: tr 
(âˆ‡x(a)Câ‹†
X)âŠ¤âˆ‡CEX(C)
C=Câ‹†
X Eq. (S42)= 2 tr 
(âˆ‡x(a)Câ‹†
X)âŠ¤FCâ‹†
XCâ‹†
X Eq. (S43)=
2 tr 
(âˆ‡x(a)Câ‹†
X)âŠ¤SXCâ‹†
XÎµâ‹†
X
= tr
(âˆ‡x(a)Câ‹†
X)âŠ¤âˆ‡Câ‹†
Xtr 
(Câ‹†
XâŠ¤SXCâ‹†
Xâˆ’I)Îµâ‹†
X
=
âˆ‡x(a)tr 
(Câ‹†
XâŠ¤SXCâ‹†
Xâˆ’I)Îµâ‹†
XEq. (S39)= 0 . If in practice Câ‹†
Xis not exactly optimized, the contri-
bution from âˆ‡x(a)Câ‹†
Xshould also be considered; see Pulay [111] for detailed treatments.
Note that only the âˆ’(âˆ‡x(a)Â¯Vext,X)âŠ¤Â¯Î“â‹†
Xterm ( Â¯Î“â‹†
Xis the vector of flattened optimal density matrix Î“â‹†
X:=
Câ‹†
XCâ‹†
XâŠ¤), as a part of âˆ’(âˆ‡x(a)EX)(Câ‹†
X), corresponds to the HF force (see Eq. (S38)). Other terms in the
40

analytical force f(a)
ana,Xin Eq. (S63) are collectively called the Pulay force after [111], that is, fana,Xâˆ’fX.
They come in the form of the gradient of the basis functions with respect to atom coordinates, and hence
are non-zero when using atomic basis and differentiate the analytical force from the HF force.
Implementation Although the analytical force is regarded as a more accurate estimation when using
atomic basis, we still take the HF force to evaluate the results, since the way to calculate the analytical
force is different for different methods: KSDFT and M-OFDFT require different types of Coulomb inte-
grals under different basis sets, and M-NNP and M-NNP-Den only require back-propagating the gradient
through the deep learning model. Moreover, the HF force in Eq. (S62) only depends on the density from
the ground-state solution, so it can also be seen as a relevant metric to evaluate the solved density.
For each molecular system, we take the true value of HF force as that given by the KSDFT solution, where
the density is taken after density fitting (see Supplementary Section A.4.1). The HF force by M-OFDFT is
calculated directly from the optimized density. For M-NNP and M-NNP-Den, as they cannot provide the
density, only the corresponding analytical forces, âˆ’âˆ‡XEtot,Î¸(X)andâˆ’âˆ‡XEtot,Î¸(X,pinit), are available.
Note that since they predict the total energy, the inter-nuclear force âˆ’âˆ‡XEnuc(X)(Eq. (S60)) is included
in the gradients. To convert them into HF forces, we extract from the gradients with the inter-nuclear
force as well as the Pulay force: f(a)
M-NNP ,X=âˆ’âˆ‡x(a)Etot,Î¸(X)âˆ’(âˆ’âˆ‡x(a)Enuc(X))âˆ’(f(a)
ana,Xâˆ’f(a)
X),
and similarly for f(a)
M-NNP-Den ,X, where the analytical force f(a)
ana,Xand the HF force f(a)
Xare calculated from
KSDFT. Following previous works [40, 37], the error in predicted force is measured by the mean absolute
error (MAE) over each of the three spacial components of the force on each atom in each molecule in the
test set.
Supplementary Section A.6 Scaling Property under Atomic Basis
The KEDF has an exact scaling property which describes its change after uniformly scaling (stretching
or squeezing) a density. Under a scaling (squeezing) rate Î», the uniformly scaled density is given by the
rule of change of variables: Ë†Î»Ï(r) :=Î»3Ï(Î»r). The scaling property is stated as the following [112]:
TS[Ë†Î»Ï] =Î»2TS[Ï]. (S64)
Designing the model to exactly satisfy this condition not only guarantees reasonable results in some phys-
ical sense, but would also reduce the functional space where the model needs to search by learning, hence
improving accuracy and the generalization and extrapolation ability. Some prior investigations [113, 114]
for machine-learning KEDF indeed observed accuracy improvement in some cases (though not as sub-
stantial in some other cases).
Now we consider leveraging this property for the KEDF model TS(p,M)under atomic basis. As input
density is represented under an atomic basis {Ï‰Âµ}M
Âµ=1using coefficient pasÏpgiven by Eq. (S25), we
need to first represent the scaled density Ë†Î»Ïpunder the same basis with coefficient Ë†Î»(p):
Ë†Î»Ïp:=X
ÂµpÂµË†Î»Ï‰Âµ(r) =X
ÂµË†Î»(p)ÂµÏ‰Âµ(r). (S65)
Solving for Ë†Î»(p)using least squares gives Ë†Î»(p) =Wâˆ’1W(Î»)p, where W(Î»)
ÂµÎ½:=âŸ¨Ï‰Âµ|Ë†Î»Ï‰Î½âŸ©here, and
WÂµÎ½:=âŸ¨Ï‰Âµ|Ï‰Î½âŸ©as the same as above. The scaling property Eq. (S64) is then transformed as:
TS(Wâˆ’1W(Î»)p,M) =Î»2TS(p,M). (S66)
However, to preserve Eq. (S64) exactly, the expansion Eq. (S65) must hold exactly. But this is not the
case: the finite basis set {Ï‰Âµ}M
Âµ=1is incomplete, and the scaled basis functions {Ë†Î»Ï‰Âµ}M
Âµ=1are not linearly
dependent on the original ones. Hence, the transformed scaling property Eq. (S66) under the atomic basis
isnot exact , thus may not provide much benefit to the KEDF model TS,Î¸(p,M).
There seems to be a possibility when using the even-tempered atomic basis set, which comes in the
following form:
Ï‰Âµ=(a,Ï„,Î¾)(r) =wa,Ï„,Î¾(râˆ’x(a)),
where wa,Ï„,Î¾(r= (x, y, z )) := xÎ¾1yÎ¾2zÎ¾3exp(âˆ’Î±a,|Î¾|Î²Ï„âˆ¥râˆ¥2), (S67)
with tempering ratio Î² > 1, monomial-exponent parameter Î¾= (Î¾1,Î¾2,Î¾3), and exponent parame-
terÎ±a,|Î¾|shared across Î¾values with the same |Î¾|:=Î¾1+Î¾2+Î¾3. The index Ï„runs from 0 to
Ta,|Î¾|. Therefore, under the scaling with Î²âˆ’1
2, we have:dÎ²âˆ’1
2Ï‰Âµ=(a,Ï„,Î¾)(r) =dÎ²âˆ’1
2wa,Ï„,Î¾(râˆ’x(a)) =
41

Î²âˆ’3
2wa,Ï„,Î¾(Î²âˆ’1
2râˆ’x(a)) = ( Î²âˆ’1
2)3+|Î¾|wa,Ï„âˆ’1,Î¾(râˆ’x(a)/Î²âˆ’1
2), which is in the same even-tempered
basis set but on a scaled molecular structure M/Î²âˆ’1
2:={Z,X/Î²âˆ’1
2}, as long as Ï„ >0. For Ï„= 0, it
corresponds to the most flat basis function, and its coefficient pÂµ=(a,Ï„=0,Î¾)is close to zero and hence can
be omitted for density representation. Therefore, under the scaling with Î²âˆ’1
2, the scaled density can be
expressed as:
dÎ²âˆ’1
2Ïp,M(r) =X
a,Î¾Ta,|Î¾|X
Ï„=0pa,Ï„,Î¾dÎ²âˆ’1
2Ï‰a,Ï„,Î¾(r)
=X
a,Î¾Ta,|Î¾|X
Ï„=1pa,Ï„,Î¾(Î²âˆ’1
2)3+|Î¾|wa,Ï„âˆ’1,Î¾(râˆ’x(a)/Î²âˆ’1
2) +X
a,Î¾pa,0,Î¾dÎ²âˆ’1
2Ï‰a,0,Î¾(r)
â‰ˆX
a,Î¾Ta,|Î¾|X
Ï„=1pa,Ï„,Î¾(Î²âˆ’1
2)3+|Î¾|wa,Ï„âˆ’1,Î¾(râˆ’x(a)/Î²âˆ’1
2)
=:X
a,Î¾Ta,|Î¾|X
Ï„=0p(Î²âˆ’1
2)
a,Ï„,Î¾wa,Ï„,Î¾(râˆ’x(a)/Î²âˆ’1
2) =Ï
p(Î²âˆ’1
2),M/Î²âˆ’1
2(r),
where the new coefficients are defined as:
p(Î²âˆ’1
2)
a,Ï„,Î¾:=(Î²âˆ’1
2)3+|Î¾|pa,Ï„+1,Î¾, Ï„ < Ta,|Î¾|,
0, Ï„ =Ta,|Î¾|.
The scaling property Eq. (S64) can then be written as:
TS(p(Î²âˆ’1
2),M/Î²âˆ’1
2) = (Î²âˆ’1
2)2TS(p,M).
However, this holds only for one value of scaling depending on the basis set (also for (Î²âˆ’1
2)nfor integer
nas long as the contributions from the first ncoefficients can be omitted). Moreover, this constraint
connects the original input to a scaled conformation M/Î²âˆ’1
2, which can be far from an equilibrium
structure. The behavior of the TS,Î¸model for these scaled conformations may be less relevant for real
applications. Hence still, it does not seem definite to gain benefits from the scaling property.
Another possibility to exactly expressing the scaling property is to replace the molecular structure M
with the basis overlap matrix Wto characterize the atomic basis. In this way, the rescaling of atomic
basis can be described by the change of the overlap matrix:
Ë†Î»WÂµÎ½:=Z
Ë†Î»Ï‰Âµ(r)Ë†Î»Ï‰Î½(r) dr=Î»3Z
Ï‰Âµ(r)Ï‰Î½(r) d(Î»r) =Î»3WÂµÎ½.
Hence, we do not need to expand the rescaled basis onto the original one, and the scaling property in
Eq. (S64) becomes:
TS(p, Î»3W) =Î»2TS(p,W).
It is promising future work to investigate whether the overlap matrix is sufficient to specify the spacial
and type configurations of the basis functions, and to design proper model architecture for effectively
processing such pairwise feature and ensuring the above scaling property.
42

Node
EmbeddingGBF
Coefficient
Adapter
AtomRef ğ™ğ©ğ—
G3D
G3D
MLPÎ£ ğ‘‡S,ğœƒ â‹¯
ğ™ à·¥ğ©
Atom 
Embedding
Shrink 
Gate
MLPğ“”
MLPÎ£
ğ¡c NodeEmbedding 
à·¥ğ©Local 
Frame
Natural 
Reparameterization
Dimension -wise 
Rescalingb CoefficientAdapter
ğ© a
à·¥ğ©
ğ©â€™ğ©â€™MLPà·©ğ“” ğ“”
ğ¡
ğ¡d
ğ¡
SelfAttentionLayerNorm
MLPLayerNormà·©ğ“”G3DSupplementary Figure S1: The KEDF model architecture. (a) (See also Alg. 1.) Overview of the
model architecture. The model calculates the non-interacting kinetic energy (or a variant of it) from the
given density, specified by the density coefficient vector pon an atomic basis set (Supplementary Sec-
tion B.1.1), as well as the atomic numbers Zand coordinates Xof all atoms in the target molecule for
characterizing the basis functions. The coefficient vector pis first processed by the CoefficientAdapter
module to make an SE(3) -invariant density features Ëœpand reduce the gradient scale for the rest of the
model to fit. The Ëœpfeatures are then used to construct initial node features hby the NodeEmbedding
module (Supplementary Section B.1.3), for which the types Zand positions Xof the atoms are also in-
corporated to provide information to interpret the coefficient features. The positional input Xis perceived
by the model only in terms of pairwise distance features Eproduced by the Gaussian basis function
(GBF ) module (Supplementary Section B.1.2). The node features hare subsequently updated by sev-
eral Graphormer-3D ( G3D ) modules (Supplementary Section B.1.4). They calculate the interaction of
features on every pair of nodes hence cover the non-local effect, in which relative position features ËœEbe-
tween each pair of nodes are considered, which are processed from Eby a multi-layer perceptron ( MLP )
module (Eq. (S69)). The final node features are aggregated by another MLP module and summed over
the nodes to produce a scalar, and the final output is its addition with the output of the AtomRef enhance-
ment module (Methods 4.3, Supplementary Section B.3.3) which shares the burden to model a large-scale
gradient. (b)(See also Alg. 2.) Structure of the CoefficientAdapter module. It consists of the LocalFrame
module (Methods 4.2; Supplementary Section B.2) to convert SE(3) -equivariant features into invariant
features, followed by two enhancement modules NaturalReparameterization andDimensionwiseRescal-
ing(Methods 4.3; Supplementary Section B.3.2 and B.3.1) which reduce the gradient scale for subsequent
modules to fit. (c)Structure of the NodeEmbedding module (Supplementary Section B.1.3). It integrates
information from three sources for each node: atom-type (for basis-set type) Zwhich is mapped to a
feature vector by atom embedding, positional feature in terms of pairwise distance feature Ewhich is
aggregated over nodes and processed by an MLP module, and density features Ëœpwhich is mapped to a
mild numerical range by a shrink gate and processed by another MLP module. (d)Structure of the G3D
module (Supplementary Section B.1.4). The SelfAttention module updates node features by the non-local
cross-node interaction of features hbased on spacial relation features ËœEbetween nodes. An MLP module
further processes the updated node features. The LayerNorm module is applied before each of the two
modules for numerical convenience. Note that the data on each streamline is the concatenated features
across the nodes. Only the SelfAttention module models the interaction across nodes, while other modules
are applied to the features of each node independently.
43

Supplementary Section B M-OFDFT Technical Details
In this section, we provide more technical details of the proposed M-OFDFT framework. We first provide
an overall description of the non-local neural network model for KEDF and detail the architecture of
neural network components in Supplementary Section B.1. We then elaborate on other components in
the model that handle the unconventional challenges for learning a neural network model, including the
use of local frame to guarantee geometric invariance of the model in Supplementary Section B.2, and
enhancement modules for expressing a large gradient range in Supplementary Section B.3. Furthermore,
we discuss explorations on learning different density functional variants in Supplementary Section B.4.
Finally, we detail the density optimization techniques in Supplementary Section B.5, which is the usage
of M-OFDFT to solve a given molecular structure.
Supplementary Section B.1 Model Specification
We start with more explanations on the design idea of the model formulation. As motivated in the main
paper, M-OFDFT utilizes the expansion coefficients pof the density under an atomic basis set as the direct
density feature input into the functional model. Compared to the grid-based representation, a common
alternative for density representation, using the atomic-basis representation saves thousands of times the
representation dimension. This is critical for implementing a non-local calculation in the model, which is
well-known vital for approximating KEDF [30, 14, 8, 31, 20], but would otherwise be prohibited by the
millions times more cost. The aggregation to atoms further reduces the required number of interacting
nodes (from grids to atoms) in the non-local calculation. Noting that atomic basis are also used in material
systems, so this way of density representation hence M-OFDFT is not restricted to non-periodic molecular
systems in formulation. We note that there are also works in the context of learning the XC functional
that develop machine-learning models also using features under atomic basis [65, 66], but the models
do not take the molecular structure in input and focus on processing the features hosted on each atom
individually, hence are essentially not non-local models.
Since the atomic basis depends on the molecular structure M:={Z,X}, the model also needs to include
Minto its input for a complete specification of the input density, where the atom types Z:={Z(a)}A
a=1
are used to specify the types of basis functions (since atoms of different elements are assigned with dif-
ferent types (that is, parameters) of basis functions) and the conformation X:={x(a)}A
a=1to specify the
centers of the basis functions. Although including Minto the model input sacrifices formal universality,
explicit dependency on Mis arguably inevitable for an efficient density representation, which requires
the structure of the problem, that is, the pattern of the density (â€œinductive biasâ€ in machine learning), to re-
duce the representation dimension. Even the irregular grid representation inherits the pattern of molecular
structure, hence a non-local model using grid input feature still requires generalization across molecular
systems. On the other hand, the non-local Graphormer model architecture, based on which our neural
network model is designed, has shown attractive capability to generalize across conformations and chem-
icals for a range of molecular tasks in previous studies [33, 34, 62]. Our extrapolation study in Results 2.3
directly validates the superior generalization capability for OFDFT. Particularly, Fig. 3(c) shows that M-
OFDFT outperforms classical KEDFs which only use raw electron density input by a large margin even in
an extrapolation setup, indicating the benefit of accuracy of non-local calculation enabled by the concise
density representation based on the molecular structure Moutweighs the sacrifice of formal universality.
As for the generalization to molecules with unseen elements, since the atom type Zhere as perceived by
the model only represents the type of basis functions that is used to hold the electron density near the atom
but does not represent the physics of the actual nucleus or its interaction with electrons, we can assign
the atom of unseen element with a seen atom type, and use the basis functions of the seen element to
hold the electron density around that atom. Nevertheless, due to a different electron structure, the model
has not seen the pattern of density coefficients for the unseen element, so there exists a generalization
challenge. This could potentially be mitigated by using a common basis set for all elements or including
more elements in training, which will be investigated in future work.
The KEDF model takes atomic numbers Z, positions X, and density coefficients pof all atoms in the
molecule as input. Note that given pandM, there is no need of explicit density gradient or Laplacian
features since they are transmitted to the features of the basis functions and thus already embodied in
M. The input variables are first used to construct node features encoding information about the elec-
tron density surrounding each atom. These features are further transformed and employed to predict the
non-interacting kinetic energy. The gradient of the KEDF with respect to density coefficients for density
optimization is obtained through auto-differentiation [106]. Considering the fact that non-local calculation
is indispensable for KEDF, we build the non-local model based on the Graphormer architecture [33, 34].
Notably, Graphormer can handle varying-length input feature (as needed since different molecules have
44

Algorithm 1 Evaluation of the KEDF model TS,Î¸(p,M)(or the kinetic residual model TS,res,Î¸(p,M)or
the TXC model ETXC,Î¸(p,M); see also Supplementary Figure S1)
Require: Input molecular structure M={X,Z}comprising positions X:={x(a)}A
a=1and atomic
numbers Z:={Z(a)}A
a=1of all atoms in the molecule, input density coefficients p(see Supplemen-
tary Section B.1.1).
1:Construct pairwise distance features Eâ†GBF(X)andËœEâ†MLP(E)(Eq. (S68), Eq. (S72),
Eq. (S69));
2:Process coefficient features: (Ëœp,pâ€²)â†CoefficientAdapter (p)(Alg. 2);
3:Construct initial atomic representations: hâ†NodeEmbedding (Z,E,Ëœp)(Eq. (S70));
4:foriin1Â·Â·Â·Ldo
5: Update atomic representations using the i-th G3D module: hâ†G3D(i)(h,ËœE)(Eq. (S74),
Eq. (S71), Eq. (S73));
6:end for
7:Compute the output of the atomic reference module: Tâ€²â†TAtomRef (pâ€²,M)(Eq. (8));
8:Compute the kinetic energy: TSâ†PA
a=1MLP(ha) +Tâ€²(Eq. (S75));
9:return TS
different numbers of atoms) in a permutation-invariant manner, which is not straightforward using other
popular architectures such as multi-layer perceptions alone. The architecture has shown attractive perfor-
mance in processing molecular structure to predict various properties, for example, ground-state energy
and HOMO-LUMO gap of molecular systems [33, 34], and also structure sampling from a thermody-
namical ensemble [62]. The non-local architecture has an O(N2)complexity, which does not increase
the complexity of OFDFT. Our empirical results in Supplementary Section D.4.2 indicate the non-local
formulation is crucial; restricting atomic interactions with distance cutoffs leads to a performance decline
on considered molecular systems.
An overview of the KEDF model is sketched in Supplementary Figure S1(a) and summarized in Alg. 1.
The process can be narrated in four stages.
(i)To adapt the Graphormer architecture for learning a physical functional, a few modifications are
needed. To address the additional learning challenges mentioned in Methods 4.2 and 4.3, the input density
coefficients, formulated following the details in Supplementary Section B.1.1, are first processed by the
CoefficientAdapter module (Supplementary Figure S1(b)), in which the local frame module (Methods 4.2;
Supplementary Section B.2) first converts the rotational equivariant coefficients to rotational invariant
coefficients, and two enhancement modules NaturalReparameterization and DimensionwiseRescaling
(Methods 4.3; Supplementary Section B.3.2 and B.3.1) follow in order so as to reduce the gradient scale.
(ii)Before leveraging the Graphormer architecture, initial node (that is, atom) features need to be pre-
pared, which, in addition to the node type ( Z) and geometry ( E, processed from X) information in the
original version of Graphormer, density coefficients ( Ëœp, processed from p) should also be blended in so
that the resulted node features hold the information of electron density around the respective atoms. This
process is handled by the NodeEmbedding module (Supplementary Figure S1(c); Supplementary Sec-
tion B.1.3). Note that the conformation input Xis perceived by the model only in the form of pairwise
distance features E, which is produced by the Gaussian Basis Function (GBF) module (Supplementary
Section B.1.2) from pairwise distances of all atom pairs. These features are consumed by the NodeEm-
bedding module and also by the G3D module next. In this way, the model is naturally invariant with
respect to the translation and rotation of atom coordinates X.
(iii)Several concatenated Graphormer modules then process the node features. Since in processing the
interaction between node features, the spacial relation between the two nodes needs to be considered,
we use the Graphormer-3D (G3D) version [34], where the conformation information is input as pairwise
distance features processed by the GBF module and a multi-layer perceptron (MLP) module. In the
G3D module (Supplementary Figure S1(d); Supplementary Section B.1.4), the SelfAttention module
carries out calculation on any pair of node features, which covers non-local interaction or correlation of
density features at distance. The LayerNorm module in the G3D module is adopted following successful
experiences of such models, which shifts and scales the feature distribution to make the following layer
easier to process numerically.
(iv)Finally, the last-layer node features are processed by MLP and then got aggregated into one scalar,
which is added with the output of the AtomRef enhancement module TAtomRef (pâ€²,M)(Methods 4.3;
Supplementary Section B.3.3) to construct the final energy output (Supplementary Section B.1.5). The
45

AtomRef module shares the burden to express large gradient thus reduces the difficulty to fit large gradient
for the neural-network G3D branch.
We next detail the neural network components in the model, including GBF, NodeEmbedding, and G3D
modules. Other rule-based or pre-arranged modules fall in standalone topics, so we provide their details in
subsequent subsections, including local frame in Supplementary Section B.2, and enhancement modules
of DimensionwiseRescaling, NaturalReparameterization and AtomRef in Supplementary Section B.3. We
start with handling the formatting of density coefficient features.
Supplementary Section B.1.1 Density Basis and Coefficient Specification
As mentioned in Methods 4, M-OFDFT adopts atomic basis as an efficient density representation for
molecules. Each basis function Ï‰Âµis specified by the position of the center atom and the basis function
index, and we hence use Âµ= (a, Ï„)to index the Ï„-th basis function centered at atom a. The basis
coefficients therefore can be correspondingly attributed to each atom, which naturally serve as node-wise
density features for the atom point cloud.
Specifically, we choose an even-tempered basis set [74] (Eq. (S67)) as the density basis set, with its Î²
parameter taken as 2.5. Each atom type Z(that is, atomic number) has its own set of basis functions and
the size of each set TZvaries from different atom types. The detailed composition of each atom type is
summarized in Supplementary Table S2.
The difference of basis functions on different types of atoms makes the coefficient features hold different
meanings and even come with different dimensions. This is unconventional and challenging for machine
learning models to process. To make the coefficient vector homogeneous over all the atoms, the basis
function sets on different atom types are joined together, and this united basis set is broadcast to all atoms,
making a unified T-dimensional density coefficient vector paon any atom a, where T:=P
ZTZis the
sum of the number of basis functions over all considered atom types. The final density coefficient vector
for the entire system is thus the concatenation of these T-dimensional vectors: p:= concat( {pa}A
a=1).
In more detail, in the QM9 dataset, the 477-dimensional concatenated coefficient vector consists of 20,
109, 116, 116 and 116 basis functions which correspond to H,C,N,OandF, respectively. For example,
given the coefficient of a hydrogen ( H) atom, we place them at the first 20 dimensions and use zero-
valued vectors to mask other positions. A graphic illustration is shown in Supplementary Figure S2. The
zero-valued mask is also employed to mask the predicted gradient during density optimization, avoiding
introducing irrelevant gradient information from masked positions.
Supplementary Table S2: Orbital composition of the basis set associated with each atom type. Note
that even though the compositions for N, O and F are the same, their basis sets can still be different from
each other because they can have different exponents and contraction coefficients.
Atom Type Orbitals
H 6s3p1d
C 11s8p7d3f2g
N 11s8p7d4f2g
O 11s8p7d4f2g
F 11s8p7d4f2g
Supplementary Section B.1.2 Gaussian Basis Function (GBF) Module
As shown in Supplementary Figure S1(a), the model converts the conformation (atom coordinates) input
Xinto pairwise distance features before sent to the rest part of the model, so that geometric invariance
with respect to Xis guaranteed. These features are produced by the GBF module. It first converts atom
coordinates X={x(a)}A
a=1into pairwise distances, and then expand each distance value âˆ¥xaâˆ’xbâˆ¥into
a feature vector by evaluation under a series of Gaussian basis functions:
define E:=GBF(X) :Ek
ab:=1âˆš
2Ï€Ïƒkeâˆ’(âˆ¥xaâˆ’xbâˆ¥âˆ’Âµk)2
2Ïƒ2
k , (S68)
where ÂµkandÏƒkare learnable scalar parameters representing the center and scale of the k-th Gaussian
basis function.
The pairwise distance features are used by the SelfAttention module in the G3D module (Supplementary
Figure S1(d); Supplementary Section B.1.4), which help identify the strength and ways of interactions
46

0 477 201
01pH,
0 477 20 1291
01pC,
0 477 129 2451
01pN,
0 477 245 3611
01pO,
0 477 361
1
01pF,
Supplementary Figure S2: Concatenation of atomic basis for different atom types. Ï„is the index
of density coefficient dimension and pZ,Ï„denotes the coefficient vector for atom type Z. Note that this
is a schematic illustration, so the coefficient dimensions may not correspond to the actual dimensions
precisely.
between node features. They are also used to construct the initial node features in the NodeEmbedding
module introduced next (Supplementary Figure S1(c); Supplementary Section B.1.3).
Supplementary Section B.1.3 NodeEmbedding Module
As shown in Supplementary Figure S1(c), we design a NodeEmbedding module to integrate various input
features into initial node features h, which, for each atom a, embody the atom type Z(a)(for specifying
the type of basis functions centered at that location), the spacial position relation with other atoms (basis
centered at other locations) specified by X, and the density coefficient vector paon that atom repre-
senting the electron density around the atom (see Supplementary Section B.1.1 above). Specifically, (i)
for the atom type Z(a), an AtomEmbedding module assigns a learnable feature vector cZ(a)to the atom
according to its type Z(a).(ii)For positional features encoding the spacial relation of the atom awith
respect to other atoms, distance features with respect to all other atoms are summed:PA
b=1Eab.(iii)For
the density coefficient paon the atom a, it is first processed by the CoefficientAdapter module (Supple-
mentary Figure S1(b)) detailed later in Supplementary Section B.2 and Supplementary Section B.3. The
processed coefficient vector Ëœpabecomes translation and rotation (that is, SE(3) ) invariant, and is properly
transformed to exhibit a mild scale range of itself and of the corresponding gradient. Due to the trade-off
between the scale range of Ëœpaand the corresponding gradient, the scale of Ëœpais still large for a neural
network to process according to our trials. Therefore, we introduce a ShrinkGate module:
ShrinkGate (Ëœpa) :=Î»cotanh( Î»mulËœpa),
where Î»coandÎ»mulare learnable scalar parameters. The tanh function is applied element-wise, which
maps to a bounded space hence suppresses extreme values to avoid numerical challenges. Due to the
47

SE(3) -invariant property of Ëœpa, applying such a nonlinear operation on it still preserves this geometry
invariance.
Before aggregating features from the three sources, positional features and density coefficient features are
processed by multi-layer perceptron (MLP) modules. MLP (also called feed-forward network in some
context) are the most classical neural network architecture, which has been proven to be able to approx-
imate any continuous function under certain limit [115], and are indispensable components in modern
neural network architectures. Specifically, all the MLP modules in our model shown in Supplementary
Figure S1 follow the general expression:
MLP(x) :=U(2)gelu(U(1)x+b(1)) +b(2), (S69)
where xhere represents a general feature vector of a unit (node or atom pair), U(1)andU(2)are learnable
weight matrix parameters, b(1)andb(2)are learnable bias vector parameters, and gelu(x) := xÎ¦(x)
for any scalar input xâˆˆRis the Gaussian error linear unit activation function [116] that introduces
nonlinearity to the module, where Î¦(x)is the cumulative distribution function of the standard Gaussian
distribution. When applied to a feature vector, gelu operates element-wise: gelu(x)k:=gelu(xk).
To sum up, the whole NodeEmbedding module can be formulated as:
define h:=NodeEmbedding (Z,E,Ëœp) :
ha:=cZ(a)+MLP(E)AX
b=1Eab
+MLP(p) 
ShrinkGate (Ëœpa)
, (S70)
where MLP(E)andMLP(p)are two MLP instances with independent parameters.
Supplementary Section B.1.4 Graphormer-3D (G3D) Module
The Graphormer-3D (G3D) module (Supplementary Figure S1(d)) is the main neural network module in
the KEDF model (Supplementary Figure S1(a)). It is a Transformer-based [35] graph neural network,
which performs non-local calculation between every pair of node features even located at distance, while
is tailored to taking into account the spacial relations between the two locations. Each G3D module
contains two layer normalization (LayerNorm) modules, a SelfAttention module, and an MLP module,
assembled in the way shown in Supplementary Figure S1(d).
LayerNorm The Layer Normalization module [117] is a widely adopted technique in Transformer-
based architectures. It shifts and scales the running feature vector in a neural network so that the values
over the vector components distribute with zero mean and unit variance, which is the numerical range
over which neural network modules are the most sensitive, thus facilitates stable and faster training and
a better fit to data. This module is applied before sending node features to the SelfAttention module and
the MLP module. It normalizes the node feature vector on each node independently:
LayerNorm (ha) :=sâŠ™haâˆ’Âµa
Ïƒa+b,
where Âµa:=1
DhidDhidX
k=1hk
a, Ïƒ a:=vuut1
DhidDhidX
k=1(hkaâˆ’Âµa)2, (S71)
Dhidis the dimension of the input feature vector hafor node a,sâˆˆRDhidandbâˆˆRDhidare learnable
vector parameters, and âŠ™denotes element-wise multiplication.
SelfAttention The SelfAttention module is the processor responsible for non-local calculation that pro-
duces the result of interaction between any two node feature vectors. For updating feature vector on
node (that is, atom) a, the vanilla SelfAttention module [35] considers interaction of the current feature
vector haof node awith the feature vector hbof each of the other nodes as well as itself. This is done
by constructing a â€œqueryâ€ feature vector Qa:=U(query)hafor node ato interact with other nodes (in-
cluding itself), and each of the other nodes, say, node b(could be a), constructs a â€œkeyâ€ feature vector
Kb:=U(key)hbto respond to the â€œqueryâ€, and a â€œvalueâ€ feature vector Vb:=U(value)hbto convey its
contribution. Here, U(query),U(key)andU(value)are learnable weight matrix parameters in RDâ€²
hidÃ—Dhid,
where Dhidis the dimension of each node feature vector haorhb, and Dâ€²
hidis another hyperparameter
determining the dimension of the key, query and value feature vectors. The respond of the â€œkeyâ€ to the
â€œqueryâ€ is modeled byQâŠ¤
aKbâˆš
Dâ€²
hidwhich is treated as the unnormalized log-probability, or logit, to determine
48

the portion that node bcontributes to the new node feature vector of node a. The probability, or the portion
of contribution, is recovered by applying the softmax function to the logits:
softmax (â„“)b:=eâ„“b
P
bâ€²eâ„“bâ€².
In matrix form, the updated node feature vectors hâ€²:= [hâ€²
1,Â·Â·Â·,hâ€²
A]âˆˆRDâ€²
hidÃ—Aas a stacked array over
all the nodes is:
hâ€²:=VsoftmaxQâŠ¤Kp
Dâ€²
hidâŠ¤
âˆˆRDâ€²
hidÃ—A,
where Q:=U(query)h,K:=U(key)h,V:=U(value)h,h:= [h1,Â·Â·Â·,hA]âˆˆRDhidÃ—A.
To enlarge expressiveness, it is common practice to introduce â€œmulti-head attentionâ€, where the above
self attention calculation is repeated Dhead times, and the Dhead-fold results are concatenated. More
explicitly, the updated node feature vector array is:
hâ€²:= [hâ€²
1,Â·Â·Â·,hâ€²
A]âˆˆR(DheadDâ€²
hid)Ã—A,
where hâ€²
a:=concatenate ({hâ€²(1)
a,Â·Â·Â·,hâ€²(Dhead)
a })âˆˆRDheadDâ€²
hid,âˆ€a= 1Â·Â·Â·A,
are reshaped from:
[hâ€²(d)
1,Â·Â·Â·,hâ€²(d)
A] :=V(d)softmaxQ(d)âŠ¤K(d)
p
Dâ€²
hidâŠ¤
âˆˆRDâ€²
hidÃ—A,âˆ€d= 1Â·Â·Â·Dhead,
where Q(d):=U(query ,d)h,K(d):=U(key,d)h,V(d):=U(value ,d)h,h:= [h1,Â·Â·Â·,hA]âˆˆRDhidÃ—A.
To let the updated feature vector array hâ€²have the same shape as the original h, hyperparameters Dhead
andDâ€²
hidare chosen such that DheadDâ€²
hid=Dhid.
The vanilla SelfAttention module handles general featured point (node) cloud input, but for a set of fea-
tured atoms, there is a spacial or positional relation between a pair of atoms. Distance is a natural way to
describe such relation. For example, a shorter distance generally indicates a stronger interaction between
the two node feature vectors. To inform the attention mechanism of this characteristic, Graphormer-3D
(G3D) [34] introduce pairwise distance features into the attention mechanism. To accommodate for the
different usage of pairwise distance features from that in the NodeEmbedding module, a learnable MLP
layer is applied to the original pairwise distance features, pair by pair, which maps each distance feature
vector to dimension Dhead:
ËœE:=MLP(E)âˆˆRAÃ—AÃ—Dhead:ËœEab:=MLP(Eab)âˆˆRDhead, (S72)
where the MLP in the latter expression follows the general formulation in Eq. (S69). This new pairwise
distance feature array is incorporated into the vanilla self attention as a bias to the contribution logits. For
an explicit expression, let ËœE(d):= [ËœE(d)
ab]abdenote the AÃ—Amatrix combining the d-th distance feature
for all the pairs. The expression for the SelfAttention module is:
define hâ€²= [hâ€²
1,Â·Â·Â·,hâ€²
A] :=SelfAttention (h,ËœE)âˆˆRDhidÃ—Aforh:= [h1,Â·Â·Â·,hA]âˆˆRDhidÃ—A:
hâ€²
a:=concatenate ({hâ€²(1)
a,Â·Â·Â·,hâ€²(Dhead)
a })âˆˆRDheadDâ€²
hid=Dhid,âˆ€a= 1Â·Â·Â·A,
where [hâ€²(d)
1,Â·Â·Â·,hâ€²(d)
A] :=V(d)softmaxQ(d)âŠ¤K(d)
p
Dâ€²
hid+ËœE(d)âŠ¤
âˆˆRDâ€²
hidÃ—A,âˆ€d= 1Â·Â·Â·Dhead,
Q(d):=U(query ,d)h,K(d):=U(key,d)h,V(d):=U(value ,d)h.(S73)
Assembly The third component in the G3D module is an MLP module, which follows the same form as
given in Eq. (S69), and is applied to the node feature vector of each node independently (that is, the nodes
share the same MLP module to process their feature vectors). These modules are combined to make the
G3D module following the illustration in Supplementary Figure S1(d). Explicitly in equation,
define hâ€²:=G3D(h,ËœE) :
hâ€²:=MLP(LayerNorm (hâ€²â€²)) +hâ€²â€²,
hâ€²â€²:=SelfAttention (LayerNorm (h),ËœE) +h.(S74)
49

Supplementary Section B.1.5 Output Process
To produce a scalar output, each node feature vector hais processed to produce a scalar by an MLP
module following the form of Eq. (S69), whose output dimension (that is, number of rows of U(2)) is
1. The scalars from all the nodes are summed up, and the summed value is added with the output of the
AtomRef module (Methods 4.3; Supplementary Section B.3.3) to produce the final energy output:
TS:=AX
a=1MLP(ha) +TAtomRef (pâ€²,M). (S75)
Supplementary Section B.1.6 Model Configuration
In all experimental settings, we employ the same backbone architecture, Graphormer, specifically utilizing
the Graphormer-3D ( G3D ) encoder module. To ensure a fair comparison, most hyperparameters in all
models are maintained consistently (for example, model depth and hidden dimension). A summary of
all hyperparameter choices can be found in Supplementary Table S3. It is worth mentioning that in the
shrink gate of NodeEmbedding , the initial value of learnable parameters Î½cois set to 10, while the initial
value of Î½mulis set to 0.02 for the ethanol dataset and 0.05 for all other datasets. We did not conduct
an extensive hyperparameter search, and most hyperparameters were chosen following Graphormer [33].
All models are implemented using the PyTorch deep learning framework [106].
Supplementary Table S3: Hyperparameters of the Graphormer model. These hyperparameters are
adopted in all methods in the present work (M-OFDFT, M-NNP, M-NNP-Den).
Hyperparameter M-OFDFT
G3D Modules 12
Hidden Dimension 768
MLP Hidden Dimension 768
Number of Heads 32
GBF Dimension 128
Dropout 0.1
Attention Dropout 0.1
Optimizer Adam
Learning Rate Schedule Linear decay
Adam ( Î²1, Î²2) (0.95, 0.99)
Adam Ïµ 1Ã—10âˆ’8
Supplementary Section B.1.7 Training Hyperparameters
Following Graphormer [33], all models are trained using the Adam optimizer and a linear decay learning
schedule. The peak learning rate is set to 1Ã—10âˆ’4for our functional models and 3Ã—10âˆ’4for baseline
models (that is, M-NNP and M-NNP-Den). Other optimizer hyperparameters can be found in Supple-
mentary Table S3. A warmup stage featuring a linearly increasing learning rate is introduced to stabilize
training during the initial stage. The number of warmup steps is set to 30k for M-OFDFT and 60k for
baseline models. The batch size is set to 256 for the ethanol dataset and 128 for all other datasets. All
models are trained on Nvidia Tesla V100 GPUs.
For different molecule datasets, the number of training epochs is determined by examining the loss curve.
Training is halted once the validation loss fails to decrease for 20 epochs. Specifically, our functional
models are trained for approximately 600 and 700 epochs on the ethanol and QM9 datasets, respectively.
For the QMugs dataset, our model is trained for approximately 700 epochs, while the M-NNP and M-
NNP-Den models are trained for 2,300 epochs. Notably, we propose a series of QMugs datasets with
increasing molecular size in Results 2.3, where the number of SCF datapoints will slightly increase as the
average molecular size increases (larger molecules generally require more SCF iterations to converge).
To ensure a fair comparison, we maintain approximately the same number of training epochs for different
datasets. In the chignolin experiment, there are four chignolin datasets with increasing peptide length and
data size. Our functional models are trained for 1,400, 1,100, 800, and 750 epochs, respectively, while
the M-NNP and M-NNP-Den models are trained for 3,000, 3,000, 1,000, and 900 epochs, respectively.
In practice, we employ a weighted loss function to optimize the KEDF model:
L=Î¾engLeng+Î¾gradLgrad+Î¾denLden,
50

à·œğ±
à·œğ²à·œğ’›
à·œğ±
à·œğ±à·œğ²
à·œğ³à·œğ²à·œğ³
à·œğ±
à·œğ²à·œğ’›a
b
ğ±ğ‘(0)ğ±ğ‘(1)
ğ±ğ‘(2)
H
O
C
Supplementary Figure S3: Illustration of the local frame .(a)The local frame is constructed for each
atom according to its local environment, which is equivariant with respect to. the transformation of the
global frame. (b)Local frames of similar substructures (for example, the three H-Cbonds in the methyl
group) rotate with substructures accordingly, resulting in invariant coefficient features for locally similar
density patterns.
where Leng,Lgrad, and Ldenrepresent the KEDF energy loss (Eq. (3)), gradient loss (Eq. (4)), and
projected density loss (Eq. (S80)), respectively. Î¾eng,Î¾grad, and Î¾denare the corresponding loss weights.
The selection of loss weights is determined through grid search on the validation set for various datasets
independently, and the utilized loss weights are provided in Supplementary Table S4.
Supplementary Table S4: Loss weights for various datasets and learning targets. The loss weights
are tuned on each setting by conducting a grid search on the validation set.
Dataset Î¾eng Î¾grad Î¾den
Ethanol- TS,res 1 0.1 0.08
Ethanol- ETXC 1 0.03 1
QM9- TS,res 1 0.12 0.005
QM9- ETXC 1 0.12 0.05
QMugs 1 0.1 1
Chignolin 1 0.1 1
Supplementary Section B.2 Local Frame Module for Geometric Invariance
As mentioned in Methods 4.2, by expanding the electron density on a set of atomic basis, the expansion
coefficients pmathematically comprise geometric tensors of various orders equivariant to rotations and
translations. A local frame simultaneously rotating with the structure is adopted to decouple the density
feature from the change of the coordinate system and unnecessary geometric variability. As shown in
Supplementary Figure S3(a), to construct the local frame at an atom at x(0)
a, we choose Ë†xpointing to its
nearest atom x(1)
a. The Ë†zaxis lies in the line of the cross-product of Ë†xwith the direction to the second-
nearest not-on- Ë†xatomx(2)
aand the Ë†yaxis is then given by Ë†y=Ë†zÃ—Ë†x. Note that we exclude the hydrogen
atoms in the neighborhood of the center atom following [75], making the obtained local frame depend
more on heavy atoms, whose positions are more stable than those of hydrogen atoms and reflect more
reliable local structures. Denote the local frame associated with atom aas:
Ra:= (Ë†x,Ë†y,Ë†z),
the density coefficient vector and the gradient vector under the local frame are calculated by:
pâ€²l
a=Dl(Ra)pl
a,âˆ‡pâ€²laTS=Dl(Ra)âˆ‡plaTS. (S76)
where lis the degree of tensors or azimuthal quantum number. plcorresponds to the coefficient of basis
functions of the degree l(or type- ltensors in mathematics). Dl(Ra)is the Wigner-D matrix of degree l.
51

Notably, an additional benefit of the local frame is that it makes a stable feature for locally similar density
patterns. For example, consider the coefficients corresponding to the atomic basis on the three hydrogen
atoms in a methyl group (Supplementary Figure S3(b)). As the three H-Cbonds are highly indistinguish-
able, the densities on the bonds are very close and contribute almost identically to the kinetic energy.
But as the three bonds have different orientations relative to a common global coordinate system, the
coefficients on the hydrogen atoms are vastly different if the basis functions on different hydrogen atoms
are aligned to the same directions. In contrast, basis functions under local frames are oriented equiv-
ariantly with the orientation of the local substructures, leaving the coefficients largely invariant, which
makes more physical meanings to predict the energy. For example, coefficients on basis functions align-
ing with the Ë†xdirection always represent the density on the shortest bond with the atom. As a result, the
local frame makes almost the same density coefficients on the three hydrogen atoms in the methyl group
even though the H-Cbonds have different orientations. We also provide a numerical visualization of the
ethanol dataset, where the target functional is chosen as the residual energy of the non-interacting kinetic
energy after deducted by the value given by the base kinetic energy functional APBE [44]. As shown in
Supplementary Figure S4, the local frame always attains a noticeable scale (standard deviation) reduction
across various atom types, demonstrating its capability to eliminate unnecessary geometric variability
caused by various orientations of the chemical bonds. More importantly, as shown in Supplementary
Figure S5, this approach also brings a substantial scale reduction for gradient labels across various atom
types, especially for Hatoms, where almost all coefficient dimensions exhibit a >60\% gradient scale
reduction. This is crucial to alleviate the large gradient range and make the subsequent dimension-wise
rescaling module easier (See more discussions in Supplementary Section B.3.1). These two advantages
of the local frame are beneficial for the efficient optimization of the KEDF model. The empirical results
in Supplementary Section D.4.3 suggest that the KEDF model achieves a 2-fold lower training and test
error by using this technique.
Supplementary Section B.3 Enhancement Modules for Expressing Vast Gradient
Range
As mentioned in Methods 4.3, the raw gradient range of the input is vast, and conventional data normal-
ization techniques are not applicable in our task since minimizing the gradient scale is conflicting with
minimizing the coefficient scale. For the same reason we neither can take the logarithm of the density
feature, as a number of works have adopted [118], since the gradient would be correspondingly scaled up.
Another way to decrease the gradient scale is to downscale the energy value (that is, using a larger energy
unit). However, this does not bring further improvement in our trials. As mentioned in the main context,
this is due to the trade-off between easy learning and model resolution: an error in the normalized (small)
scale will be enlarged in the original scale. It neither works to use a separate model to learn the gradient
directly, as we have empirically observed that the energy value model easily overfits the training data, and
during density optimization the gradient model unnecessarily decreases the energy constructed from the
value model and even appears non-conservative as the optimization diverges.
Having both large coefficients and gradients implies a function with a large Lipschitz coefficient, which
leads to a great challenge for the optimization of conventional neural networks. According to these
observations, we turn to elaborating on a series of enhancement modules to express the vast gradient
range and enable effective training.
Supplementary Section B.3.1 Dimension-wise Rescaling
Dimension-wise rescaling of coefficients and gradients is applied after the process of local frame and
natural reparameterization. The two modules made the rescaling easier, but there is still a scale trade-
off between coefficients and gradients. We hence choose moderate values for the scale parameters in
Eq. (5): the target gradient scale sgradis set to 0.05and the maximal coefficient scale scoeffis set to 50.
Compared to the gradient scale, handling input coefficients with a larger range is more manageable. For
example, incorporating a ShrinkGate module to further compress the coefficients into a bounded space,
as discussed in Supplementary Section B.1.3. We found this technique yields an admirable performance
empirically. Consequently, we prefer allowing a larger coefficient scale in the trade-off. In the context of
deep learning, the Lipschitz coefficient (that is, the maximum absolute gradient that the neural network
model could express) is usually used to indicate the capability of a model fitting gradient label. Following
this convention, we take the maximum absolute gradient value to measure the label scale.
To illustrate the importance of dimension-wise rescaling, we visualize in Supplementary Figure S6
the scales of gradients and density coefficients over the dimensions before and after the processing of
dimension-wise rescaling in the setting of learning residual KEDF with APBE base KEDF on the QM9
52

dataset. The shown gradient scales are estimated after centralizing the gradient values by subtracting the
gradient mean of each dimension with the atomic reference module. As plotted in Supplementary Fig-
ure S6(a), many gradient dimensions have an extremely large scale. We found this leads to great difficulty
in the practical optimization of deep learning models. After dimension-wise rescaling, most dimensions
are rescaled to have the desired gradient scale (Supplementary Figure S6(b)). As illustrated in Supple-
mentary Figure S6(c), the density coefficients are rescaled to a larger scale, and a ShrinkGate module is
introduced to normalize the coefficients into a friendly space (Supplementary Section B.1.3). As a result,
we found that the neural network model without the dimension-wise rescaling technique hardly converges
and the loss curve is particularly volatile, due to the smoothness of common neural network architectures
which restricts the range of the output gradient of the model. Applying the dimension-wise rescaling
technique effectively mitigates this dilemma and enables efficient optimization.
Supplementary Section B.3.2 Natural Reparameterization
Although dimension-wise rescaling allows the trade-off between numerical scales of coefficients and gra-
dients, the trade-off often has a difficult frontier that the two sides cannot be simultaneously made in a
mild scale. A way to improve the trade-off is to balance the difficulties over the dimensions. The dimen-
sions exhibit different scales since they have different sensitivity to the density hence the energy. Different
basis functions spread over different regions in space, so a same amount of coefficient change on different
basis functions influences the density function differently. To understand and balance the sensitivities, we
derive how the change of coefficients affect the density function. Consider a small change âˆ†pto the coef-
ficients p, which leads to a change in the density function âˆ†Ï(r) :=Ïp+âˆ†p(r)âˆ’Ïp(r). This difference is
typically measured by the L2-metric in the function space:R
|Ïp+âˆ†p(r)âˆ’Ïp(r)|2dr=R
|âˆ†Ï(r)|2dr=R
|Ïâˆ†p(r)|2dr=RP
Âµâˆ†pÂµÏ‰Âµ(r)P
Î½âˆ†pÎ½Ï‰Î½(r) dr= âˆ†pâŠ¤Wâˆ†p, where WÂµÎ½:=R
Ï‰Âµ(r)Ï‰Î½(r) dr
is the overlap matrix of the atomic basis for density. As the atomic basis is not orthonormal, Wis not
isotropic (that is, cannot be turned proportional to the identity matrix by orthogonal transformations), so
the same amount of coefficient change in different dimensions has different effects on the density function.
The proposed natural parameterization Ëœp:=MâŠ¤p, where Mis a square matrix satisfying MMâŠ¤=
W, fulfills the desired balance. To see this, noting that Wis non-singular hence is M, we have
p=Mâˆ’âŠ¤Ëœp, so the density change isR
|âˆ†Ï(r)|2dr= âˆ†pâŠ¤Wâˆ†p= âˆ† ËœpâŠ¤Mâˆ’1WMâˆ’âŠ¤âˆ†Ëœp=
âˆ†ËœpâŠ¤Mâˆ’1MMâŠ¤Mâˆ’âŠ¤âˆ†Ëœp= âˆ†ËœpâŠ¤âˆ†Ëœp, hence the change of coefficient in any dimension contributes
equally to the density change. The gradient is reparameterized accordingly, following âˆ‡ËœpTS=
(âˆ‡Ëœpp)âŠ¤âˆ‡pTS=Mâˆ’1âˆ‡pTS.
Choosing the matrix Mstill faces a degree of freedom of an orthogonal transformation. We choose:
M=Qâˆš
Î›, (S77)
whereâˆš
Î›denotes element-wise square-root operation, and the diagonal matrix Î›and orthogonal matrix
Qcome from the eigenvalue decomposition of W=QÎ›QâŠ¤(noteWis symmetric positive definite
so such decomposition exists). We found it achieves more balanced sensitivity than using Cholesky de-
composition in terms of the resulted largest gradient scale after dimension-wise rescaling. Although the
eigenvalue decomposition requires a cubic complexity, it is only needed once per molecular structure,
hence does not introduce a large overhead compared to the cost in density optimization. Empirically, nat-
ural reparameterization considerably stabilizes the optimization of the functional model and brings lower
training and validation loss, as presented in Supplementary Section D.4.3, underscoring the necessity of
balancing the density coefficient with a physical metric.
Supplementary Section B.3.3 Atomic Reference Module
As mentioned in Methods 4.3, the per-type bias statistics {Â¯TZ}Zand the global bias Â¯Tglobal on the dataset
are solved from the following over-determined linear equations using least squares method:
Â¯TM(d):=X
ZA(d)
ZÂ¯TZ+Â¯Tglobal = mean {T(d,k)
Sâˆ’Â¯gM(d)âŠ¤p(d,k)}k,âˆ€d,
where A(d)
Z:= \#{aâˆˆ M(d):Z(a)=Z}is the number of atoms of type Zin molecule M(d). Note
that on the ethanol and chignolin datasets where all structures share the same constitution, we only use
the global bias, that is, Â¯TM:=Â¯Tglobal . A similar treatment for energy bias is also adopted in [119].
To demonstrate the benefit of the atomic reference module, we visualize the gradient scale reduction by
the module on the QM9 dataset in Supplementary Figure S7. We find that the atomic reference module
53

Algorithm 2 Evaluation of the CoefficientAdapter module
Require: Input density coefficients p.
Require: Pre-computed dimension-wise rescaling factors {Î»Z,Ï„}Z,Ï„(see Eq. (5) and Supplementary
Section B.3.1); pre-computed quantities for the target molecular structure M: Wigner-D matrices
{{Dl
a:=Dl(Ra)}lmax
l=0}A
a=1(see Eq. (S76)) for transforming coefficients on each atom onto the
local frame of the atom, and the square-root matrix Mof the density-basis overlap matrix W(see
Eq. (S77)).
1:forain1Â·Â·Â·Ado
2: forlin0Â·Â·Â·lmaxdo
3: Transform density coefficients using the Wigner-D matrix: pl
aâ†Dl
apl
a(Eq. (S76));
4: end for
5:end for
6:Conduct natural reparameterization: pâ€²â†MâŠ¤p(Eq. (7));
7:forain1Â·Â·Â·Ado
8: forÏ„in1Â·Â·Â·T do
9: Rescale density coefficients dimension-wise: Ëœpa,Ï„â†Î»Z(a),Ï„pâ€²
a,Ï„(Eq. (6));
10: end for
11:end for
12:return (Ëœp,pâ€²)
substantially reduces the scale (variance) of gradient labels, especially for dimensions corresponding to
â€˜sâ€™ atomic orbital functions. Since â€˜sâ€™ orbitals usually have large gradient mean values but small variance
values, subtracting the gradient mean from these dimensions greatly simplifies the learning of gradient
labels.
CoefficientAdapter Module These enhancement modules and the neural network model are combined
in the way shown in the model architecture Supplementary Figure S1(a). In the architecture, the natural
reparameterization module and the dimension-wise rescaling module are combined with the local frame
technique into the CoefficientAdapter module (Supplementary Figure S1(b)). It transforms the vanilla
density coefficient pto a neural-network-friendly density feature Ëœpas the direct input to the neural net-
work model. It not only guarantees geometric invariance of density features, but also facilitates efficient
learning with vast gradient labels. We outline the implementation in Alg. 2.
Supplementary Section B.4 Functional Variants
In principle, besides the unknown KEDF, any energy density functionals in the decomposition formulation
of ground-state energy (Eq. (S10)) can be taken as the training objective of the proposed M-OFDFT
framework. Here we first describe the two versions we mainly employed in the implementation of M-
OFDFT, and then describe our exploration in other functional variants.
Supplementary Section B.4.1 Residual KEDF
In the implementation of M-OFDFT, the first functional version we introduced aims to reduce the diffi-
culty of modeling KEDF directly by employing an existing KEDF as the base functional and learning the
residual:
TS,Î¸(p,M) :=TS,res,Î¸(p) +TS,base(p,M). (S78)
Specifically, we chose the APBE KEDF [44] as the base KEDF since it best fits the training data on the
QM9 dataset among four functionals mentioned in Methods 4.7.
The residual KEDF formulation also allows leveraging a lower bound of KEDF which introduces non-
negativity to the residual model, in hope for better extrapolation performance by encoding this analytical
property into the model. Nevertheless, we find that it is not straightforward to gain benefits from this
property, as it introduces additional training challenge which outweighs its merit. Specifically, we take
the von Weizs Â¨acker (vW) KEDF [46] as the base KEDF, which is a lower bound of the true KEDF [96,
Thm. 1.1]. The training challenge is that it renders the gradient for the residual model to learn in vast
range. The visualization of processed gradient and coefficient scales presented in Supplementary Fig-
ure S8. Even after the processing of local frame, natural reparameterization, atomic reference model, and
dimension-wise rescaling, the processed gradient scale, in terms of the maximum absolute value across all
dimensions and all datapoints, is 2.08Ã—107on the QM9 dataset (Supplementary Figure S8(a)), which is
orders larger than the scale 2.74 when using the APBE as the base KEDF (Supplementary Figure S6(b)),
54

and the scale 4.82 for the TXC version below (that is, learning the sum of the KEDF and XC functional),
even allowing a larger processed density coefficient scale of 1113.87 (Supplementary Figure S8(b)) vs.
507.44 for APBE base KEDF (Supplementary Figure S6(c)) and 451.59 for the TXC version. This large
gradient scale impedes any effective training of the neural network model. We even tried dropping out
datapoints with particularly large gradient labels that exceed a chosen threshold for training, but observed
a performance degradation, due to reduced information on a broader range of densities. We suspect that
this difficulty may be due to the divergence between learning an easier rule and learning a numerically
more friendly target. The vW functional leaves the neural network model to learn a non-negative resid-
ual, which can be regarded as an easier rule. But since the vW functional is a lower bound, it may not
approximate the KEDF closely, hence could leave the residual and its gradient in a large scale.
Nevertheless, in Supplementary Section D.1.1 we empirically verified the learned KEDF model satisfies
this lower bound.
Supplementary Section B.4.2 TXC Functional
The residual KEDF version has achieved a simpler learning target with a tractable gradient range, but
it has a computational bottleneck of evaluating the value and gradient of the APBE base KEDF as well
as the PBE XC functional from the density coefficient, which is conducted on a grid. As discussed in
Supplementary Section A.3.2, the time complexity of calculating the value is O(MN grid)(recall Mis the
number of basis functions). Evaluating the gradient by automatic differentiation requires the same time
complexity as evaluating the value, and moreover, it also requires O(MN grid)memory occupation to store
intermediate values for back-propagation. Considering the large prefactor of Ngrid(commonly âˆ¼103N),
the computational cost for residual KEDF to conduct density optimization becomes unaffordable for
large-scale systems. Such cost was observed on the QMugs dataset, for which Supplementary Section D.3
presents more detailed results.
To get rid of calculation on grid, the second version of learning target for the deep learning model is
introduced as the sum of the KEDF and the XC functional, which we call the TXC functional, denoted as
ETXC,Î¸(p,M).
Supplementary Section B.4.3 Learning Other Functionals
Besides the two versions above, we also experimented other direct targets for the deep learning model to
learn, including directly learning the KEDF TS[Ï](that is, without a base KEDF), the universal functional
U[Ï](Eq. (S7)), and the total energy functional Etot,M[Ï], that is, electronic energy E[Ï]in Eq. (S10)
plus inter-nuclear energy Enuc(M)in Eq. (S59) later. We found that both directly learning the KEDF
and the universal functional are hard to optimize due to their large gradient range, which is even larger
than that for learning the residual KEDF model and the TXC model, and cannot be effectively handled
even using the proposed techniques. Interestingly, learning the total energy functional Etot,M[Ï], that
is, adding the external energy Eextand inter-nuclear energy Enuc(M)to the universal functional as the
target, substantially reduces the gradient range and enables stable optimization, especially on protein
systems we considered. Specifically, on the 50 test chignolin structures considered in Results 2.3, learning
the total energy model Etot,Î¸(p,M)using data of all peptides gives a better density optimization result
(per-atom energy MAE 0.071 kcal/mol as in Fig. 3(e) â€˜M-OFDFT/Pretrainâ€™, per-atom relative energy
MAE 0.097 kcal/molas in Fig. 3(c) â€˜M-OFDFTâ€™) than learning the TXC model ETXC,Î¸(per-atom energy
MAE 0.102 kcal/mol, per-atom relative energy MAE 0.148 kcal/mol). Nevertheless, these results by the
ETXC,Î¸model are still reasonable and effective, and are substantially better than the classical OFDFT
using the APBE KEDF (per-atom relative energy MAE 0.684 kcal/molas in Fig. 3(c) â€˜APBEâ€™).
Supplementary Section B.5 Density Optimization Details
After a functional model is learned, M-OFDFT solves a given molecular structure by density optimization,
as shown in Alg. 3. Here we provide additional results and details for density optimization.
Supplementary Section B.5.1 Additional Density Optimization Results
As mentioned in Methods 4.4, M-OFDFT can produce reasonable optimization curves for a given molec-
ular structure from either of the two initialization methods H Â¨uckel and ProjMINAO, and gives accurate
energy and HF force results. Here we illustrate the density optimization behaviors of various initialization
strategies using a QM9 molecule. As shown Fig. S9, the optimization process starting from the MINAO
density leads to an obvious gap from the target KSDFT energy. In contrast, the optimization curve start-
ing from H Â¨uckel density converges closely to the target energy, although it shows a larger energy error
55

Algorithm 3 Usage of M-OFDFT to solve a given molecular system (density optimization process)
Require: A trained kinetic residual model TS,res,Î¸(p,M)or TXC model ETXC,Î¸(p,M), molecular struc-
tureM={X,Z}of the given system comprising atomic numbers Zand positions Xof all atoms,
gradient descent step size Îµ, initialization method init method , a trained density coefficient projec-
tion branch âˆ†pÎ¸ifinit method == â€˜ProjMINAOâ€™ (Supplementary Section B.5.2).
1:Compute constants for this molecular structure M: density basis normalization vector w, overlap ma-
trixWand its square-root matrix Mfor natural reparameterization, 2-center-2-electron Coulomb in-
tegral ËœWfor evaluating EH(Eq. (S47)), external potential vector vextfor evaluating Eext(Eq. (S49)),
rotation matrices and Wigner-D matrices for local frame, build grid and calculate density basis values
on grid points for evaluating EXCandTS,base if using TS,res,Î¸model;
2:ifinit method == â€˜HÂ¨ uckelâ€™ then
3: p(1)â†HÂ¨ uckel init(M);
4:else if init method == â€˜ProjMINAOâ€™ then
5: pâ†MINAO init(M);
6: p(1)â†pâˆ’âˆ†pÎ¸(p,M);
7:end if
8:kâ†1
9:while stopping criterion (described in Supplementary Section B.5.3) is not met do
10: Compute electronic energy using the PyTorch implementation (see Alg. 1 for evaluating TS,res,Î¸
orETXC,Î¸):
11: ifusing the kinetic residual model then
12: E(k)â†TS,res,Î¸(p(k),M)+TS,base(p(k),M)+EH(p(k),ËœW)+EXC(p(k),M)+Eext(p(k),vext);
13: else if using the TXC model then
14: E(k)â†ETXC,Î¸(p(k),M) +EH(p(k),ËœW) +Eext(p(k),vext);
15: end if
16: Calculate the gradient of the electronic energy with respect to the density coefficients:
âˆ‡pE(k)â†auto grad(E(k),p(k));
17: Update the density coefficients using projected gradient:
p(k+1)â†p(k)âˆ’Îµ
Iâˆ’wwâŠ¤
wâŠ¤w
âˆ‡pE(k);
18: kâ†k+ 1
19:end while
20:return (E,p)at the iteration determined by the stopping criterion.
than MINAO density at initialization. Furthermore, ProjMINAO initialization also converges closely to
the target energy, and finally achieves a better performance than H Â¨uckel initialization. These findings
demonstrate that both H Â¨uckel and ProjMINAO initializations have a better alignment with the training-
density manifold, and thus lead to better generalization performance during density optimization. More
attractively, Fig. S9 implies that M-OFDFT only requires an on-manifold initialization but does not need
projection in each density optimization step, implying its better robustness than previous methods.
To further show the advantage of the density optimization results, we compare M-OFDFT with classi-
cal OFDFT using well-established KEDFs, following the same setting as Fig. S9 except on a different
QM9 molecule. First, we compare the optimization curves in energy by M-OFDFT and OFDFT with
classical KEDFs including TF [4, 5], TF+1
9vW [12], TF+vW [45], and APBE [44], from the conven-
tional MINAO [48] initialization. As illustrated in Supplementary Figure S10(a-b), M-OFDFT converges
closely to the true ground-state energy using both initialization methods, while all classical KEDFs lead
to optimization curves falling below the true ground-state energy. We did not plot the curve for TF+1
9vW
since it diverges so vastly that the curve soon runs out of the shown range.
We further plot the density error along with the optimization process, measured in the L2-metricR
|Ï(r)âˆ’Ïâ‹†(r)|2drfrom the KSDFT ground-state density Ïâ‹†(r)(see Supplementary Section B.3.2 for
calculation details). As shown in Supplementary Figure S10(c-d), M-OFDFT from either initialization
continuously drives the density towards the true ground-state density and results in a small density error,
even though the optimization process is not driven by minimizing the density error (but by minimizing
electronic energy). This indicates that the energy objective of M-OFDFT constructed from the learned
functional model is not artificial in the sense of only producing the correct energy after optimization,
but it also leads to the correct electron density, so the functional holds the desired physical meaning. In
contrast, density error curves by classical KEDFs diverge quickly and present an ascending trend, reveal-
56

ing a problem for applying these functionals to molecular systems. Again, we did not plot the curve for
TF+1
9vW since it soon runs out of the shown range.
A subtlety in the density optimization process is the non-negativity of the density value everywhere in
space. As the basis functions follow the form of the multiplication of a Gaussian radial function which
is always non-negative, and a spherical harmonic function or a monomial (as is the case of the even-
tempered basis in Eq. (S67)) that accounts for the angular anisotropicity which can take negative values.
The coefficients hence need to be within a certain region to guarantee that the represented density function
is non-negative everywhere. Due to the complexity of the basis functions, an explicit expression for
such a constraint is not obvious. We hence implemented a numerical guarantee that enforces the non-
negativity of density value on each grid point, by adding the following artificial energy penalty term to
the minimization objective Eq. (S45) of density optimization:
Enonneg (p,M) :=NgridX
g=1max{âˆ’Ïp(r(g)),0}, (S79)
where Ïp(r) :=P
ÂµpÂµÏ‰Âµ(r)is the electron density function represented by coefficient vector p(see
Eq. (S25)), and {r(g)}Ngrid
g=1is a set of grid points for the molecular structure M. Nevertheless, in our
empirical trials, we found that this additional term is seldom activated during density optimization, and
density optimization without this term already leads to an electron density that is non-negative on almost
all grid points. The number of exceptional grid points is even smaller than that due to density fitting error.
Considering the cost of evaluating density values on grid points, we hence omitted this step.
For ensuring density non-negativity, it is possible to represent the density as the square of linear combi-
nation of the atomic orbital basis functions, as adopted in many existing OFDFT implementations (for
example, refs. [14, 68, 26, 29]). But this would revert the computational complexity to quartic ( O(N4))
due to the Hartree term, and sacrifice the advantage over KSDFT. Future explorations could be expanding
the density onto a set of non-negative-valued basis functions.
Supplementary Section B.5.2 Density Initialization Details
Recall that we introduced two initialization methods in Methods 4.4, H Â¨uckel and ProjMINAO, for sta-
ble density optimization in M-OFDFT. Initialization is worthy of the attention since it should stay in the
training-data manifold of the functional model for reliable prediction. Training data come from eigen-
solutions of effective one-electron Hamiltonian matrix, which motivates the first choice of the H Â¨uckel
initialization [84, 120] which follows the same mechanism. The second choice is to project the initial
density onto the manifold, inspired by previous methods, for example, using local PCA [21, 23] or ker-
nel PCA [67]. But these existing projection techniques is not easily applicable for M-OFDFT since the
density optimization domain is the coefficient space generated by the given molecular structure, which
differs for different molecular structures, so it is not straightforward to determine the training-data man-
ifold within the coefficient space for an unseen molecular structure. We hence use an additional deep
learning model to conduct the projection of the MINAO initialization, which we call ProjMINAO.
To implement the ProjMINAO initialization, we construct an additional output branch âˆ†pÎ¸(p,M)to
predict the required correction to project the given density coefficient ptowards the ground-state density
coefficient pâ‹†for the molecular structure M. Due to the formulation of KSDFT, the ground-state density
corresponds to the eigensolution of an effective Hamiltonian (Supplementary Section A.2), hence is on
the training-data manifold for sure. Projecting towards the ground state could also accelerate convergence
in density optimization. The branch is built on top of the last G3D module shown in Supplementary
Figure S1(a) of the KEDF model. It processes the hidden representations of each atom through an MLP
module to predict the corresponding coefficient difference. The branch is trained to project the density
coefficient data along the SCF iteration of KSDFT onto the corresponding ground-state coefficient:
X
dX
kâˆ†pÎ¸(p(d,k),M(d))âˆ’(p(d,k)âˆ’p(d,â‹†)). (S80)
Note that even though the projection is aimed at the ground state, we do not rely on it for accuracy, as the
functional model could continue optimizing the density (Fig. S9).
Since the MINAO or H Â¨uckel initialization primarily produces orbital coefficients, density fitting (Sup-
plementary Section A.4.1) is required to convert them to density coefficients to complete the density
initialization. Note that this procedure is performed only once for each molecular structure, so the cubic
computational cost of density fitting (and generating H Â¨uckel initialized orbitals) does not dominate over
the quadratic scaling of density optimization on regular workloads.
57

For very large molecules, we also propose an alternative initialization to directly generate the density
coefficients by superposition of isolated-atom densities, in the same spirit of MINAO which uses the
superposition of isolated-atom orbitals. This amounts to concatenating the fitted MINAO density coef-
ficients of each atom as if in isolation, which only has a linear computational cost. This constructed
density coefficients are then fed to the projection branch for the correction, which is the final initialized
density. In practice, this technique drastically reduces the time needed to construct the initial coefficients,
further reducing the overall time consumption of M-OFDFT, and renders only a minor increase of error
(approximately 0.02 kcal/molin per-atom energy error on the interested protein systems in Results 2.4).
The effectiveness of this approach to handle off-the-training-data-manifold issue in density optimization is
demonstrated qualitatively by the convergent curves of energy and density in Fig. S9 and Supplementary
Figure S10, and quantitatively by the results in Results. 2.2 and Results. 2.3. In contrast to some other
deep learning OFDFT methods [21, 67, 23] which require the on-manifold projection in each density
optimization step, M-OFDFT only requires an on-manifold initialization, meaning at most one projection.
This makes the implementation much more efficient especially for large systems, and also indicates that
the learned functional generalizes better since it is more robust to unseen densities.
Supplementary Section B.5.3 Stopping Criterion
Another required specification for density optimization is stopping criterion. Ideally, the solution from
M-OFDFT is expected at the stationary point with zero projected gradient of the electronic energy. But in
practice, the exact zero-gradient point may be missed due to discretization error. Therefore, we propose
a set of stopping criteria to determine the practical stationary point over a chosen number of optimization
steps.
â€¢ For molecules in a similar scale as those in training, the stopping criterion is chosen as the step with
minimal single-step energy update, that is, at the global (over the chosen number of optimization
steps) minimum of single-step energy update. This is the closest step to the stationary point in prac-
tice.
â€¢ For larger-scale molecules than those in training, however, the above criterion is not as effective as on
in-scale systems. The reason is that the reliable coefficient region around the ground-state density for
the model is likely smaller and anisotropic due to the extrapolation risk, so the coefficients may find
a mistaken direction to â€sneak offâ€ the ground state in an extensive optimization. Note that it is hard
to project the confronted density during optimization onto the training-data manifold precisely, since
the manifold of coefficient is unknown for a molecular structure not present in training. Therefore,
we mitigate this problem by controlling the extent of density optimization to balance optimization
effectiveness and reliability. Specifically, we consider two additional criteria: (1)the step where the
projected (onto the tangent space of normalized coefficients; not the training-data manifold) gradient
norm first stops decreasing, that is, the first local minimum of the projected gradient norm. (2)the
step where the single-step energy firststops decreasing, that is, the first local minimum of single-step
energy update. These two criteria characterize a local minimum on the optimization process, hence
represent a certain level of optimization effectiveness. Meanwhile, they are chosen to be the first
encountered point with such characterization, where is likely still in the reliable region for the model.
Based on empirical results on the validation set, we prefer first invoking criterion (1). If this criterion
is not met within the chosen optimization steps, we seek for criterion (2). If this is still not met,
we resort to the original criterion of the global minimum of single-step energy update, which always
exists.
Supplementary Section B.5.4 Density Optimization Hyperparameters
During the deployment stage, we use the ProjMINAO as the initialization choice for all settings in Re-
sults 2. The adopted gradient descent steps and step size Îµare chosen according to the performance of
evaluation datasets. The gradient descent steps and step size are set to 1000 and 1Ã—10âˆ’3for all molecular
datasets and functional variants (including traditional KEDF baselines), with the exception of implement-
ing the learned KEDF models TS,res,Î¸andETXC,Î¸on the ethanol dataset, where the step size Îµis set to
5Ã—10âˆ’4. The vanilla Stochastic Gradient Descent (SGD) optimizer is adopted in the density optimization
process.
Supplementary Section C Related Work
In this section, we give a more detailed discussion on prior works that leverage machine learning to ap-
proximate the kinetic energy density functional (KEDF) for OFDFT. Kernel ridge regression is employed
58

in pioneering works [21, 67, 22, 23, 113, 114], including the extension that leverages kernel gradients [28].
These works have proven the success of the idea of machine-learning OFDFT. Such models can be seen
as non-local, but the costly calculation on grid restricts the applications to 1-dimensional systems. Some
other works fit a linear combination of classical KEDF approximations with explicit expression [121],
including non-local ones [122], but the demonstrated systems are still effectively 1-dimensional. Deep
neural networks have also been explored recently, including multi-layer perceptrons (also called feed-
forward neural networks) [24, 25, 123, 68, 26, 69] and convolutional neural networks [27, 28, 124, 29].
Many of them learn the kinetic energy density at each grid point from semi-local density features on that
point [26], and to compensate for non-local effects, third-order [24, 25, 68] and fourth-order [123] density
derivative features are leveraged. Others consider the interaction of density features at different locations
hence are non-local [27, 28, 124, 69, 29]. Many of such works enable the calculation on 3-dimensional
systems [27, 24, 25, 123, 26, 124, 29], and the lower computational complexity than KSDFT has been
shown empirically [26]. Nevertheless, the demonstrated systems are still limited to tiny molecules of
dozen atoms, with exceptions of ref. [27] with about 30 atoms but restricted to alkanes, and ref. [26]
with a few thousands atoms but on material systems and without accuracy evaluation. The contribution
of our work M-OFDFT is the applicability to diverse and larger molecules (for example, the chignolin
polypeptide containing 168 atoms) with extrapolation capability while maintaining the low complexity of
OFDFT. Technical contributions are summarized in the beginning of Methods 4.
Supplementary Section D Additional Empirical Results
Supplementary Section D.1 In-Scale Results
Supplementary Section D.1.1 Quantitative Results
Supplementary Table S5: Performance of M-OFDFT with different learning targets and initial-
ization configurations, as an extension to the results narrated in Results 2.2. The mean absolute
errors (MAEs) in energy and Hellmann-Feynman (HF) force are listed in kcal/mol andkcal/mol/ËšA,
respectively. The results are calculated on ethanol test structures ( n=10,000) and QM9 test molecules
(n=13,388).
MethodEthanol QM9
Energy HF force Energy HF force
TS,res-HÂ¨uckel 5.27 27.73 9.88 12.73
TS,res-ProjMINAO 0.18 1.18 0.93 2.91
ETXC-HÂ¨uckel 3.95 126.65 9.69 123.51
ETXC-ProjMINAO 0.18 2.07 0.73 4.73
M-NNP 0.10 25.24 0.25 7.72
Functional variants and initialization As mentioned in Results 2.2, M-OFDFT achieves chemical ac-
curacy on unseen conformations (ethanol) and chemicals (QM9) in a similar scale as seen during training,
using a deep learning model targeting the residual KEDF on top of a base KEDF. Here we provide re-
sults under more technical choices in Supplementary Table S5. Firstly, we let the model learn the sum of
the kinetic and XC energy ETXCto get rid of the large prefactor of grid computation for calculation on
large-scale molecules (Supplementary Section B.4.2). This is adopted in time complexity (Results 2.4)
and extrapolation (Results 2.3) studies, and here we list its performance in this in-scale case. Secondly,
as presented in Methods 4.4, two initialization strategies are proposed to address the out-of-distribution
issue of initialized density. The results reported in Results 2.2 are produced by ProjMINAO initializa-
tion, and here we also list the results produced by the H Â¨uckel initialization method, which is one of the
standard initialization methods in common DFT calculation [48, 120], and does not require an additional
deep learning model (or module). We also provide the results of M-NNP, that is, a deep learning model
with the same architecture that directly predicts the ground-state energy of the given molecular structure
in an end-to-end way.
From Supplementary Table S5, we can see that although the ETXCmodel combines with an additional un-
known density functional, it can still be effectively trained, and does not have obvious negative effects on
the accuracy. The energy accuracy is even improved on both datasets given the same initialization method.
For the initialization methods, even without the deep learning shortcut, M-OFDFT still achieves a reason-
able accuracy using the H Â¨uckel initialization. With an additional deep learning module for predicting a
correction, the ProjMINAO initialized density stays close to the ground-state density, so it can yield better
59

results. Finally, M-OFDFT achieves a comparable energy accuracy with the end-to-end method M-NNP,
even though the process to give the energy by M-OFDFT (density optimization using the KEDF model)
is not directly supervised. Notably, on tasks that do not have a direct supervision, M-OFDFT is superior
over M-NNP. As the table shows, M-OFDFT achieves a substantially more accurate HF force estimation
than M-NNP, even though neither has seen HF force label in training. Abundant evidence in Results 2.3
also shows the dominating advantage of M-OFDFT in extrapolation to molecules larger than seen during
training.
Details on the potential energy surface (PES) study As shown in Fig. 2(c) of Results 2.2, M-OFDFT
can accurately reproduce the PESs of ethanol. Here we provide more implementation specifics of the
evaluation process. To benchmark the PES, we generate a series of ethanol structures by varying either
the H CCO torsion angle or the O H bond length, starting from the equilibrium ethanol conforma-
tion (optimized by classical molecular dynamics simulation). The torsion angles are taken uniformly on
[âˆ’180â—¦,180â—¦)with 15â—¦increment, where the 0â—¦angle is defined when the four atoms are on the same
plane and H and O are on the same side. The bond lengths are taken uniformly on [0.856ËšA,0.144ËšA]
with 0.015 ËšA increment. The interval is taken as the range of the O H bond length in the training dataset.
The energy curves in Fig. 2(c) are produced by the residual KEDF TS,res,Î¸version (Supplementary Sec-
tion B.4.1) of M-OFDFT with ProjMINAO density initialization. This setting is consistent with other
results shown in Results 2.2. The curves are notevaluated on densities solved by KSDFT [27, 25].
Geometry optimization study To investigate the utility of M-OFDFT for geometry optimization, we
integrate the M-OFDFT implementation with the geometry optimization framework in PySCF [48],
wherein the HF force (Supplementary Section A.5) by M-OFDFT is used. We generate a set of initial
ethanol structures by varying the H CCO torsion angle from the equilibrium ethanol conformation.
The torsion angles are taken uniformly on [âˆ’180â—¦,60â—¦]with 30â—¦increment. For each initial structure, we
relax the structure using both KSDFT and M-OFDFT for at most 100 steps. For M-OFDFT, the resid-
ual KEDF TS,res,Î¸version (Supplementary Section B.4.1) with ProjMINAO density initialization is used,
which is consistent with other results shown in Results 2.2.
To evaluate the optimized structures by M-OFDFT, we first calculate the rooted mean square deviation
(RMSD) between the optimized structure by M-OFDFT and the optimized structure by KSDFT for each
initial ethanol structure. The mean RMSD value across all the initial structures is 0.07 ËšA, indicating a
good consistency of M-OFDFT with KSDFT. To further evaluate the optimized structures by M-OFDFT,
we compare the bond lengths and angles against those of optimized structures by KSDFT. For a ref-
erence to assess the error, for each bond or angle type, we plot the distribution in the form of violin
plot of the bond length or angle value in the ethanol training dataset. As the dataset is from the MD17
dataset [40, 41], the plot represents the distribution in thermodynamic equilibrium. As depicted in Supple-
mentary Figure S12, the majority of the bond lengths and angles of the optimized structures by M-OFDFT
exhibit good agreement with the results of KSDFT, and align closely with the high-density region of the
corresponding thermodynamic equilibrium distributions. The difference from KSDFT results is also sub-
stantially smaller than the span of the corresponding thermodynamic equilibrium distribution. This result
underscores the practical efficacy of M-OFDFT for geometry optimization.
Comparison with vW as a lower bound As mentioned in Supplementary Section B.4.1, since the
von Weizs Â¨acker (vW) KEDF [46] is a lower bound of the true KEDF [96, Thm. 1.1], taking it as the
base KEDF could inform the TS,resmodel to be non-negative, but unfortunately introduces more training
challenges. Hence it remains to be explored to leverage the lower-bound property of the vW KEDF. Nev-
ertheless, we can empirically verify that our learned KEDF models already satisfy this lower bound. For
this, we present a scatter plot in Supplementary Figure S13, where each point represents the vW KEDF
value (x-axis coordinate) and the kinetic energy value by our learned KEDF model (y-axis coordinate) of
each electron density. The densities for this evaluation are taken as the ground-state densities optimized
by our KEDF model on unseen ethanol test structures (Supplementary Figure S13(a-b)) or unseen QM9
test structures (Supplementary Figure S13(c-d)), following the setting in Results 2.2. Our KEDF model
takes either the residual KEDF version (Supplementary Section B.4.1) with APBE base KEDF (Supple-
mentary Figure S13(a,c)), or the TXC functional version (Supplementary Section B.4.2) which gives the
kinetic energy value by subtracting the EXCvalue from the model-predicted value ETXC(Supplementary
Figure S13(b,d)). The figure clearly shows that in all cases, the kinetic energy values by our KEDF model
are larger than the corresponding vW values, hence our learned KEDF models satisfy this lower bound
property.
Supplementary Section D.1.2 Visualization of Optimized Densities
Radial density plot For more qualitative investigation of M-OFDFT, we provide more visualization re-
sults of the optimized density by M-OFDFT, in addition to Fig. 2(b). Supplementary Figure S14 shows the
60

radial density by spherical integral around the other two heavy atoms in the ethanol molecule, that is, the
Î±-carbon atom (the one connected to the oxygen atom) and the Î²-carbon atom. The results demonstrate
that the density curves of M-OFDFT again align almost identically with the KSDFT density curves. In
contrast, the classical KEDF using APBE for KEDF again leads to a substantial deviation around bonding
regions between peaks of core electrons.
Partial charge and dipole moment visualization As reported in Results 2.2, the optimized density
of M-OFDFT can accurately reproduce Hirshfeld partial charges [47] and dipole moments of molecules.
To further illustrate the results, we provide a representative example in Supplementary Figure S15 of
atomic partial charge on each atom in an unseen test ethanol structure as well as the dipole moment of the
structure, based on the optimized density by M-OFDFT. The results show that both the Hirshfeld partial
charges for each atom and the dipole moment from M-OFDFT are in close agreement with those obtained
from KSDFT in the ethanol molecule.
Supplementary Section D.2 Extrapolation Results
Supplementary Section D.2.1 Results on QMugs and Chignolin
Validation error in the extrapolation experiments As shown in Results 2.3, M-OFDFT has shown
a superior extrapolation performance in energy than end-to-end deep learning counterparts M-NNP and
M-NNP-Den that predict the total energy directly. We emphasize here that this is not due to that M-NNP
and M-NNP-Den are not well trained or overfit to the training dataset. Supplementary Table S6 shows
that M-NNP and M-NNP-Den achieve a better validation error than M-OFDFT, but their extrapolation
errors increase much more vastly than that of M-OFDFT.
Supplementary Table S6: Training, validation and extrapolation error (per-atom energy MAE in
kcal/mol) of our M-OFDFT and reference methods M-NNP and M-NNP-Den on the QMugs and
chignolin extrapolation setups. For the QMugs case, all the methods are trained and validated on two
disjoint splits of QM9 molecules and QMugs molecules with no more than 15 heavy atoms, and are tested
for extrapolation on QMugs molecules ( n=50) with 56-60 heavy atoms. For the chignolin case, all the
methods are trained and validated on two disjoint splits of fragment structures of all peptide lengths (2-5),
and are tested for extrapolation on chignolin structures ( n=1,000). Note that for M-OFDFT, the training
and validation energy errors are from end-to-end model output, while the extrapolation error is from the
result after density optimization using the model, consistent with Fig. 3.
MethodQMugs Chignolin
Train Validation Extrapolation Train Validation Extrapolation
M-NNP 0.008 0.017 1.768 0.003 0.003 0.084
M-NNP-Den 0.004 0.019 1.824 0.002 0.002 0.079
M-OFDFT 0.044 0.055 0.112 0.015 0.016 0.071
Extrapolation performance in HF force We then show the better extrapolation performance of M-
OFDFT in HF force. As shown in Supplementary Figure S16(a), M-OFDFT with functional model trained
on molecules with fewer than 15 heavy atoms consistently maintains a substantially lower HF force MAE
than M-NNP and M-NNP-Den on larger-scale molecules. M-OFDFT also outperforms the two end-to-end
counterparts across various fragment datasets in the chignolin experiment (Supplementary Figure S16(b)).
In the finetuning setting of chignolin, Supplementary Figure S16(c) shows that M-OFDFT always
achieves much better HF force results than M-NNP and M-NNP-Den in each of the three setups. M-
OFDFT attains a substantial 40.8\% HF force error reduction in finetuning over training from scratch,
showing the ability of capturing transferable knowledge from accessible-scale data, and more efficiently
leveraging limited large-scale data. In contrast, the end-to-end deep learning methods exhibit even worse
HF force results when incorporating accessible-scale data, manifesting an extrapolation difficulty to a
larger scale.
Extrapolation comparison to NNP with other architectures Results 2.3 have demonstrated the qual-
itatively better extrapolation performance of M-OFDFT than direct energy prediction, that is, the neural
network potential (NNP) formulation, using the same model architecture as M-OFDFT, denoted as M-
NNP. We further verify that this conclusion still holds even using more advanced and recent architec-
tures for NNP. We consider Equivariant Transformer (ET) [38] and Equiformer [39], which are recently
proposed NNP architectures that have shown remarkable performance and competitiveness in the field.
Notably, ET is one of state-of-the-art equivariant NNP architectures that use Cartesian vector features to
61

maintain SE(3) -equivariance, and Equiformer is a cutting-edge approach amongst NNP architectures that
leverage high-order spherical harmonics tensors to encode molecular features.
Following the setting of Fig. 3(a) introduced in Results 2.3, we train the NNP models on QM9 and QMugs
molecules with no more than 15 heavy atoms, and test them on increasingly larger molecules from the
QMugs dataset. Results in Supplementary Figure S17 demonstrate that although using the more advanced
architectures improves the performance over M-NNP, the error in the extrapolation cases is still larger than
that of M-OFDFT, and the error still increases with molecule size, while the error of M-OFDFT keeps
constant or even decreasing.
Evaluation of electron density in the chignolin experiment For a thorough assessment of the ex-
trapolation capability of M-OFDFT, we evaluate the electron density solved by M-OFDFT on peptide
structures in various lengths. As peptides are relatively large molecules, it is inconvenient to directly
visualize the densities. We hence calculate the Hirshfeld partial charge [47] and dipole moment from the
solved density.
This evaluation is conducted in parallel with the setting in Results 2.3. To construct the evaluation bench-
mark, we prepare a test set encompassing a range of short-peptide structures, from dipeptides to pen-
tapeptides, as well as chignolin structures (of length 10). We sample 50 structures for each category of
peptides. More details about the peptide structures are described in Methods 4.5.4. For solving test pep-
tide structures of lengths 2 to 5, we apply the total energy functional model Etot,Î¸trained on all training
peptide structures (of lengths 2-5), following the same setting as Fig. 3(c). For solving test chignolin
structures, we further finetune the model on 800 training chignolin structures. We take KSDFT results to
evaluate error, and compare the results with the classical OFDFT using the TF+1
9vW KEDF. Results in
Supplementary Table S7 demonstrate that M-OFDFT consistently outperforms the classical OFDFT over
peptides of all lengths, in terms of the accuracy on these density-related quantities. The partial charge
MAE of M-OFDFT is substantially lower, by two orders of magnitude. This substantial improvement
further underscores the power of M-OFDFT.
Supplementary Table S7: Hirshfeld partial charge and dipole moment results in mean absolute
error (MAE) from KSDFT. The units for partial charge and dipole moment are eandD, respectively.
Each dataset contains 50 structures.
Test dataset Quantity M-OFDFT TF+1
9vW
DipeptidePartial charges 2.62Ã—10âˆ’30.147
Dipole moment 0.217 2.970
TripeptidePartial charges 2.68Ã—10âˆ’30.153
Dipole moment 0.283 3.556
TetrapeptidePartial charges 2.68Ã—10âˆ’30.139
Dipole moment 0.390 3.420
PentapeptidePartial charges 2.85Ã—10âˆ’30.141
Dipole moment 0.543 3.474
ChignolinPartial charges 3.32Ã—10âˆ’30.132
Dipole moment 1.077 12.049
Supplementary Section D.2.2 Additional Extrapolation Study from QM9
Results on charged molecules As clarified in Supplementary Section B.1, the input atom types Z
and positions Xare only used to inform the KEDF model of the types and centers of the atomic basis
functions under which the expansion coefficient input is defined, but not for the physics of the actual
atoms. This formulation makes M-OFDFT inherently general for handling input densities from either
neutral or charged molecular systems, just by feeding the KEDF model with expansion coefficients of the
corresponding electron density onto the atomic basis functions whose types and positions are specified by
ZandX, even if the KEDF model is trained on data only from neutral molecules.
Here, we demonstrate the efficacy of M-OFDFT in handling charged molecules. To construct an evalu-
ation benchmark, we randomly select five unseen carboxylic acid molecules from the QM9 test set, and
deprotonate the hydrogen cation from the carboxyl group ( C(O)OH) of each molecule, thereby generat-
ing five carboxylate anions ( C(O)Oâ€“). We employ the TXC functional model ETXC,Î¸trained on the QM9
training set, which comprises neutral molecules only, to solve these charged systems. We initialize the
62

electron density using ProjMINAO (Methods 4.4), which projects the electron density from the MINAO
initialization onto the training-data manifold by a deep learning model. Since MINAO gives the initial
electron density by treating the system as neutral, we rescale the density coefficients produced by Proj-
MINAO to normalize to the correct number of electrons of the charged system. Note that in the density
optimization process (Eq. (2)), the number of electrons is kept throughout, so the ground-state density so-
lution also respects the correct charge of the system. We evaluate the performance of M-OFDFT in terms
of the mean absolute error (MAE) from KSDFT results over the five systems in the energy difference
between the neutral and the corresponding charged system.
The result is that M-OFDFT achieves a 3.80 kcal/mol MAE of energy difference. In comparison, the
result of the classical OFDFT using the TF+1
9vW KEDF is 30692.16 kcal/mol, an error of five orders
larger. This result indicates that M-OFDFT trained on neutral systems is still effective in handling charged
molecules, an extrapolation capability of a new kind. The capability for charged molecules can be further
improved if data from charged molecules are included.
Results on unseen chemical environments Extrapolating a trained machine-learning model to unseen
chemical environments, for example, bond types not present in training data, is another challenging ex-
trapolation evaluation. To investigate this type of extrapolation capability of M-OFDFT, we apply M-
OFDFT to the carbon monoxide molecule CO, which contains a triple bond C O that is not encountered
in training the KEDF model. The initial COstructure is generated using the RDKit software [125] which
gives the bond length of 1.118 ËšA. We then augment four additional COstructures by adjusting bond
lengths to 1.102 ËšA, 1.112 ËšA, 1.122 ËšA and 1.132 ËšA, containing both squeezed and stretched bond lengths.
The residual KEDF TS,Î¸model trained on QM9 training set is used to solve these systems. The H Â¨uckel
method is chosen for density initialization, which exhibits better robustness to various bond lengths than
the ProjMINAO initialization in our trials.
We evaluate the results in mean absolute error (MAE) with respect to KSDFT results. The Hirshfeld
partial charge MAE and dipole moment MAE of optimized densities by M-OFDFT over the five CO
structures are 0.102 eand 0.150 D, respectively. As a reference, these MAE numbers are 0.296 eand
0.496 Dwhen using the classical OFDFT with the TF+1
9vW KEDF. Hence M-OFDFT still achieves a
substantial improvement over classical OFDFT even in this extrapolation scenario.
It should be noted that neither charged molecular systems nor the triple bond C O has been encountered
in our training data, thus they are indeed challenging extrapolation tasks. While the formulation of M-
OFDFT is designed to be universally applicable to all densities and molecular systems, its performance as
a neural network model is hard to completely avoid extrapolation error. This is reflected in the (absolute)
energy MAE of M-OFDFT on charged QM9 molecules, which stands at 3.79 kcal/mol, higher than
0.73kcal/molon neutral molecules from the same dataset. Despite this, the extrapolation performance of
M-OFDFT is still reasonable, and is still substantially better than classical OFDFT methods, showcasing
the potential of M-OFDFT in these more challenging scenarios. The performance of M-OFDFT on these
systems can be further improved by enriching the train data with charged molecules and new bond patterns
such as triple bond C O, which will be investigated in future work.
Supplementary Section D.3 Empirical Time Cost Results
Settings As mentioned in Results 2.4, we aim to demonstrate the lower time complexity of M-OFDFT
compared to KSDFT by empirically benchmarking the two methods on two types of large molecular
systems. For KSDFT calculations on all systems, we adopt the same settings as introduced in Meth-
ods 4.6. M-OFDFT calculations are benchmarked on a 32-core CPU server with 216 GiB memory, which
is equipped with one Nvidia A100 GPU with 80 GiB memory. Fig. 4 is produced by running M-OFDFT
and KSDFT on QMugs molecules. For QMugs molecules, since they are generally much larger than
ethanols and QM9 molecules to the extent that the residual KEDF TS,res,Î¸formulation becomes substan-
tially costly due to grid-based computation for TS,base andEXC, we apply the learned TXC functional
model ETXC,Î¸in the same setting as is used to produce Fig. 3(a).
Time cost of each computational component To better understand the structure of the time cost in the
density optimization process (that is, the process to use M-OFDFT to solve a queried molecular system),
we split the time cost into various computational components in M-OFDFT. Both the residual KEDF
TS,res,Î¸formulation (Supplementary Section B.4.1) and the TXC functional ETXC,Î¸formulation (Supple-
mentary Section B.4.2) of M-OFDFT are considered. As shown in Supplementary Figure S18(a), in the
TS,res,Î¸formulation, the three major parts of the time cost are the evaluation of the XC functional (denoted
as â€œEXCâ€), the evaluation of the base KEDF (denoted as â€œTs-Baseâ€), and the evaluation of the TS,res,Î¸
model (denoted as â€œML-Predâ€). Noting that the first two components are evaluated on grid, we conclude
that grid-based computation is the main restriction to running M-OFDFT on large molecules, conform-
63

ing to Supplementary Section B.4.2, hence the TXC functional ETXC,Î¸formulation is motivated, which
does not require any grid-based computation. We also note that grid-based computation also occupies a
substantial amount of GPU memory (Supplementary Section B.4.2). Using the hardware specified above,
we can only afford systems of up to 230 electrons under the TS,res,Î¸formulation, which is where the plot
ends. To compare the component-wise time cost of the TS,res,Î¸formulation and the ETXC,Î¸formulation,
we conduct the same analysis with the ETXC,Î¸KEDF model. As shown in Supplementary Figure S18(b),
due to the removal of grid-based computations, the total running time is substantially reduced. We note
that in this case, the evaluation of the ETXC,Î¸model takes the largest computational cost. This part has
anO(N2)computational complexity due to the need for non-local calculation. As the molecular size
increases, this could lead to considerable computational demands. Despite the importance of the non-
local calculation (Supplementary Section D.4.2), its influence presumably does not extend infinitely, thus
allowing us to reduce the complexity by using a distance cutoff for large molecular systems. Specifically,
with a distance cutoff rc, the Transformer-based model, Graphormer, can be modified to capture non-
local interactions between one atom and its neighboring atoms within the cutoff. The complexity is then
O(AArc), where Arcis the average number of neighboring atoms within the distance cutoff rc. Asrcis
taken as a constant, Arcis a constant. Hence the modification reduces the complexity of the model to lin-
ear:O(AArc) =O(A) =O(N). We also note that analogous approaches to trim the neighborhood based
on distance cutoffs have been utilized to achieve linear cost scaling using the Transformer architecture in
the realm of machine learning [126, 127], which pave the way for further improvement of M-OFDFT.
Details on the time evaluation on proteins To further highlight the scaling advantage of M-OFDFT,
we evaluate it on two large-scale protein molecular systems containing 709 and 738 atoms. The all-atom
conformations of these systems are obtained from Lindorff-Larsen et al. [52] and neutralized following
the same pipeline for processing chignolin structures (detailed in Methods 4.5.4). Such a scale is already
seldom encountered in the context of KSDFT calculations. Considering the substantial scale difference
between the protein systems evaluated and the molecules used during training, we employ the total energy
functional model Etot,Î¸(p,M)trained on all chignolin fragments and finetuned on 800 chignolin struc-
tures. This model is expected to provide the best extrapolation capacity within our available resources.
The results indicate that M-OFDFT achieves a 25.6-fold and 27.4-fold speedup over KSDFT on the
two protein systems, respectively. The per-atom energy MAE of M-OFDFT on the two test systems
is 0.23 kcal/mol and 0.31 kcal/mol, respectively. While higher than the error on the chignolin case,
the result still demonstrates a substantial advantage over M-NNP, which gives per-atom energy MAE of
0.36kcal/moland 0.63 kcal/mol, respectively.
Supplementary Section D.4 Ablation Study
Supplementary Section D.4.1 Multi-Step Data and Gradient Label
To capture the energy landscape in the density coefficient space, we generate multiple psamples for
each molecular structure Mand compute the gradient label âˆ‡pTSfor additional supervision informa-
tion. An ablation experiment is conducted on the ethanol dataset to investigate the importance of each
type of supervision by excluding the supervision label during the training process. The ablation results
are presented in Supplementary Table S8. We observe that removing the gradient label âˆ‡pTSleads to a
considerable decrease in energy and HF force accuracy for both density initialization strategies, empha-
sizing the importance of maintaining the optimization of M-OFDFT on a physical track. Incorporating
multi-step psamples is also crucial for enhancing the performance of M-OFDFT, particularly for the
ProjMINAO initialization, as the accuracy of the density corrector model depends on the size of training
densities.
Supplementary Section D.4.2 Non-locality
Considering the non-local dependency of KEDF on electron density, it is essential to incorporate non-
local calculations in OFDFT, which motivates us to use Graphormer as our backbone architecture. To
demonstrate the importance of non-locality, we conduct an ablation study by gradually decreasing the
receptive field (that is, the distance cutoff of atom neighborhood) of the functional model (specifically,
the TXC model (Supplementary Section B.4.2)) of M-OFDFT. The experiment follows the extrapolation
setting in parallel with Supplementary Table S6, that is, the functional model is trained on molecules with
no more than 15 heavy atoms and the resulting M-OFDFT is tested on 50 QMugs molecular structures
with 56-60 heavy atoms. As illustrated in Supplementary Figure S19, both the energy and HF force
prediction error tend to worsen as the distance cutoff decreases, providing empirical evidence in our
setting that a non-local model is indeed indispensable for well approximating a functional involving the
kinetic energy density functional (KEDF).
64

Supplementary Table S8: Ablation study for various data augmentation strategies. All results are
evaluated on test ethanol molecules with the learned TS,res,Î¸model. The MAEs in energy and HF force
are listed in kcal/mol andkcal/mol/ËšA, respectively.The results are calculated on ethanol test structures
(n=10,000).
Multi-step pGradient âˆ‡pTSDensity Initialization Energy HF Force
âœ“ âœ“HÂ¨uckel 5.27 27.73
ProjMINAO 0.18 1.18
âœ“HÂ¨uckel 166.84 144.78
ProjMINAO 170.83 143.67
âœ“HÂ¨uckel 6.43 32.83
ProjMINAO 20.61 38.56
Supplementary Section D.4.3 Local Frame and Enhancement Modules
As discussed in Methods 4.2, the input coefficients are tensors equivariant to rotation, but the output
energy is a scalar hence invariant, which requires the model to have such invariance built-in. In addition,
the vast range of gradient data makes optimizing the KEDF model challenging. A CoefficientAdapter
module is proposed to guarantee the geometric invariance of the model and express the vast gradient
range. We conducted an ablation experiment to study the importance of each component in this module.
As shown in Supplementary Table S9, utilizing the local frame results in a considerable performance
improvement compared to the baseline model using the standard global frame (for example, derived
by PCA on the atom coordinates), demonstrating the effectiveness of the local frame in reducing the
geometric variability of input data.
Furthermore, since different basis functions have varying importance in representing a density, some co-
efficient dimensions can substantially influence the density function and the electronic energy. This is
difficult for M-OFDFT to handle, as the model treats each dimension equally. During density optimiza-
tion, this influence is amplified when the input density is far from the ground state. We address this
issue by introducing the natural reparameterization technique to balance the impact of each coefficient
dimension. The results in Supplementary Table S9 show that implementing natural reparameterization
can substantially improve the performance of the model, especially for the H Â¨uckel initialization, which
is far from the ground-state density. Here, the energy MAE is reduced by one or two orders of magni-
tude, highlighting the powerful capability of natural reparameterization in balancing sensitivities across
coefficient dimensions.
We also attempted to examine the importance of the other two enhancement modules, the atomic reference
module and the dimension-wise rescaling module, but found it difficult to optimize the learning objective
when removing either module from the model. Consequently, the unreasonable evaluation results are
omitted in Supplementary Table S9. Moreover, the substantial impact on expressing a vast gradient range
and the quantitative results for reducing geometric variability, as clarified in Supplementary Section B.3.1,
also strongly support the necessity of these two modules.
Supplementary Table S9: Ablation study for components in the CoefficientAdapter module. All
results are evaluated on test ethanol molecules with the learned TS,res,Î¸model. The MAEs in energy and
HF force are listed in kcal/mol andkcal/mol/ËšA, respectively.The results are calculated on ethanol test
structures ( n=10,000).
Local Frame Nat. Reparam. Density Initialization Energy HF Force
HÂ¨uckel 567.69 73.17
ProjMINAO 0.57 2.02
âœ“HÂ¨uckel 514.56 36.55
ProjMINAO 0.30 1.29
âœ“HÂ¨uckel 25.41 37.18
ProjMINAO 0.18 1.81
âœ“ âœ“HÂ¨uckel 5.77 27.73
ProjMINAO 0.18 1.18
65

Supplementary Section D.4.4 Results Using Other Training Strategies
As stressed in the main paper, one of the additional challenges for learning a functional model beyond con-
ventional machine learning tasks is that the model is used to construct an objective function for optimizing
the input, hence in addition to accurate end-to-end prediction on a discrete set of input queries, the model
also needs to capture the output landscape (tendency of change in each locality) to properly guide the
optimization. To address this challenge, we introduced techniques to generate multiple density datapoints
each also with a gradient label for each molecular structure (Methods 4.1, Supplementary Section A.4),
which provides a more holistic depiction of the output landscape. While there are alternative methods to
regularize the optimization behavior in the context of learning the XC functional model [71, 70, 72, 66],
our experiments indicate that they are not as effective for our task.
Kirkpatrick et al. [71] design an SCF loss based on second-order perturbation theory to regularize the
functional stationary at provided self-consistent solutions. This effectively acts as a gradient supervision
at the ground state, that is, the given self-consistent orbitals, for each molecular structure. The SCF loss
itself is not directly suitable for learning the KEDF model as it is for supervising the gradient with respect
to orbitals but not density (also explained by del Mazo-Sevillano and Hermann [69]). Approaches under
the same spirit to supervise the gradient at the electronic ground state of each molecular structure are
also explored in learning a KEDF model [28, 68, 26, 69]. In contrast, M-OFDFT introduces gradient
labels on multiple electron density states, instead of only the ground-state density, for each molecular
structure, which provides richer and broader landscape information, leading to a substantial performance
improvement as shown in our ablation study results in Supplementary Table S8 (row 1 and row 3).
Other studies shape the output landscape and regularize the optimization behavior by directly supervising
the model-optimized energy and density (or orbitals). This approach supervises the end goal, but un-
fortunately complicates the learning process of the model, since the dependency of the model-optimized
results on model parameters is nested iteratively following an optimization iteration. Some works resort
to gradient-free optimization methods (for example, evolutionary-style algorithms) to optimize the model
parameters [70], which are not as efficient as gradient-based optimization. Li et al. [72] choose to directly
invoke automatic differentiation on the loss with respect to model parameters through the optimization
process of orbitals, which is, in the context of learning the XC functional for KSDFT, typically conducted
through the self-consistency field (SCF) iteration that solves the Kohn-Sham equations, and hence called
the Kohn-Sham regularizer. We tried its counterpart for learning the KEDF for OFDFT, by supervising the
model-optimized energy and density after the density optimization process using gradient descent (note
SCF iteration is unnatural here), and then applying automatic differentiation through the gradient-descent
iteration. In our experiments on QM9, we found this incurs substantially more computational cost for
computing the gradient for model parameters. Even only using the ETXCversion of functional model
(Supplementary Section B.4.2) to get rid of the costly grid-based computation, only a few number of
density optimization steps up to 8 is affordable. With this maximally affordable cost, this strategy could
work when applied to an end-to-end pretrained model on the energy and gradient labels, as we observed
improved energy accuracy on held-out test molecules using 8 steps of density optimization. However,
an undesired observation indicates that the model learned in this way still does not meet the goal: the
density optimization process often does not converge, and even when it does, the accuracy is much worse
than that at the 8-th step. This makes the method, instead of learning a density functional that holds a
physical meaning, more like an end-to-end ground-state energy predictor constructed by unrolling the
machine-learning model 8 times through the gradient descent process.
Under the same idea to supervise model-optimized energy, Chen et al. [66] find it is possible to avoid
automatic differentiation through the iterative orbital optimization process if the resulting energy EÎ¸(Câ‹†
Î¸)
is indeed optimal in the view of the current functional model, since âˆ‡Î¸EÎ¸(Câ‹†
Î¸) = ( âˆ‡Î¸EÎ¸)
Câ‹†
Î¸+
(âˆ‡Î¸Â¯Câ‹†
Î¸)âŠ¤(âˆ‡Â¯CEÎ¸)Â¯Câ‹†
Î¸= (âˆ‡Î¸EÎ¸)
Câ‹†
Î¸(Â¯Cdenotes the vector of flattened Cmatrix, and âˆ‡Î¸Â¯Câ‹†
Î¸is the
Jacobian matrix) as the optimality condition indicates (âˆ‡Â¯CEÎ¸)Â¯Câ‹†
Î¸= 0on the admissible space of orbital
coefficients, so âˆ‡Î¸Â¯Câ‹†
Î¸is avoided. The same argument also applies to the optimization process of density
coefficients. Since finding the optimal orbitals after every model update is expensive, the method opts to
optimize the model and density alternately. However, this modification biases the orbital/density optimal-
ity thus the optimization process may also deviate from being effective. In our experiment on chignolin,
we iterated the alternate optimization for four steps (and even carefully optimized the density in each step
with a large cost), but observed only marginal performance improvement. Specifically, the performance
gains from each step are 0.5\%, -0.06\%, 1.7\%, and -0.8\%, respectively, where the negative signs indicate
that some steps even worsen the results.
66

0 3 6 9 12
Density coefficient dimension1.00
0.75
0.50
0.25
0.000.250.500.751.00Reduction ratio of coefficient feature scalep orbitals
d orbitals(a)H atom
036912151821242730333639424548515457606366697275788184879093
Density coefficient dimension1.00
0.75
0.50
0.25
0.000.250.500.751.00Reduction ratio of coefficient feature scalep orbitals
d orbitals
f orbitals
g orbitals
(b)C atom
0369121518212427303336394245485154576063666972757881848790939699102
Density coefficient dimension1.00
0.75
0.50
0.25
0.000.250.500.751.00Reduction ratio of coefficient feature scalep orbitals
d orbitals
f orbitals
g orbitals
(c)N atom
0369121518212427303336394245485154576063666972757881848790939699102
Density coefficient dimension1.00
0.75
0.50
0.25
0.000.250.500.751.00Reduction ratio of coefficient feature scalep orbitals
d orbitals
f orbitals
g orbitals
(d)O atom
Supplementary Figure S4: Coefficient scale changes by the use of the local frame. For each coef-
ficient dimension Ï„of atom type Z, the height of the bar represents the relative change in scale, calcu-
lated asstdcoeffâ€²
Z,Ï„âˆ’stdcoeff Z,Ï„
stdcoeff Z,Ï„, where stdcoeff Z,Ï„:= std {p(d,k)
a,Ï„}a:Z(a)=Z, k, d andstdcoeffâ€²Z,Ï„:=
std{pâ€²(d,k)
a,Ï„}a:Z(a)=Z, k, d measure the coefficient scale before and after being transformed by the local
frame transformation, respectively. A negative value indicates a reduction in the scale of the coefficient
due to the local frame transformation. Importantly, the local frame substantially decreases the coefficient
scale across diverse basis dimensions and atom types.
67

0 3 6 9 12
Density coefficient dimension1.00
0.75
0.50
0.25
0.000.250.500.751.00Reduction ratio of gradient label scalep orbitals
d orbitals(a)H atom
036912151821242730333639424548515457606366697275788184879093
Density coefficient dimension1.00
0.75
0.50
0.25
0.000.250.500.751.00Reduction ratio of gradient label scalep orbitals
d orbitals
f orbitals
g orbitals
(b)C atom
0369121518212427303336394245485154576063666972757881848790939699102
Density coefficient dimension1.00
0.75
0.50
0.25
0.000.250.500.751.00Reduction ratio of gradient label scalep orbitals
d orbitals
f orbitals
g orbitals
(c)N atom
0369121518212427303336394245485154576063666972757881848790939699102
Density coefficient dimension1.00
0.75
0.50
0.25
0.000.250.500.751.00Reduction ratio of gradient label scalep orbitals
d orbitals
f orbitals
g orbitals
(d)O atom
Supplementary Figure S5: Gradient scale changes by the use of the local frame. For each
coefficient dimension Ï„for atom type Z, the height of the bar represents the relative change
in the gradient scale, calculated bymax absgradâ€²
Z,Ï„âˆ’max absgradZ,Ï„
max absgradZ,Ï„, where max absgradZ,Ï„:=
maxâˆ‡pa,Ï„T(d,k)
S	
a:Z(a)=Z, k, dandmax absgradâ€²
Z,Ï„:= maxâˆ‡pa,Ï„Tâ€²(d,k)
S	
a:Z(a)=Z, k, dmea-
sure the gradient scale before and after being transformed by the local frame transformation, respectively.
The local frame transformation notably reduces the gradient scale across a wide range of basis dimensions
and atom types.
68

0 100 200 300 400 500
Density coefficient dimension0200400600800100012001400Maximum absolute value of gradients
p
d
f
g(a)Gradient scales after processed by local frame, natural reparameterization, and atomic
reference model (before dimension-wise rescaling).
0 100 200 300 400 500
Density coefficient dimension0.00.51.01.52.02.5Maximum absolute value of gradients
p
d
f
g
(b)Gradient scales further after dimension-wise rescaling.
0 100 200 300 400 500
Density coefficient dimension0100200300400500Maximum absolute value of coefficients
p
d
f
g
(c)Density coefficient scales further after dimension-wise rescaling.
Supplementary Figure S6: Gradient and density coefficient scales over dimensions on the QM9
dataset in the setting of learning residual KEDF with APBE base KEDF. The maximum absolute
value is used to measure the maximum scale of data. (a)-(b) present the gradient scale before and after
the dimension-wise rescaling transformation. This technique properly balances the scales of gradient
and density coefficient in each dimension, hence reduces the gradient scale for easier learning, while
maintaining a reasonable density coefficient scale. (c)shows the corresponding coefficient scale after the
dimension-wise rescaling transformation.
69

010 20 30 40 50 60 70 80 90100 110 120 130 140 150
Density coefficient dimension1.00
0.75
0.50
0.25
0.000.250.500.751.00Reduction ratio of gradient label scales
pSupplementary Figure S7: Gradient scale changes by the use of the atomic reference mod-
ule. For coefficient dimension Ï„of the atom type Z, the height of the bar is also calculated by
max absgradâ€²
Z,Ï„âˆ’max absgradZ,Ï„
max absgradZ,Ï„, where max absgradZ,Ï„:= maxâˆ‡pa,Ï„T(d,k)
S	
a:Z(a)=Z, k, dand
max absgradâ€²
Z,Ï„:= maxâˆ‡pa,Ï„Tâ€²(d,k)
S	
a:Z(a)=Z, k, dmeasure the gradient scale before and after
being centralized by the atomic reference module, respectively. The atomic reference module consider-
ably covers the vast gradient scale.
70

0 100 200 300 400 500
Density coefficient dimension0.000.250.500.751.001.251.501.752.00Maximum absolute value of gradient1e7
s
p
d
f
g(a)Gradient scales after processed by local frame, natural reparameterization, atomic refer-
ence model, and dimension-wise rescaling, in parallel with Supplementary Figure S6(b).
0 100 200 300 400 500
Density coefficient dimension02004006008001000Maximum absolute value of coefficients
p
d
f
g
(b)Density coefficient scales after processed by local frame, natural reparameterization,
atomic reference model, and dimension-wise rescaling, in parallel with Supplementary Fig-
ure S6(c).
Supplementary Figure S8: Gradient and density coefficient scales over dimensions on the QM9
dataset in the setting of learning residual KEDF with the vW base KEDF. The maximum absolute
value is used to measure the maximum scale of data. (a)and(b)respectively present the gradient scales
and density coefficient scales after processed by local frame and all enhancement modules (natural repa-
rameterization, atomic reference model, and dimension-wise rescaling), in parallel with Supplementary
Figure S6(b) and (c) for APBE base KEDF respectively. Although these techniques have reduced the
original gradient scale under a reasonable density scale, the processed gradient scale is still exceedingly
largeâˆ¼107for a neural network to learn.
71

0 200 400 600 800 1000
M-OFDFT density optimization step250
0250500750100012501500Relative energy (kcal/mol)0 20 40 60 80 10020
10
01020KSDFT Energy
MINAO
Huckel
ProjMINAOSupplementary Figure S9: Typical density optimization curves of M-OFDFT for a QM9 molecule
with different initialization methods. MINAO, the common KSDFT initialization, leads the optimiza-
tion to a large gap from the target energy, since it is not from the eigensolution to an effective Hamilto-
nian matrix, hence lies off the training-data manifold (out of distribution). H Â¨uckel initialization solves
an eigenvalue problem, which indeed converges the curve with a much smaller energy error of 5.34
kcal/mol. ProjMINAO initialization uses a deep learning model to project the MINAO density onto the
training-data manifold, which also converges the curve and achieves the best result of 0.60 kcal/mol
energy error. The inset figure highlights the role of density optimization even though the ProjMINAO
density is close to the ground-state density.
72

abcdSupplementary Figure S10: Density optimization curves by M-OFDFT and OFDFT with classical
KEDFs. The figures come in parallel with Fig. S9 but on a different QM9 molecule. (a)Curves in
relative energy along the density optimization process of M-OFDFT with different initialization methods.
M-OFDFT converges closely to the true ground-state energy using either initialization method. (b)Curves
in relative energy along the density optimization process of OFDFT using classical KEDFs. The curves
run below the true ground-state energy with a large gap. (c)Curves in density error along the density
optimization process of M-OFDFT with different initialization methods. M-OFDFT converges closely to
the true ground-state density using either initialization method, even though the optimization is not driven
by minimizing the density error. (d)Curves in density error along the density optimization process of
OFDFT using classical KEDFs. The curves diverge quickly. Curves for TF+1
9vW are omitted in (b)and
(d)since they soon run out of the shown range.
train1
Nheavy15
train2
Nheavy20
train3
Nheavy25
train4
Nheavy30
020000400006000080000100000120000Number of molecular structuresQM9([1,9])
bin1([10,15])bin2([16,20])
bin3([21,25])bin4([26,30])
Supplementary Figure S11: Compositions of training datasets from QM9 and QMugs for the ex-
trapolation setting in Fig. 3(b). The training datasets contain the same number of datapoints (molecular
structures), while involve increasingly larger molecules. The proportion of molecules in each scale range
(that is, QM9, bin1-bin4) respects the proportion in the joint dataset. The range of the number of heavy
atoms in each data source is listed in the figure (that is, [1,9]for QM9).
73

1.0 1.2 1.4 1.6
Bond length (Ã…)C-C
C-O
C-H
O-HBond typeKSDFT
M-OFDFT
80 90 100 110 120 130
Angle (Degree)C-C-O
C-C-H
H-C-H
H-C-OAngle typeKSDFT
M-OFDFTSupplementary Figure S12: Bond lengths and bond angles of ethanol structures by geometry op-
timization with M-OFDFT and KSDFT. Different points represent the optimizes results from differ-
ent initial structures. The violin plots in the background depict the distributions of the corresponding
bond lengths or angles under thermodynamic equilibrium, which are calculated on ethanol structures
(n=20,000). The vertical dashed lines represent the quantiles. The mean RMSD between the optimized
structure by M-OFDFT and by KSDFT over the initial structures is 0.07 ËšA.
a
cb
d
Supplementary Figure S13: Kinetic energy value comparison between M-OFDFT and the vW
KEDF. Each point represents the vW KEDF value (x-axis coordinate) and the kinetic energy value by
our learned KEDF model (y-axis coordinate) of each electron density. Two versions of our learned KEDF
model are considered, including the residual KEDF version (a,c) and the TXC functional version (b,d) .
The densities for evaluation come from ground-state densities optimized by our KEDF model on 10,000
unseen ethanol test structures (a-b) or 13,388 unseen QM9 test structures (c-d) .
74

0.0 0.5 1.0 1.5 2.0
C CO
Distance from nucleus (Ã…)01020304050Radial density (Ã…1)
(a)Î±-carbon
0.0 0.5 1.0 1.5 2.0
C C
Distance from nucleus (Ã…)01020304050Radial density (Ã…1)
KSDFT
APBE
M-OFDFT (b)Î²-carbon
Supplementary Figure S14: Additional visualization of the density optimized by various methods.
Integrated density on spheres of varying radii around each of the two carbon atoms in an ethanol structure
is plotted in each figure, in parallel with Fig. 2(b).
0.017
0.017
-0.211
-0.214
0.040
0.039-0.098
-0.0990.031
0.0280.043
0.041
0.028
0.0260.009
0.0070.159
0.157
red: M-OFDFT; gray: KSDFTH
 O
 C
Partial charges ( e)
Dipole moment (D)
[-0.368, 1.164, 0.921]
[-0.396, 1.137, 0.915 ]
Supplementary Figure S15: Visualization of Hirshfeld atomic partial charge on each atom in an
ethanol structure as well as the dipole moment of the structure. Both the atomic partial charges and
the dipole moment derived from the solved electron density by M-OFDFT align closely with those solved
by KSDFT.
75

1621263136414651566166717681869196101
Number of heavy atoms in test246810121416HF force MAE (kcal/mol/Ã…)
M-NNP M-NNP-Den M-OFDFT(a)QMugs (fixed training dataset, in parallel with
Fig. 3(a))
2.0 2.5 3.0 3.5 4.0 4.5 5.0
Maximum length of peptides in training20406080HF Force (kcal/mol/Ã…)
M-PES
M-PES-Den
M-OFDFT (b)Chignolin (fixed test dataset, in parallel with Fig. 3(d))
Pretrain FromScratch Finetune
M-OFDFT05101520HF force MAE (kcal/mol/Ã…)
-40.8\%
Pretrain FromScratch Finetune
M-NNP-Den+4.7\%
Pretrain FromScratch Finetune
M-NNP+2.7\%
(c)Chignolin (finetuning setting, in parallel with Fig. 3(e))
Supplementary Figure S16: Extrapolation performance in HF force of M-OFDFT compared with
M-NNP and M-NNP-Den on the two extrapolation settings. The data are mean and the shades as
well as error bars in all figures show 95\% confidence intervals. (a)HF force MAE on increasingly larger
molecules from the QMugs dataset, using models trained on molecules with no more than 15 heavy
atoms from QM9 and QMugs datasets, in parallel with Fig. 3(a). Each value is calculated on 50 QMugs
molecules. (b)HF force MAE on chignolin structures ( n=1,000), using models trained on a series of
datasets including increasingly longer peptides, in parallel with Fig. 3(d). (c)HF force MAE on chignolin
structures ( n=50), using models trained on all peptides without (â€˜Pretrainâ€™) and with (â€˜Finetuneâ€™) fine-
tuning on 500 chignolin structures, in parallel with Fig. 3(e). Also marked are error reduction ratios by
the finetuned models over models trained from scratch (â€˜FromScratchâ€™) on the 500 chignolin structures
only.
76

1621263136414651566166717681869196101
Number of heavy atoms in test0.00.51.01.52.0Per-atom energy MAE (kcal/mol)
Trained with 15 heavy atoms
M-NNP ET Equiformer M-OFDFTSupplementary Figure S17: Extrapolation performance of M-OFDFT compared to other advanced
deep learning architectures. Each line denotes the mean absolute error (MAE) in per-atom energy on
increasingly larger molecules from the QMugs dataset, using a model trained on molecules with no more
than 15 heavy atoms from QM9 and QMugs datasets. Each value is calculated on 50 QMugs molecules
and the bars show 95\% confidence intervals. The setting is in parallel with Fig. 3(a). Beyond M-NNP,
that is, the NNP using the same model architecture as M-OFDFT and already presented in Fig. 3(a), two
more architectures for NNP are investigated: Equivariant Transformer (ET) [38] and Equiformer [39].
ab
Supplementary Figure S18: Empirical time cost of various computational components in the density
optimization process of M-OFDFT, under (a)the residual KEDF TS,res,Î¸formulation and (b)the TXC
functional ETXC,Î¸formulation. Computational components are defined in the following. â€œInit-Denâ€:
initialization of density (including density fitting); â€œInit-Gridâ€ (only for the TS,res,Î¸formulation): gener-
ation of grid points and evaluation of basis function values on them; â€œML-Predâ€: evaluation of the deep
learning model, TS,res,Î¸orETXC,Î¸, including the local frame module (Supplementary Section B.2) and en-
hancement modules (Supplementary Section B.3); â€œCoeff-Derivâ€: automatic differentiation to compute
the gradient of the deep learning model with respect to input density coefficients; â€œAna-Engâ€: computa-
tion of the values and gradients with respect to density coefficients of energy terms that have analytical
expressions, that is, the Hartree energy EH(Eq. (S47)) and the external potential energy Eext(Eq. (S49));
â€œHF-Forceâ€: Hellmann-Feynman force computation (Supplementary Section A.5) conducted after den-
sity optimization; â€œTs-Baseâ€ (only for the TS,res,Î¸formulation): evaluation of the value and gradients with
respect to density coefficients of the base KEDF on the grid; â€œEXCâ€ (only for the TS,res,Î¸formulation):
evaluation of the value and gradients with respect to density coefficients of the XC functional on the grid.
77

5 10 15
Distance cutoff of neighborhood (a0)0.100.150.200.250.300.35Per-atom energy MAE (kcal/mol)
(a)Energy results
5 10 15
Distance cutoff of neighborhood (a0)0.100.150.200.250.300.35HF force MAE (kcal/mol/Ã…)
 (b)HF force results
Supplementary Figure S19: Ablation study results for the non-locality of the functional model ar-
chitecture. The shades show 95\% confidence intervals. (a)and(b)show the energy error and HF force
error of M-OFDFT on QMugs molecules ( n=50) with 56-60 heavy atoms, for which the model is trained
on QM9 and QMugs molecules of no more than 15 heavy atoms. The distance cutoff âˆrepresents a
fully-connected graph (non-local model). The experiment is conducted on the extrapolation setting in
parallel with Supplementary Table S6.
78

\end{document}