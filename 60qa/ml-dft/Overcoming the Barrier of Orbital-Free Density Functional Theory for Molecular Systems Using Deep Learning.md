# 60QA for "Overcoming the Barrier of Orbital-Free Density Functional Theory for Molecular Systems Using Deep Learning"

## 1. Introduction (0/10 completed)

### Question 1. What is the social background of this research?

Density functional theory (DFT) is a powerful quantum chemistry method that has become one of the most popular computational approaches for solving electronic states and determining the energy and properties of molecular systems. Its widespread adoption stems from providing an appropriate balance between accuracy and computational efficiency, which has enabled numerous scientific discoveries across chemistry, materials science, and related fields. The prevailing Kohn-Sham DFT (KSDFT) formulation, while highly successful, requires optimizing N orbital functions for an N-electron system, resulting in computational complexity that scales as O(N³). This higher complexity is increasingly problematic for contemporary research where large-scale system simulations are in high demand for practical applications. The computational burden limits the ability to study larger molecules, proteins, and complex molecular systems that are crucial for understanding real-world phenomena in drug discovery, materials design, and biological processes. Consequently, there is growing interest in orbital-free DFT (OFDFT), which follows the original spirit of DFT by optimizing a single electron density function rather than multiple orbital functions, thereby reducing computational complexity to O(N²). However, OFDFT's accuracy has been limited by the challenge of approximating the kinetic energy density functional, particularly for non-periodic molecular systems where electron density is highly non-uniform. Recent advances in machine learning, especially deep learning, present new opportunities to overcome this accuracy barrier by learning complex functional approximations from data, potentially enabling efficient and accurate quantum chemistry calculations for large-scale molecular systems.

**Technical Terms:**
- **Density Functional Theory (DFT)**: A quantum mechanical method for calculating the electronic structure of many-body systems by using electron density rather than wave functions as the fundamental variable.
- **Kohn-Sham DFT (KSDFT)**: The prevailing formulation of DFT that represents the system using N orbital functions for N electrons, allowing explicit calculation of non-interacting kinetic energy.
- **Orbital-Free DFT (OFDFT)**: An alternative DFT formulation that optimizes only the electron density function, eliminating the need to compute individual orbitals and reducing computational complexity.
- **Electron Density (ρ(r))**: The probability distribution of electrons in space, representing the one-body reduced density.
- **Kinetic Energy Density Functional (KEDF)**: The functional that approximates the non-interacting kinetic energy of electrons as a function of electron density, denoted as T_S[ρ].
- **Computational Complexity Scaling**: The relationship between system size (N electrons) and computational cost; O(N³) for KSDFT versus O(N²) for OFDFT.
- **Non-periodic Molecular Systems**: Molecules that lack the repeating lattice structure found in crystalline materials, making them more challenging for OFDFT approximations.

### Question 2. What is the target problem of this work?

The target problem of this research is to develop an accurate and computationally efficient orbital-free density functional theory (OFDFT) method capable of solving molecular systems, particularly non-periodic molecules that have traditionally been beyond the reach of OFDFT approaches. The core challenge lies in approximating the kinetic energy density functional (KEDF), denoted as T_S[ρ], which is the central task in OFDFT. Classical approximations for KEDF, developed based on uniform electron gas theory, have achieved success for periodic material systems but remain highly inaccurate for molecular systems where electron density is far from uniform. The problem is further complicated by the need to capture non-local interactions of electron density at different spatial points, which is essential for accurate KEDF approximation but computationally prohibitive with traditional grid-based density representations. The research addresses the fundamental tension between accuracy and computational efficiency in quantum chemistry: while Kohn-Sham DFT provides good accuracy, it scales as O(N³) which becomes prohibitive for large molecular systems; OFDFT offers better O(N²) scaling but lacks sufficient accuracy for molecules. This work aims to overcome the accuracy barrier of OFDFT for molecular systems by using deep learning to approximate the KEDF, while maintaining the computational scaling advantage that makes OFDFT attractive for studying large-scale molecular systems including proteins and other complex biomolecular structures.

**Technical Terms:**
- **Kinetic Energy Density Functional (KEDF)**: A functional T_S[ρ] that approximates the non-interacting kinetic energy of electrons as a function of electron density, which is the central approximation challenge in OFDFT.
- **Uniform Electron Gas Theory**: A theoretical framework assuming electrons are distributed uniformly in space, which forms the basis for classical KEDF approximations but fails for molecular systems.
- **Non-local Interactions**: Interactions between electron density at spatially separated points, which require considering density values across the entire molecular system rather than just local neighborhoods.
- **Grid-based Density Representation**: A method of representing electron density by discretizing space into a grid and storing density values at grid points, which requires many points (~10⁴N) for molecular systems.
- **Computational Scaling**: The relationship between system size and computational cost; O(N³) means cost increases with the cube of system size, while O(N²) means quadratic increase.

### Question 3. Explain a typical use case.

A typical use case for M-OFDFT is calculating the ground-state electronic properties of large molecular systems, such as proteins, where traditional Kohn-Sham DFT becomes computationally prohibitive. For example, the research demonstrates M-OFDFT's application to chignolin, a small protein consisting of 10 residues and 168 atoms after neutralization. In this scenario, researchers need to determine the electronic energy, electron density distribution, and forces on atoms for conformational analysis or molecular dynamics simulations. The workflow begins with a molecular structure M defined by atomic coordinates and atomic numbers. Using a pre-trained deep learning KEDF model, M-OFDFT optimizes the electron density represented as expansion coefficients p under an atomic basis set to minimize the total electronic energy. Through iterative gradient descent optimization constrained to maintain normalized density, the method converges to the ground-state density and energy. The optimized density reveals the shell structure around atoms and provides accurate energies comparable to Kohn-Sham DFT but with significantly reduced computational cost. Another typical use case involves studying conformational spaces, such as analyzing ethanol structures from molecular dynamics trajectories. Here, M-OFDFT can rapidly evaluate energies and forces for thousands of molecular conformations to understand relative stability and dynamics. The method is particularly valuable when extrapolating to molecules much larger than those in the training set, such as calculating properties of molecules with 224 atoms when trained only on molecules up to 15 heavy atoms, enabling large-scale molecular simulations previously unaffordable with conventional quantum chemistry methods.

**Technical Terms:**
- **Ground-state Electronic Properties**: The lowest energy electronic configuration and associated properties like density and forces when all electrons occupy their most stable quantum states.
- **Conformational Analysis**: The study of different three-dimensional arrangements (conformations) of a molecule and their relative energies and properties.
- **Molecular Dynamics Simulations**: Computational simulations that model the time-dependent motion of atoms in molecules by calculating forces and updating positions iteratively.
- **Atomic Basis Set**: A set of mathematical functions {ω_μ(r)} centered on atoms used to represent electron density as a linear combination, where each function describes electron distribution patterns around atoms.
- **Expansion Coefficients**: The numerical weights p_μ in the linear combination ρ(r) = Σ_μ p_μ ω_μ(r) that determine how much each basis function contributes to the total electron density.
- **Shell Structure**: The organization of electron density into concentric shells around atomic nuclei, with core electrons close to the nucleus and valence electrons in outer shells.
- **Heavy Atoms**: Non-hydrogen atoms in a molecule, which typically determine molecular size complexity in quantum chemistry calculations.

### Question 4. Why is this task challenging?

This task is fundamentally challenging because approximating the kinetic energy density functional (KEDF) for molecular systems requires capturing the complex, non-uniform distribution of electron density in molecules, which differs dramatically from the uniform electron gas assumptions underlying classical KEDF approximations. The electron density in molecules exhibits sharp variations near atomic nuclei due to the nuclear cusp condition, forms shell structures around atoms, and involves covalent bonding regions where density is shared between atoms—all features that classical KEDFs based on uniform or slowly-varying density assumptions fail to capture accurately. The challenge is amplified by the essential requirement of non-locality in KEDF approximation, meaning the kinetic energy at any point depends on electron density at distant spatial locations, not just the immediate neighborhood. Incorporating non-locality is computationally expensive, especially with grid-based density representations that require approximately 10⁴N grid points for N-electron molecular systems, making non-local calculations prohibitively costly. Furthermore, the task involves unconventional machine learning challenges beyond standard supervised learning. The KEDF model serves as an optimization objective rather than a direct prediction function, requiring it to accurately capture the entire energy landscape over the space of possible density configurations for each molecular structure, not just at a single ground-state point. This necessitates generating multiple density datapoints with gradient labels for each molecular structure during training. Additionally, the model must respect geometric invariance—the output kinetic energy must remain unchanged under molecular rotation while the input density coefficients transform equivariantly. Finally, achieving meaningful extrapolation to molecules much larger than training data is critical but difficult, as the ground-state energy involves intricate many-body interactions among electrons and nuclei, creating a highly complex function that typically fails to generalize well across different molecular scales without appropriate problem formulation.

**Technical Terms:**
- **Nuclear Cusp Condition**: A physical constraint requiring electron density to exhibit a specific sharp increase (cusp) at nuclear positions, following the relationship ∂ρ/∂r|_{r→0} = -2Zρ(0) where Z is the atomic number.
- **Covalent Bonding Regions**: Spatial regions between atoms where electron density is shared and accumulated due to chemical bond formation, characterized by non-uniform density distributions.
- **Energy Landscape**: The multidimensional surface representing how total energy varies as a function of all possible density configurations, which the optimization process must navigate to find the minimum.
- **Gradient Labels**: The partial derivatives ∇_p E with respect to density coefficients p, which indicate the direction and magnitude of energy change and are essential for training the model to support gradient-based optimization.
- **Geometric Invariance**: The physical requirement that predicted energies remain unchanged (invariant) under spatial transformations like rotation, while input density representations transform accordingly (equivariantly).
- **Many-body Interactions**: Complex quantum mechanical interactions among multiple particles (electrons and nuclei) that cannot be decomposed into simple pairwise interactions, making the total energy a highly non-linear function.
- **Equivariant Transformation**: A property where the representation of an object transforms in a specific, predictable way under symmetry operations (like rotation), maintaining consistency with the physical transformation.

### Question 5. Why are conventional studies insufficient?

Conventional studies are insufficient because classical orbital-free DFT approaches, while computationally efficient, fail to achieve chemical accuracy for molecular systems, and previous machine learning attempts have been limited in scope and extrapolation capability. Classical KEDF approximations such as Thomas-Fermi (TF), von Weizsäcker (vW), and various gradient expansion corrections were developed based on uniform electron gas theory and have achieved successes for periodic material systems with relatively uniform electron density. However, these methods produce energy errors orders of magnitude larger than the chemical accuracy threshold (1 kcal/mol) for molecules because molecular electron density is highly non-uniform, featuring sharp nuclear cusps, shell structures, and covalent bonding regions that violate the slowly-varying density assumptions. Early machine learning approaches using kernel ridge regression showed promise only for one-dimensional systems, lacking the scalability needed for three-dimensional molecular applications. More recent deep learning approaches have explored learning point-wise kinetic energy density from electron density features, but these semi-local methods cannot capture the essential non-local interactions required for accurate KEDF approximation. Other works attempting to incorporate non-locality using grid-based density representations face prohibitive computational costs, limiting applications to systems with only dozens of atoms—far too small to demonstrate the scaling advantage that motivates OFDFT development. Neural network potentials (NNPs) that predict ground-state energy end-to-end from molecular structure offer fast predictions but do not describe electronic states and suffer from poor extrapolation to larger molecules than those in training data. Critically, few previous studies have demonstrated accurate extrapolation well beyond training molecule sizes, which is essential for exploiting OFDFT's computational scaling benefits. Without good extrapolation, the method cannot be applied to the large-scale molecular systems for which OFDFT's efficiency advantage would be most valuable, undermining the entire motivation for using orbital-free approaches over the more accurate Kohn-Sham DFT.

**Technical Terms:**
- **Chemical Accuracy**: A threshold of approximately 1 kcal/mol (or 1.6 milli-Hartrees) for energy prediction errors, considered sufficient for reliable prediction of chemical properties and reaction energetics.
- **Gradient Expansion**: A mathematical approximation technique that expresses the KEDF as a series of terms involving the electron density and its spatial derivatives (gradients, Laplacians, etc.).
- **Thomas-Fermi (TF) KEDF**: The simplest classical kinetic energy approximation proportional to ρ^(5/3), exact for uniform electron gas but highly inaccurate for molecular systems.
- **von Weizsäcker (vW) KEDF**: A gradient correction to TF that captures some kinetic energy contributions from density variations, proportional to |∇ρ|²/ρ.
- **Kernel Ridge Regression**: A non-parametric machine learning method that learns functions by finding optimal weights for training points in a high-dimensional feature space defined by a kernel function.
- **Point-wise Kinetic Energy Density**: The kinetic energy per unit volume at each spatial location, whose integral over space gives the total kinetic energy.
- **Neural Network Potentials (NNPs)**: Machine learning models that directly predict ground-state energy from atomic positions and types without explicitly modeling electron density or electronic structure.

### Question 6. What is proposed and solved in this study?

This study proposes M-OFDFT (Molecular Orbital-Free Density Functional Theory), a novel OFDFT approach that uses a deep learning model to approximate the kinetic energy density functional, enabling accurate quantum chemistry calculations for molecular systems while maintaining computational efficiency. The key innovation is representing electron density using expansion coefficients under an atomic basis set rather than grid-based representations, which reduces dimensionality from approximately 10⁴N to only about 10N parameters, making non-local KEDF calculations computationally affordable. The deep learning model, based on the Graphormer architecture (a Transformer variant), takes as input the density coefficients distributed over atoms along with molecular structure information, and uses attention mechanisms to capture essential non-local interactions of electron density across distant spatial regions. To address unconventional machine learning challenges, the research introduces three key technical solutions: (1) methods to generate multiple density datapoints with gradient labels for each molecular structure to train the model for use as an optimization objective; (2) local frame techniques to guarantee geometric invariance where output energy remains unchanged under molecular rotation while input coefficients transform equivariantly; (3) enhancement modules that balance sensitivity, rescale gradients dimension-wise, and provide reference offsets to enable the model to express the steep gradients required by physical mechanisms. M-OFDFT solves the problem of accurate molecular electronic structure prediction by achieving chemical accuracy (errors below 1 kcal/mol for energy and approximately 3 kcal/mol/Å for forces) on diverse molecular systems including ethanol conformations and QM9 molecules—hundreds of times more accurate than classical OFDFT. More significantly, it demonstrates remarkable extrapolation capability with per-atom errors remaining constant or decreasing on molecules up to 224 atoms (10 times larger than training), successfully handling protein systems like chignolin (168 atoms) with substantially better accuracy than classical OFDFT, and exhibiting empirical time complexity of O(N^1.46), substantially lower than KSDFT's O(N^2.49), with a 27.4-fold speedup on large molecules, thereby advancing the accuracy-efficiency frontier in quantum chemistry.

**Technical Terms:**
- **Graphormer**: A graph neural network architecture based on the Transformer model, designed to process graph-structured data by using attention mechanisms to capture interactions between nodes.
- **Attention Mechanism**: A neural network component that computes weighted combinations of features from different locations, where weights are dynamically calculated based on feature content and relationships, enabling long-range interactions.
- **Geometric Invariance**: The physical principle that predicted energies and properties must remain unchanged under rigid transformations (rotation, translation) of the molecular coordinate system.
- **Local Frames**: Coordinate systems attached to local molecular features (like atoms or bonds) that transform with the molecule, used to construct rotationally invariant or equivariant representations.
- **Enhancement Modules**: Specialized neural network components designed to improve gradient expression, including sensitivity balancing, dimension-wise rescaling, and reference offset mechanisms.
- **Per-atom Error**: The prediction error divided by the number of atoms, providing a size-normalized metric for comparing accuracy across molecules of different sizes.
- **Hellmann-Feynman Force**: The force on each atom calculated as the negative gradient of total energy with respect to atomic positions, used in molecular dynamics and geometry optimization.

### Question 7. What is the difference between the proposed and conventional methods?

The proposed M-OFDFT differs from conventional classical OFDFT methods primarily in using a data-driven deep learning model to approximate the kinetic energy density functional rather than theory-based analytical formulas derived from uniform electron gas assumptions. While classical KEDFs like Thomas-Fermi, von Weizsäcker, and gradient expansion approaches are constrained by their theoretical foundations to handle only slowly-varying densities, M-OFDFT learns complex functional relationships from labeled data generated by accurate Kohn-Sham DFT calculations, enabling it to compensate for theoretical mismatches and capture the highly non-uniform density distributions in molecules. M-OFDFT differs from previous machine learning KEDF approaches through its atomic basis representation of electron density: instead of using grid-based representations requiring approximately 10⁴N points, M-OFDFT represents density with only about 10N expansion coefficients under atomic basis functions, reducing dimensionality by three orders of magnitude and making non-local calculations computationally feasible. This contrasts with prior works that either used semi-local approaches ignoring essential non-local effects or faced prohibitive computational costs with grid-based non-local methods, limiting them to systems with dozens of atoms. Compared to neural network potentials (NNPs) that predict ground-state energy end-to-end from molecular structure, M-OFDFT formulates the learning task as approximating the objective function for density optimization rather than directly predicting the final energy. This fundamental difference means M-OFDFT must learn the mechanism of electron interactions (lower complexity) while transferring much complexity to the optimization process, whereas NNPs must learn the complete many-body result (higher complexity), leading to M-OFDFT's superior extrapolation capability. M-OFDFT also differs in incorporating specialized techniques absent from conventional approaches: generating multiple density training datapoints with gradients per structure, employing local frames for geometric invariance, and using enhancement modules for expressing steep physical gradients. Finally, while conventional methods either prioritize accuracy (Kohn-Sham DFT with O(N³) scaling) or efficiency (classical OFDFT with poor accuracy), M-OFDFT achieves both chemical accuracy comparable to Kohn-Sham DFT and favorable O(N^1.46) empirical scaling.

**Technical Terms:**
- **Theory-based Analytical Formulas**: Mathematical expressions for KEDF derived from theoretical principles (like uniform electron gas) rather than learned from data, typically involving density and its derivatives.
- **Data-driven Learning**: An approach where the functional form is learned from training examples (input-output pairs) rather than derived from theoretical principles, enabling adaptation to complex patterns.
- **Density Fitting**: A technique in quantum chemistry that approximates four-center electron repulsion integrals using auxiliary basis functions to reduce computational cost from O(N⁴) to O(N³).
- **Semi-local Approaches**: Methods where the predicted quantity at a point depends only on density values in a small neighborhood, ignoring long-range interactions.
- **End-to-end Prediction**: Machine learning paradigm where the model directly maps input (molecular structure) to final output (energy) without intermediate optimization steps.
- **Objective Function**: The function to be minimized during optimization, which in M-OFDFT is the total electronic energy as a functional of density coefficients.
- **Training Datapoints**: Individual examples used to train the machine learning model, consisting of input features (density coefficients, molecular structure) and target labels (energies, gradients).

### Question 8. Explain why the difference should be introduced.

The key differences in M-OFDFT should be introduced because they address fundamental limitations that have prevented OFDFT from achieving practical accuracy and applicability to molecular systems. The atomic basis representation is essential because grid-based representations create an intractable accuracy-efficiency trade-off: molecular electron density features sharp nuclear cusps and covalent bonding regions requiring fine grids with approximately 10⁴N points for adequate resolution, but non-local KEDF calculations with such high-dimensional inputs become computationally prohibitive, eliminating the efficiency advantage that motivates OFDFT. Atomic basis functions, designed to mimic nuclear cusp conditions and naturally forming shell structures, efficiently capture molecular density patterns with only about 10N coefficients, making non-local calculations affordable while providing a representation aligned with the physical reality of how electron density organizes around atoms. The data-driven deep learning approach succeeds where classical theory fails because molecular electron density is fundamentally far from uniform, violating the assumptions underlying analytical KEDFs derived from uniform electron gas theory. By learning from accurate Kohn-Sham DFT calculations, the model compensates for theoretical mismatches and captures complex density-energy relationships that cannot be expressed in closed analytical form. Learning the objective function rather than end-to-end energy prediction is crucial for extrapolation capability: the ground-state energy emerges from intricate many-body interactions creating a highly complex function difficult to extrapolate across molecular sizes, but the underlying interaction mechanism captured by the objective function has lower complexity and better generalization properties, with optimization tools handling the remaining complexity without extrapolation issues. This formulation enables M-OFDFT to maintain constant or decreasing per-atom errors on molecules 10 times larger than training data, whereas end-to-end methods show increasing errors. The specialized techniques for generating multiple density datapoints with gradients, ensuring geometric invariance, and enhancing gradient expression are necessary because standard supervised learning approaches cannot handle the unique requirements of learning an optimization objective function that must accurately represent energy landscapes, respect physical symmetries, and exhibit steep gradients where mechanisms vary rapidly—challenges absent in conventional machine learning applications.

**Technical Terms:**
- **Accuracy-Efficiency Trade-off**: The fundamental tension in computational methods where improving accuracy typically increases computational cost, requiring careful balance for practical applicability.
- **Nuclear Cusp Condition**: A mathematical constraint describing the sharp behavior of electron density near nuclei as ∂ρ/∂r|_{r→0} ∝ -Zρ(0), which basis functions are designed to satisfy.
- **Closed Analytical Form**: An explicit mathematical formula that can be written down and evaluated directly, as opposed to implicit definitions or iterative procedures.
- **Generalization Properties**: The ability of a learned model to perform accurately on new data different from training examples, particularly important for extrapolation to different scales.
- **Energy Landscape**: The hypersurface in density coefficient space representing how electronic energy varies with all possible density configurations, which the optimizer must navigate.
- **Physical Symmetries**: Fundamental invariances in physical laws, such as energy being unchanged by rotating or translating the molecular coordinate system, which predictions must respect.
- **Optimization Objective Function**: The function being minimized (or maximized) during an optimization process, which defines the problem being solved.

### Question 9. What is the novelty of the proposed method?

The novelty of the proposed M-OFDFT method encompasses several key innovations:

- **Novel density representation for OFDFT**: First use of atomic basis expansion coefficients as input for a machine learning KEDF model, reducing dimensionality from ~10⁴N (grid-based) to ~10N while naturally aligning with molecular electron density patterns around atoms and enabling affordable non-local calculations.

- **Deep learning architecture for KEDF**: Application of Graphormer (Transformer-based) architecture with attention mechanisms to capture essential non-local electron density interactions in molecular systems, representing the first successful deep learning KEDF model achieving chemical accuracy on diverse molecular systems.

- **Learning formulation as objective function**: Novel formulation of the machine learning task as approximating the optimization objective for density rather than end-to-end energy prediction, which significantly improves extrapolation capability by reducing complexity and transferring computational burden to optimization.

- **Specialized training data generation**: Introduction of methods to generate multiple density configurations with gradient labels for each molecular structure, addressing the unique challenge that a single ground-state datapoint per structure is insufficient for learning an optimization objective.

- **Geometric invariance techniques**: Development of local frame approaches to guarantee that the model respects physical symmetries—producing invariant energy outputs from equivariant density coefficient inputs under molecular rotation—which is non-trivial with coefficient representations.

- **Gradient enhancement modules**: Novel neural network components including sensitivity balancing, dimension-wise gradient rescaling, and reference offset mechanisms specifically designed to enable the model to express the steep gradients required by rapidly-varying physical mechanisms.

- **Demonstration of practical OFDFT for molecules**: First demonstration of an OFDFT method achieving chemical accuracy on common molecular systems including conformational spaces (ethanol) and diverse chemical spaces (QM9), recovering accurate shell structures previously deemed challenging for orbital-free approaches.

- **Remarkable extrapolation capability**: Achievement of constant or decreasing per-atom errors on molecules up to 10 times larger (224 atoms) than training data, successfully handling protein systems (chignolin, 168 atoms) with high accuracy, representing unprecedented extrapolation for quantum chemistry machine learning.

- **Computational efficiency demonstration**: Empirical validation of O(N^1.46) time complexity substantially lower than Kohn-Sham DFT's O(N^2.49), with 27.4-fold speedup on large molecules, concretely demonstrating the accuracy-efficiency frontier advancement for practical large-scale molecular applications.

**Technical Terms:**
- **Chemical Accuracy**: Energy prediction errors below ~1 kcal/mol, sufficient for reliable chemical property prediction and reaction energetics.
- **Conformational Space**: The set of different three-dimensional molecular geometries (conformations) accessible through rotation around chemical bonds without breaking bonds.
- **Chemical Space**: The space of different molecular compositions and bonding patterns, representing chemically distinct molecules rather than conformations of the same molecule.
- **Shell Structure**: The organization of electron density into concentric regions around atomic nuclei corresponding to quantum mechanical shells (K, L, M, etc.).
- **Time Complexity**: Mathematical characterization of how computational time scales with system size N, expressed in big-O notation like O(N^α).
- **Frontier Advancement**: Progress in Pareto-optimal trade-offs where both competing objectives (accuracy and efficiency) are simultaneously improved beyond previous best-known solutions.

### Question 10. Show the eye-catch figure.

The eye-catch figure for M-OFDFT is Figure 1 in the paper, which provides a comprehensive three-panel overview of the method. This figure cannot be directly reproduced here as it is a complex multi-part illustration from the published paper. However, I can describe its content and significance:

**Figure 1 Content Description:**

*Panel (a)* compares KSDFT and OFDFT formulations: Shows KSDFT optimizing N orbital functions {φᵢ(r)} with O(N³) complexity versus OFDFT optimizing one density function ρ(r) with O(N²) complexity, visually illustrating the computational scaling advantage when a good KEDF T_S[ρ] is available.

*Panel (b)* illustrates the M-OFDFT architecture: Depicts how electron density is represented using atomic basis expansion coefficients p distributed over atoms (shown with a molecular structure of ethanol with spherical basis functions), how these coefficients along with molecular structure M are processed through the Graphormer deep learning model with attention mechanisms (shown with connecting lines between atoms), and how the model outputs the kinetic energy value. The attention mechanism's non-local nature is visualized with lines connecting distant atoms.

*Panel (c)* shows the density optimization workflow: Illustrates how M-OFDFT solves a molecular system by iteratively optimizing density coefficients p^(k) → p^(k+1) using gradients ∇_p E_θ to minimize the total electronic energy E_θ(p,M), with a visual representation of the energy landscape (shown in red-blue hues) that the optimization navigates to find the ground-state density ρ* and energy E*, from which properties like forces f can be derived.

The figure effectively communicates the key innovation: representing density with atomic basis coefficients enables a deep learning model with non-local interactions to approximate KEDF, which then serves as the objective for optimization to solve molecular electronic structure with both accuracy and efficiency. This visual summary captures the entire M-OFDFT workflow from input representation through model architecture to the optimization process for solving molecular systems.

**Technical Terms:**
- **Eye-catch Figure**: The main overview figure in a scientific paper that visually summarizes the key concept, method, or workflow to quickly communicate the core contribution.
- **Multi-panel Figure**: A figure composed of multiple sub-figures (panels) labeled (a), (b), (c), etc., each showing different aspects or steps of the overall concept.
- **Visual Representation**: Graphical depiction using shapes, colors, arrows, and other visual elements to communicate complex concepts more intuitively than text alone.
- **Workflow Diagram**: A schematic illustration showing the sequence of steps or processes in a method, often with arrows indicating the flow of data or operations.
- **Energy Landscape Visualization**: Graphical representation of how energy varies across a parameter space, often using color gradients (like red-blue hues) to indicate high and low energy regions.

---

## 2. Related Work (0/5 completed)

### Question 11. Explain about multiple survey papers in the related area.

Several comprehensive survey papers provide important context for understanding orbital-free density functional theory and machine learning approaches to quantum chemistry. Wang and Carter's review "Orbital-free kinetic-energy density functional theory" in Theoretical Methods in Condensed Phase Chemistry (2000) provides foundational coverage of OFDFT methods, discussing classical kinetic energy density functional approximations and their theoretical basis. Karasiev, Chakraborty, and Trickey's chapter "Progress on new approaches to old ideas: Orbital-free density functionals" in Many-electron Approaches in Physics, Chemistry and Mathematics (2014) surveys developments in OFDFT, highlighting ongoing challenges in achieving accuracy for molecular systems and discussing various gradient expansion and nonlocal approaches. More recently, Huang, von Rudorff, and von Lilienfeld's "The central role of density functional theory in the AI age" in Science (2023) surveys the intersection of DFT and artificial intelligence, discussing how machine learning is transforming computational quantum chemistry including OFDFT applications. Teale et al.'s comprehensive review "DFT exchange: sharing perspectives on the workhorse of quantum chemistry and materials science" in Physical Chemistry Chemical Physics (2022) provides extensive coverage of DFT methodology including discussions of orbital-free approaches and their limitations for molecular systems. These surveys collectively establish that while classical OFDFT has succeeded for periodic materials with relatively uniform electron density, molecular systems remain challenging due to highly non-uniform density distributions, and that machine learning presents promising opportunities to overcome these longstanding accuracy barriers by learning complex functional relationships from high-quality data generated by more accurate methods.

**Technical Terms:**
- **Survey Paper/Review Article**: A comprehensive academic article that synthesizes and critically evaluates existing research literature in a particular field, identifying trends, gaps, and future directions.
- **Gradient Expansion**: A mathematical technique for approximating functionals as series involving density and its spatial derivatives (∇ρ, ∇²ρ, etc.) to systematically improve upon local approximations.
- **Nonlocal Approaches**: Methods where the predicted quantity at one spatial point depends on values at distant points, requiring integration or convolution operations over extended regions of space.
- **Uniform Electron Density**: An idealized scenario where electron density is constant throughout space, serving as a theoretical limiting case for which exact solutions exist.
- **Exchange-Correlation Functional**: The component of DFT energy functional E_XC[ρ] accounting for quantum mechanical exchange and classical/quantum correlation effects beyond mean-field approximations.
- **Periodic Materials**: Crystalline solids with repeating lattice structures, allowing efficient computational treatment using Bloch's theorem and plane-wave basis sets.

### Question 12. Explain the first related subfield and several related papers.

The first major related subfield is classical kinetic energy density functional development, which attempts to approximate the non-interacting kinetic energy T_S[ρ] using analytical expressions based on theoretical principles. The Thomas-Fermi (TF) functional (Thomas 1927, Fermi 1928) represents the foundational work, providing the exact KEDF for uniform electron gas as T_TF[ρ] = (3/10)(3π²)^(2/3) ∫ρ^(5/3) dr, which scales with the local density to the 5/3 power. However, this severely underestimates kinetic energy in molecules where density is highly non-uniform. Von Weizsäcker's 1935 work introduced the first gradient correction T_vW[ρ] = (1/8)∫|∇ρ|²/ρ dr, which captures kinetic energy contributions from density variations and is exact for one-electron systems but overcorrects for many-electron systems. Brack, Jennings, and Chu (1976) proposed the TF+1/9 vW combination as a systematic expansion from uniform electron gas, while Karasiev and Trickey (2012) explored TF+vW variants. More sophisticated approaches include Wang and Teter's density-dependent kernel functionals (1992), Wang, Govind, and Carter's nonlocal orbital-free functionals (1999), and Huang and Carter's semiconductor-specific KEDFs (2010). Despite these advances, García-Aldea and Alvarellos (2007) and Xia et al. (2012) demonstrated that classical KEDFs still produce errors orders of magnitude larger than chemical accuracy for molecular systems. Constantin et al.'s asymptotic PBE-like (APBE) functional (2011) represents a recent classical approach using semiclassical neutral atom as reference, which serves as a reasonable base KEDF but still lacks the accuracy needed for predictive molecular calculations without machine learning corrections.

**Technical Terms:**
- **Uniform Electron Gas (UEG)**: An idealized system where electrons move in a constant positive background charge, serving as the theoretical foundation for local density approximations.
- **Gradient Correction**: Functional terms involving spatial derivatives of density (like ∇ρ) that improve upon local approximations by capturing effects of density inhomogeneity.
- **Density-Dependent Kernel**: A functional form where the response at one point to density at another point varies depending on the local density values, enabling more flexible nonlocal approximations.
- **Semiclassical Approach**: Methods combining classical physics concepts with quantum mechanical corrections, often using the WKB approximation or Thomas-Fermi theory as starting points.
- **Direct Inversion in Iterative Subspace (DIIS)**: An acceleration technique for iterative quantum chemistry calculations that extrapolates solutions using information from previous iterations to achieve faster convergence.

### Question 13. Explain the N-th related subfield and several related papers.

The second major related subfield is machine learning approaches to density functional approximation, which use data-driven methods to learn complex functional relationships that analytical expressions cannot capture. Snyder et al. (2012) pioneered this direction with "Finding density functionals with machine learning," using kernel ridge regression to approximate KEDF from labeled data on one-dimensional systems, demonstrating proof-of-concept for learning functionals. Brockherde et al. (2017) extended this in "Bypassing the Kohn-Sham equations with machine learning," showing that machine learning could learn both kinetic and exchange-correlation functionals, though requiring projection onto training manifolds during optimization. Early deep learning approaches include Yao and Parkhill (2016) applying convolutional neural networks to learn kinetic energy of hydrocarbons, and Seino et al. (2018, 2019) developing semi-local machine-learned KEDFs using gradient features of electron density. These semi-local approaches, while computationally efficient, cannot capture essential nonlocal effects. Imoto, Imada, and Oshiyama (2021) achieved order-N orbital-free calculations by machine learning functional derivatives for semiconductors and metals, using self-consistent field schemes. Meyer, Weichselbaum, and Hauser (2020) trained simultaneously on kinetic energy and its functional derivative for improved optimization stability. Remme et al.'s KineticNet (2023) explored deep learning for transferable KEDFs using perturbed external potential data. Beyond KEDF specifically, Dick and Fernandez-Serra (2020) and Chen et al.'s DeePKS (2021) learned exchange-correlation functionals using coefficient representations similar to M-OFDFT's approach. However, previous works either used grid-based representations limiting scalability, focused on semi-local approximations missing nonlocality, or demonstrated limited extrapolation capability to larger molecules—gaps that M-OFDFT addresses through its atomic basis representation and objective function learning formulation.

**Technical Terms:**
- **Kernel Ridge Regression**: A nonparametric machine learning method combining ridge regression (L2 regularization) with kernel tricks to learn functions in high-dimensional feature spaces.
- **Training Manifold**: The subspace or distribution in input space spanned by training examples, outside which the model may produce unreliable predictions (out-of-distribution).
- **Convolutional Neural Networks (CNNs)**: Deep learning architectures using convolutional layers to process grid-like data (images, 3D density grids) by learning local feature patterns with shared weights.
- **Functional Derivative**: The variation δF/δρ(r) of a functional F[ρ] with respect to infinitesimal changes in the density at each point, generalizing the gradient concept to function spaces.
- **Self-Consistent Field (SCF)**: An iterative procedure where the solution depends on itself, requiring repeated updates until convergence (e.g., electrons moving in a mean field they collectively create).
- **Transferability**: The ability of a learned model or functional to generalize accurately across different molecular systems, chemical compositions, or system sizes beyond training examples.

### Question 14. Explain standard datasets in the related fields.

Several standard datasets are widely used for benchmarking machine learning methods in quantum chemistry and molecular property prediction. The QM9 dataset (Ramakrishnan et al. 2014) is one of the most popular benchmarks, containing approximately 134,000 small organic molecules with up to 9 heavy atoms (H, C, N, O, F), with equilibrium geometries and quantum chemical properties calculated at the B3LYP/6-31G(2df,p) level. QM9 molecules are drawn from the GDB-17 chemical universe database (Ruddigkeit et al. 2012) which enumerates 166 billion possible organic molecules, making QM9 representative of diverse chemical space for small molecules. The MD17 dataset (Chmiela et al. 2017, 2019) provides molecular dynamics trajectories for small molecules including ethanol, with configurations sampled from room-temperature MD simulations, offering non-equilibrium conformations for testing generalization across conformational space. The QMugs dataset (Isert et al. 2022) extends to larger molecules with approximately 665,000 biologically and pharmacologically relevant molecules from the ChEMBL database, including structures with up to 100 heavy atoms and multiple conformers per molecule, enabling extrapolation studies to larger molecular systems. For protein systems, the Shaw research group's fast-folding protein trajectories (Lindorff-Larsen et al. 2011) provide extensive MD simulation data for small proteins like chignolin (10 residues), enabling investigation of quantum chemistry methods on biomolecular systems. The M-OFDFT study utilizes all these datasets: QM9 for chemical space generalization, MD17 ethanols for conformational generalization, QMugs for size extrapolation up to 101 heavy atoms, and chignolin fragments for protein system extrapolation. Properties calculated include ground-state energies, forces, electron densities, and dipole moments at the PBE/6-31G(2df,p) KSDFT level to generate training labels.

**Technical Terms:**
- **Equilibrium Geometry**: The molecular configuration corresponding to a local minimum on the potential energy surface, where forces on all atoms are zero (or below threshold).
- **B3LYP**: A popular hybrid exchange-correlation functional combining Becke's 3-parameter exchange with Lee-Yang-Parr correlation, widely used for molecular property calculations.
- **6-31G(2df,p)**: A split-valence basis set specification where core orbitals use 6 Gaussian functions, valence orbitals are split into 3 and 1 functions, with polarization functions (2d and 1f on heavy atoms, 1p on hydrogen).
- **Chemical Universe**: The comprehensive set of all possible molecules satisfying specified constraints (atom types, bonding rules, size limits), representing the full chemical space for exploration.
- **Conformational Space**: The manifold of possible three-dimensional molecular geometries accessible through rotations around bonds and other geometric variations without breaking chemical bonds.
- **Molecular Dynamics (MD) Trajectory**: A time series of molecular configurations generated by integrating equations of motion, sampling conformations according to statistical mechanics at specified temperature and pressure.

### Question 15. What is the difference(s) between the proposed and related methods?

M-OFDFT differs from classical OFDFT methods (Thomas-Fermi, von Weizsäcker, APBE, etc.) fundamentally in its data-driven learning approach rather than relying on analytical expressions derived from uniform electron gas theory. While classical KEDFs are constrained by their theoretical foundations and systematically fail for non-uniform molecular densities with errors exceeding 10-100 kcal/mol, M-OFDFT learns from accurate KSDFT calculations and achieves chemical accuracy (~1 kcal/mol). Compared to previous machine learning KEDF methods like Snyder et al.'s kernel ridge regression or Yao and Parkhill's CNN approaches, M-OFDFT uniquely combines: (1) atomic basis coefficient representation instead of grid-based density, reducing dimensionality by three orders of magnitude from ~10⁴N to ~10N; (2) nonlocal Graphormer architecture enabling long-range density interactions impossible for semi-local methods like Seino et al.'s gradient-based approaches; (3) learning formulation as optimization objective rather than end-to-end prediction, improving extrapolation beyond Meyer et al. and Imoto et al.'s approaches. Unlike Brockherde et al. who require manifold projection at each optimization step, M-OFDFT needs only on-manifold initialization, demonstrating better optimization robustness. Compared to neural network potentials like SchNet, ANI, or Equiformer that predict energy end-to-end without modeling electronic structure, M-OFDFT explicitly optimizes electron density to describe electronic states while exhibiting superior extrapolation—per-atom errors decrease for M-OFDFT on larger molecules versus increasing errors for NNPs. M-OFDFT differs from exchange-correlation learning methods like Dick and Fernandez-Serra's or Chen et al.'s DeePKS by focusing specifically on kinetic energy functional with specialized techniques for steep gradient expression and multiple-datapoint training. The key novelty is achieving simultaneously: chemical accuracy comparable to KSDFT, O(N^1.46) empirical scaling substantially better than KSDFT's O(N^2.49), remarkable extrapolation to 10× larger molecules, and successful application to protein-scale systems (168 atoms)—a combination no previous method has demonstrated.

**Technical Terms:**
- **Analytical Expression**: A closed-form mathematical formula that can be explicitly written and directly evaluated, as opposed to iterative or numerical approximations.
- **Learning Formulation**: The specific way a machine learning problem is framed, including what is predicted (target), what information is provided (input), and how success is measured (loss function).
- **Manifold Projection**: Mathematical operation that maps a point in high-dimensional space onto a lower-dimensional subspace or curved surface (manifold) satisfying certain constraints.
- **Electronic Structure**: The complete description of electron distribution and quantum states in a molecular system, including density, orbitals, energy levels, and wavefunctions.
- **End-to-end Prediction**: Machine learning paradigm where the model directly maps raw inputs to final outputs without intermediate steps or explicit modeling of underlying mechanisms.
- **Empirical Scaling**: The relationship between computational cost and system size observed in practice through timing measurements, which may differ from theoretical worst-case complexity.

---

## 3. Problem Statement (0/8 completed)

### Question 16. What is the target problem?

The target problem addressed in this research is to develop an accurate and computationally efficient method for calculating electronic structure and properties of molecular systems using orbital-free density functional theory, which requires approximating the kinetic energy density functional (KEDF) T_S[ρ] as a function of electron density alone. The fundamental challenge is that for molecular systems with highly non-uniform electron density—featuring sharp nuclear cusps, shell structures, and covalent bonding regions—classical KEDF approximations based on uniform electron gas theory fail to achieve chemical accuracy (1 kcal/mol), producing errors that are orders of magnitude larger. The problem is further complicated by the essential requirement of capturing nonlocal interactions where kinetic energy at any point depends on electron density at distant spatial locations, which is computationally prohibitive with traditional grid-based density representations requiring approximately 10⁴N points for N-electron systems. Additionally, the learned KEDF model must serve as an optimization objective function for density, not just a direct prediction function, requiring it to accurately capture energy landscapes across the space of possible density configurations for each molecular structure. The model must also respect physical symmetries—remaining invariant under molecular rotation while handling equivariant input representations—and must extrapolate effectively to molecules substantially larger than training examples to exploit OFDFT's computational scaling advantage. Finally, the method must demonstrate practical utility by solving ground-state properties for large-scale molecular systems including proteins with hundreds of atoms, achieving both accuracy comparable to Kohn-Sham DFT and empirical time complexity substantially lower than KSDFT's O(N³) scaling, thereby advancing the accuracy-efficiency trade-off frontier for quantum chemistry calculations on complex molecular systems.

**Technical Terms:**
- **Electronic Structure Calculation**: Computational determination of electron distribution, quantum states, energies, and properties of molecular or material systems from first principles.
- **Chemical Accuracy**: Error threshold of approximately 1 kcal/mol (1.6 milli-Hartrees or 0.043 eV) considered sufficient for reliable prediction of chemical phenomena and reaction energetics.
- **Nuclear Cusp**: The sharp singularity in electron density and wavefunction slope at nuclear positions required by quantum mechanics to balance kinetic and potential energies.
- **Covalent Bonding Region**: Spatial region between atoms where electron density accumulates due to bond formation, characterized by sharing of electrons between atomic centers.
- **Energy Landscape**: The multidimensional hypersurface representing total energy as a function of all degrees of freedom (density coefficients), which optimization must navigate to find ground state.
- **Ground-state Properties**: Characteristics of the lowest-energy quantum state including electron density, total energy, forces on atoms, and derived properties like dipole moments.

### Question 17. What is the expected behavior of the system?

The expected behavior of an ideal orbital-free DFT system is to accurately predict ground-state electronic properties of molecular systems by optimizing electron density to minimize total electronic energy, achieving accuracy comparable to Kohn-Sham DFT while maintaining computational complexity scaling as O(N²) or better rather than KSDFT's O(N³). Specifically, the system should correctly predict molecular energies with mean absolute errors below the chemical accuracy threshold of 1 kcal/mol, and forces on atoms (Hellmann-Feynman forces) with errors below approximately 3 kcal/mol/Å, enabling reliable geometry optimization and molecular dynamics simulations. The optimized electron density should exhibit physically correct features including well-defined shell structures around atomic nuclei with appropriate core and valence electron populations, accurate representation of covalent bonding density between atoms, correct nuclear cusp behavior with density exhibiting proper singularities at nuclear positions, and appropriate long-range decay characteristics in outer molecular regions. The density optimization process should converge reliably from reasonable initializations to the ground state without requiring manifold projection at each step, demonstrating numerical stability comparable to conventional KSDFT calculations. Critically, the method should extrapolate effectively to molecular systems substantially larger than training examples—maintaining constant or decreasing per-atom errors on molecules 5-10 times larger—enabling application to protein-scale systems with hundreds of atoms where the computational efficiency advantage becomes meaningful. The system should respect fundamental physical symmetries with predictions invariant under translation and rotation of the molecular coordinate system. Finally, it should provide accurate potential energy surfaces showing correct relative energies, energy barriers, and equilibrium geometries across conformational changes, supporting applications in chemical reaction modeling and molecular conformational analysis.

**Technical Terms:**
- **Hellmann-Feynman Forces**: Forces on atomic nuclei calculated as F_a = -∇_{x_a} E, the negative gradient of total energy with respect to atomic position, following the Hellmann-Feynman theorem.
- **Geometry Optimization**: Computational procedure to find molecular configurations corresponding to local energy minima by iteratively adjusting atomic positions following forces until convergence.
- **Shell Structure**: The organization of electron density into concentric regions around nuclei corresponding to quantum mechanical shells (K, L, M, etc.) and subshells (s, p, d, f).
- **Core Electrons**: Inner-shell electrons tightly bound to nuclei, less involved in chemical bonding, typically well-localized with high density near nuclear positions.
- **Valence Electrons**: Outer-shell electrons involved in chemical bonding and determining chemical properties, with more delocalized distributions extending between atoms.
- **Long-range Decay**: The asymptotic behavior of electron density and wavefunctions at large distances from the molecule, typically decaying exponentially as exp(-√(2I)r) where I is ionization energy.
- **Potential Energy Surface (PES)**: The function E(R) giving total energy as a function of all nuclear positions R, defining the landscape for molecular geometry and dynamics.

### Question 18. Explain a typical sample with a figure.

A typical sample in this research consists of a molecular structure defined by atomic coordinates and atomic numbers, along with its corresponding electron density represented as expansion coefficients under an atomic basis set, ground-state electronic energy, and gradient of energy with respect to density coefficients. For example, an ethanol molecule (C₂H₆O) with 9 atoms and 26 electrons serves as a representative small organic molecule. The molecular structure M = {(x^(a), Z^(a))}_{a=1}^9 specifies the 3D coordinates x^(a) and atomic number Z^(a) (C=6, O=8, H=1) for each atom. The electron density ρ(r) is expanded using atomic basis functions as ρ(r) = Σ_{μ} p_μ ω_μ(r), where the basis functions ω_μ(r) are centered on atoms and follow various angular patterns (s, p, d orbitals), requiring typically around 260 basis functions (approximately 10N for N=26 electrons). While the original paper contains Figure 1(b) showing this representation visually, the key features include: spherical basis functions of different sizes centered on each atom (illustrated with colored spheres around atoms), the density coefficients p distributed correspondingly over atoms forming the input features, the molecular graph structure with atoms as nodes, and attention connections between distant atoms showing nonlocal interactions. The training data for each molecular structure includes multiple density configurations (p^(d,k)) from intermediate SCF iterations rather than just the ground-state density, each labeled with kinetic energy T_S^(d,k) and gradient ∇_p T_S^(d,k), providing information about the energy landscape. For ethanol conformations, relative energies with respect to equilibrium configuration and Hellmann-Feynman forces on atoms serve as validation targets, testing whether M-OFDFT captures conformational energy differences and provides accurate forces for molecular dynamics.

**Technical Terms:**
- **Atomic Coordinates**: The (x,y,z) Cartesian positions of each atom in a molecule, typically measured in Angstroms (Å) or Bohr radii (a₀).
- **Atomic Number**: The integer Z representing the number of protons in an atomic nucleus, determining the element identity and nuclear charge.
- **Expansion Coefficients**: The numerical weights p_μ in a linear combination ρ(r) = Σ_μ p_μ ω_μ(r) representing how much each basis function contributes to the total density.
- **Angular Patterns**: The directional characteristics of atomic orbitals (s=spherical, p=dumbbell-shaped, d=four-lobed, etc.) determining how basis functions are oriented in space.
- **SCF Iterations**: Sequential steps in self-consistent field calculations where electron density and effective potential are iteratively updated until convergence to self-consistency.
- **Molecular Graph**: A representation of molecular structure as a graph where atoms are nodes and chemical bonds or spatial proximity define edges for processing by graph neural networks.
- **Relative Energy**: Energy difference between a molecular conformation and a reference configuration (e.g., equilibrium), removing systematic energy offsets to focus on conformational preferences.

### Question 19. What are the inputs of the task?

The inputs for the M-OFDFT task consist of two primary components that together define the computational problem for a given molecular system. First, the molecular structure M = {(x^(a), Z^(a))}_{a=1}^A specifies the atomic composition and geometry, where A is the number of atoms, x^(a) ∈ ℝ³ represents the Cartesian coordinates of the a-th atom, and Z^(a) ∈ {1, 2, ..., 118} denotes its atomic number indicating the element type. This structural information is required to define the external potential from nuclei, determine basis function locations and types, and specify the molecular system being solved. Second, during the density optimization process, the current electron density represented by expansion coefficients p ∈ ℝ^M serves as an optimization variable input, where M is the total number of atomic basis functions (typically about 10N for N electrons). These coefficients are subject to the normalization constraint p^T w = N ensuring the integrated density equals the number of electrons, where w is the basis normalization vector with components w_μ = ∫ω_μ(r) dr. For training the KEDF model, additional inputs include multiple density coefficient datapoints {p^(d,k)}_k for each molecular structure d, obtained from intermediate steps k of SCF iterations in KSDFT calculations, along with corresponding labels: kinetic energy values T_S^(d,k) and projected gradients of kinetic energy with respect to coefficients. The molecular structure input allows the model to specify basis function locations, compute inter-atomic distances for attention mechanisms, and account for different atomic types through element-specific parameters. The coefficient input is processed through local frame transformations that align basis orientations with local molecular geometry, natural reparameterization to balance sensitivity across dimensions, and dimension-wise rescaling to manage the vast range of gradient magnitudes, ultimately producing rotationally invariant features suitable for the Graphormer neural network to process.

**Technical Terms:**
- **Cartesian Coordinates**: The (x,y,z) position of a point in three-dimensional space measured along perpendicular axes, commonly used in molecular geometry specification.
- **External Potential**: The potential energy field V_ext(r) experienced by electrons due to attractive Coulomb interaction with positively charged atomic nuclei.
- **Optimization Variable**: A quantity that is adjusted during an optimization procedure to minimize (or maximize) an objective function, here the density coefficients p.
- **Normalization Constraint**: A condition ensuring that integrated electron density equals the total number of electrons: ∫ρ(r) dr = Σ_μ p_μ w_μ = N.
- **Local Frame Transformation**: A coordinate system rotation specific to each atom defined by nearby atoms, used to construct rotationally invariant density coefficient features.
- **Natural Reparameterization**: A change of variables ˜p = M^T p where MM^T = W (overlap matrix) that equalizes the metric contribution across coefficient dimensions.
- **Attention Mechanism**: Neural network component computing weighted combinations of features from different nodes, with weights determined by learned relevance scores based on content and distances.

### Question 20. What kind of outputs are expected for the task?

The primary output expected from M-OFDFT is the optimized ground-state electron density for a given molecular structure, from which various molecular properties can be derived. Specifically, the optimized density coefficients p* that minimize the electronic energy functional E_θ(p,M) provide the ground-state density ρ*(r) = Σ_μ p_μ* ω_μ(r), representing the probability distribution of electrons in three-dimensional space. From this density, several key molecular properties are calculated as outputs. The ground-state electronic energy E* = E_θ(p*,M) gives the total energy of the electronic system including kinetic, Hartree (classical electron-electron repulsion), exchange-correlation, and external (electron-nuclear attraction) energy contributions. Hellmann-Feynman forces on atoms f^(a) = -∇_{x^(a)} E* are computed by differentiating the ground-state energy with respect to atomic positions, providing the forces needed for geometry optimization and molecular dynamics simulations. The electron density itself provides rich information: integrated quantities like Hirshfeld partial atomic charges quantifying electron transfer between atoms, dipole moments measuring charge separation, and density visualizations showing shell structures, bonding regions, and lone pairs. For potential energy surface characterization, outputs include relative energies between different conformations, energy barriers for conformational transitions, and equilibrium bond lengths and angles. The optimization trajectory also provides intermediate outputs: the sequence of density coefficients {p^(k)} and energies {E_θ(p^(k),M)} showing convergence behavior. Numerical accuracy metrics comparing to KSDFT reference include mean absolute errors in energy (target <1 kcal/mol), forces (target <3 kcal/mol/Å), density (measured by Hirshfeld charge errors <0.01 e), and dipole moments (target <0.02 D). For large molecules, computational cost outputs include wall-clock time and scaling exponents characterizing efficiency.

**Technical Terms:**
- **Ground-state Density**: The electron density ρ*(r) corresponding to the lowest-energy quantum state, obtained by minimizing the energy functional.
- **Hartree Energy**: The classical electrostatic repulsion energy E_H[ρ] = (1/2)∫∫ ρ(r)ρ(r')/|r-r'| dr dr' between electrons treated as classical charge distributions.
- **Exchange-Correlation Energy**: The quantum mechanical correction E_XC[ρ] accounting for Pauli exclusion principle (exchange) and correlated electron motion beyond mean-field.
- **Hirshfeld Partial Charges**: Atomic charge assignments q_a obtained by partitioning molecular electron density according to reference atomic densities, measuring charge transfer.
- **Dipole Moment**: A vector μ = ∫r ρ(r) dr - Σ_a Z_a x^(a) measuring charge separation, determining how molecules interact with electric fields, measured in Debye (D).
- **Optimization Trajectory**: The sequence of iterative updates p^(0) → p^(1) → ... → p* during gradient descent minimization, showing the path to convergence.
- **Wall-clock Time**: The actual elapsed time for a calculation measured by real-world clocks, as opposed to CPU time which may differ for parallel computations.

### Question 21. Define the terms used in the paper.

The paper employs numerous specialized terms from quantum chemistry, density functional theory, and machine learning. Key quantum chemistry terms include: **Kohn-Sham DFT (KSDFT)** - the prevailing DFT formulation optimizing N orbital functions {φ_i(r)} for N electrons; **Orbital-Free DFT (OFDFT)** - an alternative formulation optimizing only the electron density ρ(r); **Kinetic Energy Density Functional (KEDF)** - the functional T_S[ρ] approximating non-interacting kinetic energy from density alone; **Electron Density ρ(r)** - the probability distribution of electrons in space; **Atomic Basis Set** - functions {ω_μ(r)} centered on atoms used to expand density; **Expansion Coefficients p** - weights in the representation ρ(r) = Σ_μ p_μ ω_μ(r); **Molecular Structure M** - atomic coordinates and types {(x^(a), Z^(a))}; **Chemical Accuracy** - error threshold ~1 kcal/mol; **Hellmann-Feynman Force** - atomic force f_a = -∇_{x_a} E; **Exchange-Correlation (XC) Functional** - E_XC[ρ] accounting for quantum effects beyond classical electrostatics. Machine learning terms include: **Graphormer** - a Transformer-based graph neural network architecture; **Attention Mechanism** - neural network component computing weighted feature combinations; **Training Data** - labeled examples {M^(d), {p^(d,k), T_S^(d,k), ∇_p T_S^(d,k)}_k}_d; **Gradient Label** - the derivative ∇_p T_S providing landscape information; **Local Frame** - atom-centered coordinate systems for rotational invariance; **Neural Network Potentials (NNPs)** - models predicting energy end-to-end from structure. Functional analysis terms include: **Self-Consistent Field (SCF)** - iterative procedure for solving Kohn-Sham equations; **Density Fitting** - approximating density from orbitals using auxiliary basis; **Even-tempered Basis** - basis sets with systematically scaled exponents; **Natural Reparameterization** - change of variables equalizing metric; **Enhancement Modules** - neural components for gradient expression; **Objective Function Learning** - training models to serve as optimization objectives rather than direct predictors.

**Technical Terms:**
- **Orbital Function φ_i(r)**: A single-electron wavefunction in Kohn-Sham DFT, where the total wavefunction is a Slater determinant of N orbitals for N electrons.
- **Non-interacting Kinetic Energy**: The kinetic energy T_S[ρ] of a fictitious system of non-interacting fermions having the same density as the real interacting system.
- **Auxiliary Basis**: An additional basis set used in density fitting to expand products of orbital basis functions, typically larger and more flexible than the orbital basis.
- **Slater Determinant**: An antisymmetric wavefunction constructed from orbitals that automatically satisfies the Pauli exclusion principle for fermions (electrons).
- **Pauli Exclusion Principle**: Quantum mechanical principle stating that no two fermions can occupy the same quantum state simultaneously, enforced by antisymmetric wavefunctions.
- **Transformer Architecture**: A neural network design using attention mechanisms to process sequential or graph-structured data, originally developed for natural language processing.
- **Rotational Invariance**: Property that predictions remain unchanged when the molecular coordinate system is rotated, reflecting physical symmetry of energy under rotation.

### Question 22. What is the assumption in the paper?

The paper operates under several key assumptions and scope limitations that define the problem domain and methodology. The primary assumption is that accurate kinetic energy density functional models can be learned from data generated by Kohn-Sham DFT calculations at the PBE/6-31G(2df,p) level, which serves as the ground truth for training despite being itself an approximation to exact quantum mechanics. The study focuses on neutral, closed-shell molecular systems without spin polarization, assuming restricted-spin wavefunctions where all electrons are paired—though the methodology could potentially be extended to charged and open-shell systems as noted in Discussion. Molecules are assumed to be in near-equilibrium or physically reasonable conformations, excluding extremely high-energy distorted geometries where the SCF iterations may not converge reliably. The research considers only isolated gas-phase molecules without explicit solvent or periodic boundary conditions, though the authors note that atomic basis representation could support material systems. The atomic basis set representation assumes that electron density around atoms can be effectively captured by radially structured functions centered on nuclei, an assumption well-validated for molecular systems but that might need modification for diffuse electronic states or highly delocalized systems. Training data generation assumes that intermediate SCF iteration steps provide valid density-energy pairs for learning the energy landscape, relying on the mathematical property that each SCF step solves a non-interacting fermion problem with a well-defined kinetic energy. The extrapolation studies assume that training on smaller molecules and peptide fragments provides sufficient information to generalize to larger systems through proper learning formulation. The computational complexity analysis assumes modern hardware with CPUs for KSDFT and GPU acceleration for M-OFDFT neural network evaluation. Finally, the study assumes that the chosen Graphormer architecture with attention mechanisms provides sufficient expressiveness to capture nonlocal density-energy relationships, without exhaustively exploring alternative neural network architectures.

**Technical Terms:**
- **PBE Functional**: The Perdew-Burke-Ernzerhof generalized gradient approximation (GGA) exchange-correlation functional, widely used for its balance of accuracy and computational efficiency.
- **Closed-shell System**: A molecular system where all electrons are paired in doubly-occupied orbitals with opposite spins, having zero total spin angular momentum.
- **Open-shell System**: A molecular system with unpaired electrons (radicals, transition metals in certain oxidation states), requiring spin-polarized DFT treatment.
- **Spin Polarization**: The difference between spin-up and spin-down electron densities ρ_α(r) - ρ_β(r), arising in systems with unpaired electrons or magnetic ordering.
- **Gas-phase**: Isolated molecules without surrounding solvent or crystal environment, corresponding to ultra-high vacuum conditions or low-density gas.
- **Periodic Boundary Conditions**: Computational technique treating infinite extended systems by repeating a unit cell, commonly used for crystalline materials.
- **Diffuse Electronic States**: Quantum states with electron density extending far from nuclei, important for anions, Rydberg states, and weak intermolecular interactions.

### Question 23. Which metric is used?

The paper employs multiple complementary metrics to comprehensively evaluate M-OFDFT performance across different aspects. The primary energy metric is Mean Absolute Error (MAE) in absolute energy: MAE_E = (1/N_test) Σ_i |E_pred^(i) - E_ref^(i)|, measuring average prediction error in kcal/mol with the threshold of chemical accuracy at 1 kcal/mol. For extrapolation studies, per-atom energy MAE is used: MAE_{E/atom} = (1/N_test) Σ_i |E_pred^(i) - E_ref^(i)|/N_atoms^(i), normalizing by molecular size to compare fairly across different scales. For conformational analysis on ethanol and C₇H₁₀O₂ isomers, relative energy MAE measures accuracy in energy differences: MAE_{ΔE} = (1/N_pairs) Σ_{i,j} |(E_pred^(i) - E_pred^(j)) - (E_ref^(i) - E_ref^(j))|, which is more relevant for chemical applications than absolute energies. The Hellmann-Feynman force MAE evaluates gradient accuracy: MAE_F = (1/(N_test · A)) Σ_i Σ_a ||f_pred^(i,a) - f_ref^(i,a)||, averaging over all atoms in all test structures with typical threshold around 3 kcal/mol/Å. For electron density quality, Hirshfeld partial charge MAE measures: MAE_q = (1/(N_test · A)) Σ_i Σ_a |q_pred^(i,a) - q_ref^(i,a)| with good performance below 0.01 e. Dipole moment MAE: MAE_μ = (1/N_test) Σ_i ||μ_pred^(i) - μ_ref^(i)|| measures molecular polarity prediction in Debye units. Computational efficiency is evaluated using wall-clock time in seconds and empirical scaling characterized by fitting T(N) = (aN + b)^c + d where N is the number of electrons or heavy atoms. Statistical significance is assessed using 95% confidence intervals computed from bootstrap resampling or assuming Gaussian error distributions. All metrics compare M-OFDFT predictions against KSDFT reference calculations treated as ground truth.

**Technical Terms:**
- **Mean Absolute Error (MAE)**: The average of absolute differences between predicted and reference values, less sensitive to outliers than mean squared error.
- **Per-atom Normalization**: Dividing extensive quantities (that scale with system size) by the number of atoms to enable fair comparison across different molecular sizes.
- **Energy Difference/Relative Energy**: The difference in energy between two configurations ΔE = E₁ - E₂, more physically meaningful than absolute energies which contain arbitrary zero-point offsets.
- **Force Magnitude**: The Euclidean norm ||f|| = √(f_x² + f_y² + f_z²) of the force vector giving the total force strength on an atom.
- **Bootstrap Resampling**: A statistical method for estimating confidence intervals by repeatedly sampling with replacement from the data and computing statistics on each resample.
- **95% Confidence Interval**: A range expected to contain the true population parameter with 95% probability, typically computed as mean ± 1.96 × standard_error.
- **Wall-clock Time**: Actual elapsed time measured from start to finish of a calculation, including all computational overhead and waiting times.
- **Empirical Scaling Exponent**: The power c in T(N) ∝ N^c characterizing how computational cost grows with system size, determined by fitting to measured timing data.

---

## 4. Method (0/11 completed)

### Question 24. Which method do you extend?

M-OFDFT extends orbital-free density functional theory by incorporating a deep learning approximation for the kinetic energy density functional, building upon the Graphormer neural network architecture originally developed for molecular property prediction. The methodological foundation comes from orbital-free DFT formalism established by Hohenberg-Kohn and extended by Wang and Carter, where electronic energy is minimized as a functional of electron density alone: min_ρ E[ρ] = T_S[ρ] + E_H[ρ] + E_XC[ρ] + E_ext[ρ], with the challenge being accurate approximation of the kinetic energy term T_S[ρ]. The deep learning model architecture extends Graphormer (Ying et al. 2021, Shi et al. 2022), a Transformer-based graph neural network using attention mechanisms to process molecular graphs, which was originally designed for end-to-end property prediction from molecular structures. M-OFDFT adapts this architecture to take density coefficients as additional input beyond molecular structure, enabling it to approximate the functional T_S,θ(p,M) mapping density representations to kinetic energy values. The density representation extends the atomic basis expansion approach used in conventional quantum chemistry, where density is written as ρ(r) = Σ_μ p_μ ω_μ(r) with atomic-centered basis functions—a technique standard in Gaussian basis set methods but novel for machine learning KEDF approximation. The training methodology extends supervised learning with crucial innovations: generating multiple density datapoints per structure from SCF iterations (building on insights from Snyder et al. 2012 and Brockherde et al. 2017) and incorporating gradient labels (functional derivatives) into the training loss following the principle that the model must capture optimization landscapes, not just prediction mappings. The geometric invariance approach extends local frame methods from neural network potentials (Han et al. 2018) to handle equivariant density coefficient tensors. The density optimization procedure extends gradient descent optimization from electronic structure methods (Yoshikawa and Sumita 2022) with novel initialization strategies including H¨uckel method and projected MINAO.

**Technical Terms:**
- **Hohenberg-Kohn Theorem**: The fundamental theorems of DFT proving that ground-state properties are uniquely determined by electron density and that ground state minimizes the energy functional.
- **Slater Determinant**: An antisymmetric wavefunction built from single-particle orbitals that satisfies the Pauli exclusion principle for indistinguishable fermions.
- **Transformer**: A neural network architecture using self-attention mechanisms to process sequential or graph data by computing all-to-all interactions between elements.
- **Graph Neural Network (GNN)**: Neural architectures designed to process graph-structured data by iteratively updating node features through message passing with neighbors.
- **Gaussian Basis Set**: A set of basis functions constructed from Gaussian functions exp(-αr²) or their derivatives, commonly used in quantum chemistry for computational efficiency.
- **Functional Derivative**: The variation δF[ρ]/δρ(r) of a functional F[ρ] with respect to density at each point r, generalizing the gradient concept from functions to functionals.
- **H¨uckel Method**: A simplified molecular orbital approximation solving eigenvalue problems with parameterized Hamiltonian matrices, useful for π-electron systems and as an initialization strategy.

### Question 25. Explain that the extensions made in the proposed method are widely applicable to other methods.

The extensions introduced in M-OFDFT possess significant generality and applicability beyond the specific implementation presented. The atomic basis coefficient representation for density input can be applied to any machine learning approach targeting density functionals, including exchange-correlation functional learning (as demonstrated by Dick and Fernandez-Serra 2020, Chen et al.'s DeePKS 2021), beyond just kinetic energy functionals, since any molecular system can be represented using atomic basis expansions regardless of the specific functional being approximated. The methodology of generating multiple density datapoints with gradient labels from SCF iterations is applicable to training any functional model serving as an optimization objective, whether in quantum chemistry (approximating other energy components), molecular dynamics (learning force fields as energy gradients), or optimization problems in other scientific domains where the objective function must be learned from data. The local frame technique for achieving geometric invariance with coefficient representations extends to any equivariant molecular property prediction task where inputs transform non-trivially under rotation, including learning molecular tensors like polarizabilities, quadrupole moments, or stress tensors in materials, providing a general strategy for handling geometric equivariance alternative to global frame averaging or equivariant neural networks. The enhancement modules for expressing steep gradients—including dimension-wise rescaling, natural reparameterization, and atomic reference offsets—address a fundamental challenge in learning optimization landscapes with heterogeneous scales, applicable to learning objectives in diverse domains like molecular conformation generation, protein structure prediction, or material design where different degrees of freedom exhibit vastly different sensitivities. The Graphormer architecture itself, while used here for kinetic energy functional, can serve as a backbone for approximating other nonlocal molecular functionals, electron correlation effects, or properties requiring long-range interaction modeling. The training strategy combining energy value and gradient labels applies generally to any regression problem where gradient information is available and the learned function will be used in gradient-based optimization or where capturing local landscape curvature improves model quality. The formulation of learning an objective function rather than end-to-end prediction represents a general machine learning paradigm shift applicable to inverse problems and optimization-based inference across scientific computing.

**Technical Terms:**
- **Exchange-Correlation Functional E_XC[ρ]**: The component of DFT energy accounting for quantum exchange (Pauli principle) and correlation (beyond mean-field) effects.
- **Force Field**: A potential energy function used in classical molecular dynamics, typically parameterized as sums of bond, angle, torsion, and non-bonded interaction terms.
- **Equivariant Property**: A quantity that transforms predictably under symmetry operations (e.g., vectors rotate with coordinate system), as opposed to invariant scalars.
- **Polarizability**: A molecular property (second-rank tensor) quantifying how electron distribution responds to applied electric fields, determining induced dipole moments.
- **Quadrupole Moment**: A measure of charge distribution's deviation from spherical symmetry beyond dipole, represented as a traceless second-rank tensor.
- **Stress Tensor**: A mechanical property describing internal forces in materials, relating strain to deformation, important in materials mechanics.
- **Inverse Problems**: Computational problems where causes must be inferred from observed effects, often ill-posed and requiring regularization.
- **Optimization-based Inference**: Inference paradigm where predictions are obtained by optimizing an objective function, as opposed to direct function evaluation.

### Question 26. List the differences between the proposed method and the conventional methods.

The key differences between M-OFDFT and conventional methods are:

- **Density Representation**: M-OFDFT uses expansion coefficients (~10N parameters) under atomic basis sets, while classical OFDFT and previous ML-OFDFT methods use grid-based representations (~10⁴N points), reducing dimensionality by three orders of magnitude.

- **KEDF Approximation**: M-OFDFT learns T_S,θ(p,M) from KSDFT data using deep neural networks, while classical OFDFT (Thomas-Fermi, von Weizsäcker, APBE) derives analytical expressions from uniform electron gas theory.

- **Nonlocality**: M-OFDFT captures nonlocal density interactions through Graphormer attention mechanisms operating on compact coefficient representation, while classical methods use local/semi-local approximations and previous nonlocal ML methods face prohibitive costs with grid representations.

- **Learning Formulation**: M-OFDFT learns the optimization objective function E_θ(p,M) with density as optimization variable, while neural network potentials learn end-to-end energy prediction E(M) without density optimization.

- **Training Data**: M-OFDFT uses multiple density configurations {p^(d,k)}_k per structure from SCF iterations with both energy and gradient labels, while conventional ML uses single ground-state points per structure.

- **Geometric Invariance**: M-OFDFT employs local frames to construct rotationally invariant features from equivariant coefficients, while NNPs use distance-based features inherently invariant and equivariant GNNs process coordinates directly.

- **Gradient Enhancement**: M-OFDFT includes specialized modules (dimension-wise rescaling, natural reparameterization, atomic reference) for expressing steep gradients, absent in conventional neural networks.

- **Density Optimization**: M-OFDFT uses gradient descent with on-manifold initialization (H¨uckel or ProjMINAO), while previous ML-OFDFT methods require manifold projection at each optimization step.

- **Computational Complexity**: M-OFDFT achieves empirical O(N^1.46) scaling between classical OFDFT's O(N²) and KSDFT's O(N^2.49-3), with absolute time faster than KSDFT for all tested sizes.

- **Accuracy and Extrapolation**: M-OFDFT achieves chemical accuracy (~1 kcal/mol) with decreasing per-atom errors on 10× larger molecules, while classical OFDFT has ~10-100 kcal/mol errors and NNPs show increasing per-atom errors.

**Technical Terms:**
- **Semi-local Approximation**: Methods where predicted quantities depend only on density and its derivatives at nearby points, ignoring long-range interactions.
- **Attention Mechanism**: Neural network component computing weighted feature combinations based on learned relevance scores, enabling all-to-all node interactions.
- **End-to-end Learning**: Machine learning paradigm directly mapping inputs to final outputs without intermediate optimization or explicit modeling of mechanisms.
- **Manifold Projection**: Mathematical operation mapping points onto a constraint surface or lower-dimensional subspace satisfying physical constraints.
- **Gradient Descent**: Iterative optimization algorithm updating variables in the direction of negative gradient: p^(k+1) = p^(k) - ε∇E(p^(k)).
- **On-manifold Initialization**: Starting density optimization from a physically reasonable density satisfying constraints, without requiring projection during optimization.
- **Scaling Complexity**: How computational cost grows with system size, characterized by exponent in T(N) ∝ N^c relationships.

### Question 27. How many main modules does the proposed model have? Explain each method briefly.

The M-OFDFT model consists of four main interconnected modules that work together to approximate the kinetic energy density functional and enable density optimization:

**Module 1: Density Representation and Preprocessing**
This module transforms raw density coefficients p into model-ready features through three sub-components: (a) Local frame transformation that rotates coefficients into atom-specific coordinate systems aligned with local molecular geometry to achieve rotational invariance; (b) Natural reparameterization ˜p = M^T p that equalizes metric contributions across dimensions by setting MM^T = W (overlap matrix), implementing natural gradient descent; (c) Dimension-wise rescaling p'_a,τ = λ_Z(a),τ p_a,τ that balances coefficient-gradient scales across different atom types Z and basis patterns τ, managing the vast range of physical magnitudes.

**Module 2: Atomic Reference Module**
This linear module T_AtomRef(p,M) = ḡ_M^T p + T̄_M provides a baseline kinetic energy estimate constructed from per-element statistics. The gradient ∇_p T_AtomRef = ḡ_M is constant, offsetting the mean gradient that the neural network must learn, effectively reducing gradient label scales and facilitating training stability.

**Module 3: Graphormer Neural Network**
This is the core learnable component using Transformer architecture with L layers of attention-based feature updates. Each layer computes attention weights between all atom pairs based on their features and distances, updates each atom's features by aggregating information from all other atoms weighted by attention scores, capturing essential nonlocal density interactions. The initial node embeddings encode atom types Z^(a), rescaled density coefficients p'_a, and positional information. After L layers of updates, scalar features are extracted from each node and summed to produce the neural network's kinetic energy contribution.

**Module 4: Energy Assembly and Optimization**
This module combines the neural network output with the atomic reference to give T_S,θ(p,M), then constructs the total electronic energy E_θ(p,M) = T_S,θ(p,M) + E_H(p,M) + E_XC(p,M) + E_ext(p,M) where the last three terms are computed using standard DFT formulas. During deployment, gradient descent optimization iteratively updates p^(k+1) := p^(k) - ε[I - ww^T/(w^T w)]∇_p E_θ(p^(k),M), projecting gradients onto the constraint surface to maintain normalized density until convergence to ground state.

**Technical Terms:**
- **Feature Embedding**: Neural network transformation converting discrete input categories (like atom types) and continuous values into dense vector representations.
- **Layer-wise Updates**: Sequential processing where each network layer transforms representations, with information flowing from input through multiple intermediate layers to output.
- **Node Aggregation**: Operation combining features from multiple nodes (atoms) into a single representation, typically using summation, mean, or attention-weighted combinations.
- **Energy Component**: Individual terms in the energy functional decomposition E = T_S + E_H + E_XC + E_ext, each representing different physical interactions.
- **Constraint Projection**: Mathematical operation ensuring optimization updates maintain constraints, here achieved by subtracting the component parallel to the constraint normal.
- **Convergence**: The property that iterative optimization approaches a stable solution where further iterations produce negligible changes, typically measured by gradient norm or energy change.

### Question 28. Explain the structure of the model.

The M-OFDFT model structure follows a hierarchical architecture processing both molecular structure and density information to predict kinetic energy while respecting physical constraints and symmetries. The input processing begins with molecular structure M = {(x^(a), Z^(a))}_a and density coefficients p, which undergo local frame transformation where each coefficient p_a,τ is rotated into a coordinate system aligned with nearby atoms around atom a, making features rotationally invariant. These transformed coefficients then pass through natural reparameterization ˜p = M^T p that equalizes sensitivities across dimensions. The model splits into two parallel branches: the atomic reference branch T_AtomRef(˜p,M) = Σ_a Σ_τ ḡ_Z(a),τ ˜p_a,τ + Σ_a T̄_Z(a) + T̄_global computes a linear baseline using element-specific statistics; the neural network branch first applies dimension-wise rescaling p' = Λ˜p with factors λ_Z,τ managing gradient ranges, then feeds into the Graphormer architecture. The Graphormer network structure consists of node embeddings that combine atom type embeddings (learned vectors for each element), coefficient features p'_a distributed over atoms, and edge embeddings encoding inter-atomic distances through Gaussian basis expansion. The core architecture comprises L layers (L=12 in implementation) of multi-head attention blocks where each attention head computes attention scores α_ij between atoms i and j using queries, keys, values derived from node features and modified by spatial bias from edge embeddings, enabling long-range nonlocal interactions. Each layer performs h_a^(ℓ+1) = Σ_b α_ab h_b^(ℓ) followed by feed-forward neural networks and residual connections, progressively refining atomic features. The final layer extracts scalar features from each node's embedding and sums them: T_NN(p',M) = Σ_a ReadOut(h_a^(L)), giving the neural network's kinetic energy prediction. The total KEDF model output is T_S,θ(p,M) = T_AtomRef(˜p,M) + T_NN(p',M), combining the linear reference and nonlinear neural components. For complete energy evaluation, three additional terms are computed using established DFT formulas implemented with automatic differentiation in PyTorch: E_H(p,M), E_XC(p,M), and E_ext(p,M).

**Technical Terms:**
- **Multi-head Attention**: Attention mechanism with multiple parallel attention operations (heads) each learning different interaction patterns, whose outputs are concatenated.
- **Query-Key-Value**: The three component vectors in attention computation where attention weights are α ∝ exp(query·key) and outputs are Σ_i α_i value_i.
- **Spatial Bias**: Additional terms in attention score computation depending on geometric information (distances, angles) to inject spatial structure into graph neural networks.
- **Feed-Forward Network**: Multi-layer perceptron applied independently to each node's features, typically with ReLU activation and larger hidden dimension than input/output.
- **Residual Connection**: Neural network skip connection adding layer input to output (h^(ℓ+1) = h^(ℓ) + Transform(h^(ℓ))), improving gradient flow and enabling deeper networks.
- **ReadOut Function**: Final network layer extracting scalar or vector predictions from node embeddings, often a simple linear projection followed by pooling.
- **Automatic Differentiation**: Computational technique automatically computing derivatives of functions specified as programs, enabling gradient calculation through complex operations.

### Question 29. Define the input to the proposed method.

The input to M-OFDFT consists of two primary components that together fully specify the computational task. The molecular structure input M = {(x^(a), Z^(a))}_{a=1}^A provides atomic configuration information where x^(a) ∈ ℝ³ represents the three-dimensional Cartesian coordinates (x,y,z positions in Angstroms or Bohr) of the a-th atom, and Z^(a) ∈ {1,2,...,118} denotes its atomic number (1 for hydrogen, 6 for carbon, 8 for oxygen, etc.) indicating the element type. This structural information determines: the number and types of atoms A, the geometric arrangement defining inter-atomic distances and molecular shape, the external potential V_ext(r) = -Σ_a Z^(a)/|r-x^(a)| from nuclear charges, and the specification of atomic basis function locations and types. The density coefficient input p ∈ ℝ^M represents the electron density through expansion coefficients in the atomic basis representation ρ(r) = Σ_{μ=1}^M p_μ ω_μ(r), where M is the total number of basis functions (typically M ≈ 10N for N electrons). Each coefficient p_μ = p_{a,τ} is indexed by a center atom a and a basis function pattern τ (corresponding to different radial functions and angular momentum channels like s, p, d), making the dimensionality depend on molecular size and composition. The coefficients satisfy the normalization constraint Σ_μ p_μ w_μ = N where w_μ = ∫ω_μ(r) dr is the basis normalization and N is the total number of electrons determined by Σ_a Z^(a). During training, the model receives batches of input tuples {(M^(d), p^(d,k), T_S^(d,k), ∇_p T_S^(d,k))}_k where d indexes training molecular structures, k indexes multiple density datapoints per structure from SCF iterations, T_S^(d,k) is the kinetic energy label, and ∇_p T_S^(d,k) is the gradient label. During deployment for solving a molecular system, only M is provided as fixed input, while p becomes an optimization variable initialized using H¨uckel or ProjMINAO method and iteratively updated to minimize E_θ(p,M) until convergence to the ground-state density p*.

**Technical Terms:**
- **Cartesian Coordinates**: Position specification using three perpendicular axes (x,y,z), as opposed to spherical (r,θ,φ) or other coordinate systems.
- **Atomic Number Z**: The number of protons in an atomic nucleus, uniquely identifying the chemical element and determining nuclear charge.
- **Basis Function Pattern τ**: The angular momentum (s/p/d/f) and radial profile characteristics of atomic basis functions, determining their spatial shape.
- **Normalization Constraint**: The requirement that integrated density equals total electron count: ∫ρ(r) dr = Σ_μ p_μ w_μ = N.
- **Batch Processing**: Machine learning technique processing multiple training examples simultaneously to improve computational efficiency and gradient estimation.
- **Optimization Variable**: A quantity adjusted during iterative optimization to minimize an objective function, here the density coefficients p.
- **Initialization Strategy**: The method for choosing starting values for optimization variables, crucial for convergence to desired solutions in non-convex problems.

### Question 30. Explain how the input features are extracted.

Input feature extraction in M-OFDFT involves multiple sophisticated transformation steps to convert raw molecular structure and density coefficients into representations suitable for neural network processing while ensuring physical consistency and numerical stability. Starting from raw inputs (M,p), the extraction pipeline first constructs the atomic basis set {ω_μ(r)} on-the-fly using even-tempered Gaussian functions centered on each atom with systematically scaled exponents β^k (β=2.5, k=0,1,2,...) spanning different radial extents and angular momentum channels (s,p,d,f), with the number and types determined by each atom's element. The density coefficients p are then subjected to local frame transformation: for each atom a, a local coordinate system is established by pointing the x-axis toward the nearest heavy atom at x_a^(1), the z-axis perpendicular to the plane formed by the second-nearest atom x_a^(2), and y-axis completing a right-handed system; basis functions on atom a are rotated into this local frame, transforming the corresponding coefficients p_a accordingly to produce rotationally invariant features. Next, natural reparameterization is applied where coefficients are transformed as ˜p = M^T p with M chosen so MM^T = W (the overlap matrix with elements W_μν = ∫ω_μ(r)ω_ν(r) dr), equalizing metric contributions and enabling natural gradient descent. Statistical preprocessing then computes element-type-specific statistics from training data: biases ¯p_Z,τ = mean{p_{a,τ} | Z^(a)=Z}, gradient means ḡ_Z,τ = mean{∇_{p_{a,τ}}T_S | Z^(a)=Z}, gradient scales std_grad_Z,τ, and coefficient scales std_coeff_Z,τ. Dimension-wise rescaling applies: p'_{a,τ} = λ_Z(a),τ(˜p_{a,τ} - ¯p_Z(a),τ) where scaling factors λ_Z,τ balance coefficient and gradient magnitudes according to Eq. (5) in the paper. For molecular structure features, inter-atomic distances d_ab = ||x^(a) - x^(b)|| are computed and embedded using Gaussian radial basis functions: φ_k(d) = exp(-(d-μ_k)²/σ²) with centers μ_k spanning relevant distance ranges. Atom type features are embedded as learned vectors e_Z ∈ ℝ^d where d is the embedding dimension (512 in implementation), initialized randomly and optimized during training. The final extracted features consist of: node features combining atom type embeddings e_Z(a), rescaled coefficient vectors {p'_{a,τ}}_τ arranged by basis pattern, and edge features encoding distances through Gaussian embeddings {φ_k(d_ab)}_k for all atom pairs, ready for Graphormer processing.

**Technical Terms:**
- **Even-tempered Basis**: Basis set where exponents follow geometric progression α_k = α_0 β^k, providing systematic coverage of radial space with parameter β.
- **Angular Momentum Channel**: Quantum mechanical orbital type characterized by quantum number ℓ (ℓ=0:s, 1:p, 2:d, 3:f), determining angular dependence of wavefunctions.
- **Overlap Matrix W**: Matrix with elements W_μν = ∫ω_μ(r)ω_ν(r) dr measuring spatial overlap between basis functions μ and ν.
- **Natural Gradient Descent**: Optimization method using the inverse metric tensor (here W^(-1/2)) to account for parameter space geometry, often converging faster than standard gradient descent.
- **Gaussian Radial Basis Functions**: Smooth, localized functions φ(r) = exp(-(r-μ)²/σ²) used to embed continuous distances into fixed-dimensional feature vectors.
- **Learned Embedding**: Vector representations of discrete entities (like atom types) whose values are learned parameters optimized during training to capture relevant properties.
- **Edge Features**: Representations encoding pairwise relationships between nodes in a graph, here incorporating geometric information about atom pairs.
