# 60QA for "Overcoming the Barrier of Orbital-Free Density Functional Theory for Molecular Systems Using Deep Learning"

## 1. Introduction (0/10 completed)

### Question 1. What is the social background of this research?

Density functional theory (DFT) is a powerful quantum chemistry method that has become one of the most popular computational approaches for solving electronic states and determining the energy and properties of molecular systems. Its widespread adoption stems from providing an appropriate balance between accuracy and computational efficiency, which has enabled numerous scientific discoveries across chemistry, materials science, and related fields. The prevailing Kohn-Sham DFT (KSDFT) formulation, while highly successful, requires optimizing N orbital functions for an N-electron system, resulting in computational complexity that scales as O(N³). This higher complexity is increasingly problematic for contemporary research where large-scale system simulations are in high demand for practical applications. The computational burden limits the ability to study larger molecules, proteins, and complex molecular systems that are crucial for understanding real-world phenomena in drug discovery, materials design, and biological processes. Consequently, there is growing interest in orbital-free DFT (OFDFT), which follows the original spirit of DFT by optimizing a single electron density function rather than multiple orbital functions, thereby reducing computational complexity to O(N²). However, OFDFT's accuracy has been limited by the challenge of approximating the kinetic energy density functional, particularly for non-periodic molecular systems where electron density is highly non-uniform. Recent advances in machine learning, especially deep learning, present new opportunities to overcome this accuracy barrier by learning complex functional approximations from data, potentially enabling efficient and accurate quantum chemistry calculations for large-scale molecular systems.

**Technical Terms:**
- **Density Functional Theory (DFT)**: A quantum mechanical method for calculating the electronic structure of many-body systems by using electron density rather than wave functions as the fundamental variable.
- **Kohn-Sham DFT (KSDFT)**: The prevailing formulation of DFT that represents the system using N orbital functions for N electrons, allowing explicit calculation of non-interacting kinetic energy.
- **Orbital-Free DFT (OFDFT)**: An alternative DFT formulation that optimizes only the electron density function, eliminating the need to compute individual orbitals and reducing computational complexity.
- **Electron Density (ρ(r))**: The probability distribution of electrons in space, representing the one-body reduced density.
- **Kinetic Energy Density Functional (KEDF)**: The functional that approximates the non-interacting kinetic energy of electrons as a function of electron density, denoted as T_S[ρ].
- **Computational Complexity Scaling**: The relationship between system size (N electrons) and computational cost; O(N³) for KSDFT versus O(N²) for OFDFT.
- **Non-periodic Molecular Systems**: Molecules that lack the repeating lattice structure found in crystalline materials, making them more challenging for OFDFT approximations.

### Question 2. What is the target problem of this work?

The target problem of this research is to develop an accurate and computationally efficient orbital-free density functional theory (OFDFT) method capable of solving molecular systems, particularly non-periodic molecules that have traditionally been beyond the reach of OFDFT approaches. The core challenge lies in approximating the kinetic energy density functional (KEDF), denoted as T_S[ρ], which is the central task in OFDFT. Classical approximations for KEDF, developed based on uniform electron gas theory, have achieved success for periodic material systems but remain highly inaccurate for molecular systems where electron density is far from uniform. The problem is further complicated by the need to capture non-local interactions of electron density at different spatial points, which is essential for accurate KEDF approximation but computationally prohibitive with traditional grid-based density representations. The research addresses the fundamental tension between accuracy and computational efficiency in quantum chemistry: while Kohn-Sham DFT provides good accuracy, it scales as O(N³) which becomes prohibitive for large molecular systems; OFDFT offers better O(N²) scaling but lacks sufficient accuracy for molecules. This work aims to overcome the accuracy barrier of OFDFT for molecular systems by using deep learning to approximate the KEDF, while maintaining the computational scaling advantage that makes OFDFT attractive for studying large-scale molecular systems including proteins and other complex biomolecular structures.

**Technical Terms:**
- **Kinetic Energy Density Functional (KEDF)**: A functional T_S[ρ] that approximates the non-interacting kinetic energy of electrons as a function of electron density, which is the central approximation challenge in OFDFT.
- **Uniform Electron Gas Theory**: A theoretical framework assuming electrons are distributed uniformly in space, which forms the basis for classical KEDF approximations but fails for molecular systems.
- **Non-local Interactions**: Interactions between electron density at spatially separated points, which require considering density values across the entire molecular system rather than just local neighborhoods.
- **Grid-based Density Representation**: A method of representing electron density by discretizing space into a grid and storing density values at grid points, which requires many points (~10⁴N) for molecular systems.
- **Computational Scaling**: The relationship between system size and computational cost; O(N³) means cost increases with the cube of system size, while O(N²) means quadratic increase.

### Question 3. Explain a typical use case.

A typical use case for M-OFDFT is calculating the ground-state electronic properties of large molecular systems, such as proteins, where traditional Kohn-Sham DFT becomes computationally prohibitive. For example, the research demonstrates M-OFDFT's application to chignolin, a small protein consisting of 10 residues and 168 atoms after neutralization. In this scenario, researchers need to determine the electronic energy, electron density distribution, and forces on atoms for conformational analysis or molecular dynamics simulations. The workflow begins with a molecular structure M defined by atomic coordinates and atomic numbers. Using a pre-trained deep learning KEDF model, M-OFDFT optimizes the electron density represented as expansion coefficients p under an atomic basis set to minimize the total electronic energy. Through iterative gradient descent optimization constrained to maintain normalized density, the method converges to the ground-state density and energy. The optimized density reveals the shell structure around atoms and provides accurate energies comparable to Kohn-Sham DFT but with significantly reduced computational cost. Another typical use case involves studying conformational spaces, such as analyzing ethanol structures from molecular dynamics trajectories. Here, M-OFDFT can rapidly evaluate energies and forces for thousands of molecular conformations to understand relative stability and dynamics. The method is particularly valuable when extrapolating to molecules much larger than those in the training set, such as calculating properties of molecules with 224 atoms when trained only on molecules up to 15 heavy atoms, enabling large-scale molecular simulations previously unaffordable with conventional quantum chemistry methods.

**Technical Terms:**
- **Ground-state Electronic Properties**: The lowest energy electronic configuration and associated properties like density and forces when all electrons occupy their most stable quantum states.
- **Conformational Analysis**: The study of different three-dimensional arrangements (conformations) of a molecule and their relative energies and properties.
- **Molecular Dynamics Simulations**: Computational simulations that model the time-dependent motion of atoms in molecules by calculating forces and updating positions iteratively.
- **Atomic Basis Set**: A set of mathematical functions {ω_μ(r)} centered on atoms used to represent electron density as a linear combination, where each function describes electron distribution patterns around atoms.
- **Expansion Coefficients**: The numerical weights p_μ in the linear combination ρ(r) = Σ_μ p_μ ω_μ(r) that determine how much each basis function contributes to the total electron density.
- **Shell Structure**: The organization of electron density into concentric shells around atomic nuclei, with core electrons close to the nucleus and valence electrons in outer shells.
- **Heavy Atoms**: Non-hydrogen atoms in a molecule, which typically determine molecular size complexity in quantum chemistry calculations.

### Question 4. Why is this task challenging?

This task is fundamentally challenging because approximating the kinetic energy density functional (KEDF) for molecular systems requires capturing the complex, non-uniform distribution of electron density in molecules, which differs dramatically from the uniform electron gas assumptions underlying classical KEDF approximations. The electron density in molecules exhibits sharp variations near atomic nuclei due to the nuclear cusp condition, forms shell structures around atoms, and involves covalent bonding regions where density is shared between atoms—all features that classical KEDFs based on uniform or slowly-varying density assumptions fail to capture accurately. The challenge is amplified by the essential requirement of non-locality in KEDF approximation, meaning the kinetic energy at any point depends on electron density at distant spatial locations, not just the immediate neighborhood. Incorporating non-locality is computationally expensive, especially with grid-based density representations that require approximately 10⁴N grid points for N-electron molecular systems, making non-local calculations prohibitively costly. Furthermore, the task involves unconventional machine learning challenges beyond standard supervised learning. The KEDF model serves as an optimization objective rather than a direct prediction function, requiring it to accurately capture the entire energy landscape over the space of possible density configurations for each molecular structure, not just at a single ground-state point. This necessitates generating multiple density datapoints with gradient labels for each molecular structure during training. Additionally, the model must respect geometric invariance—the output kinetic energy must remain unchanged under molecular rotation while the input density coefficients transform equivariantly. Finally, achieving meaningful extrapolation to molecules much larger than training data is critical but difficult, as the ground-state energy involves intricate many-body interactions among electrons and nuclei, creating a highly complex function that typically fails to generalize well across different molecular scales without appropriate problem formulation.

**Technical Terms:**
- **Nuclear Cusp Condition**: A physical constraint requiring electron density to exhibit a specific sharp increase (cusp) at nuclear positions, following the relationship ∂ρ/∂r|_{r→0} = -2Zρ(0) where Z is the atomic number.
- **Covalent Bonding Regions**: Spatial regions between atoms where electron density is shared and accumulated due to chemical bond formation, characterized by non-uniform density distributions.
- **Energy Landscape**: The multidimensional surface representing how total energy varies as a function of all possible density configurations, which the optimization process must navigate to find the minimum.
- **Gradient Labels**: The partial derivatives ∇_p E with respect to density coefficients p, which indicate the direction and magnitude of energy change and are essential for training the model to support gradient-based optimization.
- **Geometric Invariance**: The physical requirement that predicted energies remain unchanged (invariant) under spatial transformations like rotation, while input density representations transform accordingly (equivariantly).
- **Many-body Interactions**: Complex quantum mechanical interactions among multiple particles (electrons and nuclei) that cannot be decomposed into simple pairwise interactions, making the total energy a highly non-linear function.
- **Equivariant Transformation**: A property where the representation of an object transforms in a specific, predictable way under symmetry operations (like rotation), maintaining consistency with the physical transformation.

### Question 5. Why are conventional studies insufficient?

Conventional studies are insufficient because classical orbital-free DFT approaches, while computationally efficient, fail to achieve chemical accuracy for molecular systems, and previous machine learning attempts have been limited in scope and extrapolation capability. Classical KEDF approximations such as Thomas-Fermi (TF), von Weizsäcker (vW), and various gradient expansion corrections were developed based on uniform electron gas theory and have achieved successes for periodic material systems with relatively uniform electron density. However, these methods produce energy errors orders of magnitude larger than the chemical accuracy threshold (1 kcal/mol) for molecules because molecular electron density is highly non-uniform, featuring sharp nuclear cusps, shell structures, and covalent bonding regions that violate the slowly-varying density assumptions. Early machine learning approaches using kernel ridge regression showed promise only for one-dimensional systems, lacking the scalability needed for three-dimensional molecular applications. More recent deep learning approaches have explored learning point-wise kinetic energy density from electron density features, but these semi-local methods cannot capture the essential non-local interactions required for accurate KEDF approximation. Other works attempting to incorporate non-locality using grid-based density representations face prohibitive computational costs, limiting applications to systems with only dozens of atoms—far too small to demonstrate the scaling advantage that motivates OFDFT development. Neural network potentials (NNPs) that predict ground-state energy end-to-end from molecular structure offer fast predictions but do not describe electronic states and suffer from poor extrapolation to larger molecules than those in training data. Critically, few previous studies have demonstrated accurate extrapolation well beyond training molecule sizes, which is essential for exploiting OFDFT's computational scaling benefits. Without good extrapolation, the method cannot be applied to the large-scale molecular systems for which OFDFT's efficiency advantage would be most valuable, undermining the entire motivation for using orbital-free approaches over the more accurate Kohn-Sham DFT.

**Technical Terms:**
- **Chemical Accuracy**: A threshold of approximately 1 kcal/mol (or 1.6 milli-Hartrees) for energy prediction errors, considered sufficient for reliable prediction of chemical properties and reaction energetics.
- **Gradient Expansion**: A mathematical approximation technique that expresses the KEDF as a series of terms involving the electron density and its spatial derivatives (gradients, Laplacians, etc.).
- **Thomas-Fermi (TF) KEDF**: The simplest classical kinetic energy approximation proportional to ρ^(5/3), exact for uniform electron gas but highly inaccurate for molecular systems.
- **von Weizsäcker (vW) KEDF**: A gradient correction to TF that captures some kinetic energy contributions from density variations, proportional to |∇ρ|²/ρ.
- **Kernel Ridge Regression**: A non-parametric machine learning method that learns functions by finding optimal weights for training points in a high-dimensional feature space defined by a kernel function.
- **Point-wise Kinetic Energy Density**: The kinetic energy per unit volume at each spatial location, whose integral over space gives the total kinetic energy.
- **Neural Network Potentials (NNPs)**: Machine learning models that directly predict ground-state energy from atomic positions and types without explicitly modeling electron density or electronic structure.

### Question 6. What is proposed and solved in this study?

This study proposes M-OFDFT (Molecular Orbital-Free Density Functional Theory), a novel OFDFT approach that uses a deep learning model to approximate the kinetic energy density functional, enabling accurate quantum chemistry calculations for molecular systems while maintaining computational efficiency. The key innovation is representing electron density using expansion coefficients under an atomic basis set rather than grid-based representations, which reduces dimensionality from approximately 10⁴N to only about 10N parameters, making non-local KEDF calculations computationally affordable. The deep learning model, based on the Graphormer architecture (a Transformer variant), takes as input the density coefficients distributed over atoms along with molecular structure information, and uses attention mechanisms to capture essential non-local interactions of electron density across distant spatial regions. To address unconventional machine learning challenges, the research introduces three key technical solutions: (1) methods to generate multiple density datapoints with gradient labels for each molecular structure to train the model for use as an optimization objective; (2) local frame techniques to guarantee geometric invariance where output energy remains unchanged under molecular rotation while input coefficients transform equivariantly; (3) enhancement modules that balance sensitivity, rescale gradients dimension-wise, and provide reference offsets to enable the model to express the steep gradients required by physical mechanisms. M-OFDFT solves the problem of accurate molecular electronic structure prediction by achieving chemical accuracy (errors below 1 kcal/mol for energy and approximately 3 kcal/mol/Å for forces) on diverse molecular systems including ethanol conformations and QM9 molecules—hundreds of times more accurate than classical OFDFT. More significantly, it demonstrates remarkable extrapolation capability with per-atom errors remaining constant or decreasing on molecules up to 224 atoms (10 times larger than training), successfully handling protein systems like chignolin (168 atoms) with substantially better accuracy than classical OFDFT, and exhibiting empirical time complexity of O(N^1.46), substantially lower than KSDFT's O(N^2.49), with a 27.4-fold speedup on large molecules, thereby advancing the accuracy-efficiency frontier in quantum chemistry.

**Technical Terms:**
- **Graphormer**: A graph neural network architecture based on the Transformer model, designed to process graph-structured data by using attention mechanisms to capture interactions between nodes.
- **Attention Mechanism**: A neural network component that computes weighted combinations of features from different locations, where weights are dynamically calculated based on feature content and relationships, enabling long-range interactions.
- **Geometric Invariance**: The physical principle that predicted energies and properties must remain unchanged under rigid transformations (rotation, translation) of the molecular coordinate system.
- **Local Frames**: Coordinate systems attached to local molecular features (like atoms or bonds) that transform with the molecule, used to construct rotationally invariant or equivariant representations.
- **Enhancement Modules**: Specialized neural network components designed to improve gradient expression, including sensitivity balancing, dimension-wise rescaling, and reference offset mechanisms.
- **Per-atom Error**: The prediction error divided by the number of atoms, providing a size-normalized metric for comparing accuracy across molecules of different sizes.
- **Hellmann-Feynman Force**: The force on each atom calculated as the negative gradient of total energy with respect to atomic positions, used in molecular dynamics and geometry optimization.

### Question 7. What is the difference between the proposed and conventional methods?

The proposed M-OFDFT differs from conventional classical OFDFT methods primarily in using a data-driven deep learning model to approximate the kinetic energy density functional rather than theory-based analytical formulas derived from uniform electron gas assumptions. While classical KEDFs like Thomas-Fermi, von Weizsäcker, and gradient expansion approaches are constrained by their theoretical foundations to handle only slowly-varying densities, M-OFDFT learns complex functional relationships from labeled data generated by accurate Kohn-Sham DFT calculations, enabling it to compensate for theoretical mismatches and capture the highly non-uniform density distributions in molecules. M-OFDFT differs from previous machine learning KEDF approaches through its atomic basis representation of electron density: instead of using grid-based representations requiring approximately 10⁴N points, M-OFDFT represents density with only about 10N expansion coefficients under atomic basis functions, reducing dimensionality by three orders of magnitude and making non-local calculations computationally feasible. This contrasts with prior works that either used semi-local approaches ignoring essential non-local effects or faced prohibitive computational costs with grid-based non-local methods, limiting them to systems with dozens of atoms. Compared to neural network potentials (NNPs) that predict ground-state energy end-to-end from molecular structure, M-OFDFT formulates the learning task as approximating the objective function for density optimization rather than directly predicting the final energy. This fundamental difference means M-OFDFT must learn the mechanism of electron interactions (lower complexity) while transferring much complexity to the optimization process, whereas NNPs must learn the complete many-body result (higher complexity), leading to M-OFDFT's superior extrapolation capability. M-OFDFT also differs in incorporating specialized techniques absent from conventional approaches: generating multiple density training datapoints with gradients per structure, employing local frames for geometric invariance, and using enhancement modules for expressing steep physical gradients. Finally, while conventional methods either prioritize accuracy (Kohn-Sham DFT with O(N³) scaling) or efficiency (classical OFDFT with poor accuracy), M-OFDFT achieves both chemical accuracy comparable to Kohn-Sham DFT and favorable O(N^1.46) empirical scaling.

**Technical Terms:**
- **Theory-based Analytical Formulas**: Mathematical expressions for KEDF derived from theoretical principles (like uniform electron gas) rather than learned from data, typically involving density and its derivatives.
- **Data-driven Learning**: An approach where the functional form is learned from training examples (input-output pairs) rather than derived from theoretical principles, enabling adaptation to complex patterns.
- **Density Fitting**: A technique in quantum chemistry that approximates four-center electron repulsion integrals using auxiliary basis functions to reduce computational cost from O(N⁴) to O(N³).
- **Semi-local Approaches**: Methods where the predicted quantity at a point depends only on density values in a small neighborhood, ignoring long-range interactions.
- **End-to-end Prediction**: Machine learning paradigm where the model directly maps input (molecular structure) to final output (energy) without intermediate optimization steps.
- **Objective Function**: The function to be minimized during optimization, which in M-OFDFT is the total electronic energy as a functional of density coefficients.
- **Training Datapoints**: Individual examples used to train the machine learning model, consisting of input features (density coefficients, molecular structure) and target labels (energies, gradients).

### Question 8. Explain why the difference should be introduced.

The key differences in M-OFDFT should be introduced because they address fundamental limitations that have prevented OFDFT from achieving practical accuracy and applicability to molecular systems. The atomic basis representation is essential because grid-based representations create an intractable accuracy-efficiency trade-off: molecular electron density features sharp nuclear cusps and covalent bonding regions requiring fine grids with approximately 10⁴N points for adequate resolution, but non-local KEDF calculations with such high-dimensional inputs become computationally prohibitive, eliminating the efficiency advantage that motivates OFDFT. Atomic basis functions, designed to mimic nuclear cusp conditions and naturally forming shell structures, efficiently capture molecular density patterns with only about 10N coefficients, making non-local calculations affordable while providing a representation aligned with the physical reality of how electron density organizes around atoms. The data-driven deep learning approach succeeds where classical theory fails because molecular electron density is fundamentally far from uniform, violating the assumptions underlying analytical KEDFs derived from uniform electron gas theory. By learning from accurate Kohn-Sham DFT calculations, the model compensates for theoretical mismatches and captures complex density-energy relationships that cannot be expressed in closed analytical form. Learning the objective function rather than end-to-end energy prediction is crucial for extrapolation capability: the ground-state energy emerges from intricate many-body interactions creating a highly complex function difficult to extrapolate across molecular sizes, but the underlying interaction mechanism captured by the objective function has lower complexity and better generalization properties, with optimization tools handling the remaining complexity without extrapolation issues. This formulation enables M-OFDFT to maintain constant or decreasing per-atom errors on molecules 10 times larger than training data, whereas end-to-end methods show increasing errors. The specialized techniques for generating multiple density datapoints with gradients, ensuring geometric invariance, and enhancing gradient expression are necessary because standard supervised learning approaches cannot handle the unique requirements of learning an optimization objective function that must accurately represent energy landscapes, respect physical symmetries, and exhibit steep gradients where mechanisms vary rapidly—challenges absent in conventional machine learning applications.

**Technical Terms:**
- **Accuracy-Efficiency Trade-off**: The fundamental tension in computational methods where improving accuracy typically increases computational cost, requiring careful balance for practical applicability.
- **Nuclear Cusp Condition**: A mathematical constraint describing the sharp behavior of electron density near nuclei as ∂ρ/∂r|_{r→0} ∝ -Zρ(0), which basis functions are designed to satisfy.
- **Closed Analytical Form**: An explicit mathematical formula that can be written down and evaluated directly, as opposed to implicit definitions or iterative procedures.
- **Generalization Properties**: The ability of a learned model to perform accurately on new data different from training examples, particularly important for extrapolation to different scales.
- **Energy Landscape**: The hypersurface in density coefficient space representing how electronic energy varies with all possible density configurations, which the optimizer must navigate.
- **Physical Symmetries**: Fundamental invariances in physical laws, such as energy being unchanged by rotating or translating the molecular coordinate system, which predictions must respect.
- **Optimization Objective Function**: The function being minimized (or maximized) during an optimization process, which defines the problem being solved.

### Question 9. What is the novelty of the proposed method?

The novelty of the proposed M-OFDFT method encompasses several key innovations:

- **Novel density representation for OFDFT**: First use of atomic basis expansion coefficients as input for a machine learning KEDF model, reducing dimensionality from ~10⁴N (grid-based) to ~10N while naturally aligning with molecular electron density patterns around atoms and enabling affordable non-local calculations.

- **Deep learning architecture for KEDF**: Application of Graphormer (Transformer-based) architecture with attention mechanisms to capture essential non-local electron density interactions in molecular systems, representing the first successful deep learning KEDF model achieving chemical accuracy on diverse molecular systems.

- **Learning formulation as objective function**: Novel formulation of the machine learning task as approximating the optimization objective for density rather than end-to-end energy prediction, which significantly improves extrapolation capability by reducing complexity and transferring computational burden to optimization.

- **Specialized training data generation**: Introduction of methods to generate multiple density configurations with gradient labels for each molecular structure, addressing the unique challenge that a single ground-state datapoint per structure is insufficient for learning an optimization objective.

- **Geometric invariance techniques**: Development of local frame approaches to guarantee that the model respects physical symmetries—producing invariant energy outputs from equivariant density coefficient inputs under molecular rotation—which is non-trivial with coefficient representations.

- **Gradient enhancement modules**: Novel neural network components including sensitivity balancing, dimension-wise gradient rescaling, and reference offset mechanisms specifically designed to enable the model to express the steep gradients required by rapidly-varying physical mechanisms.

- **Demonstration of practical OFDFT for molecules**: First demonstration of an OFDFT method achieving chemical accuracy on common molecular systems including conformational spaces (ethanol) and diverse chemical spaces (QM9), recovering accurate shell structures previously deemed challenging for orbital-free approaches.

- **Remarkable extrapolation capability**: Achievement of constant or decreasing per-atom errors on molecules up to 10 times larger (224 atoms) than training data, successfully handling protein systems (chignolin, 168 atoms) with high accuracy, representing unprecedented extrapolation for quantum chemistry machine learning.

- **Computational efficiency demonstration**: Empirical validation of O(N^1.46) time complexity substantially lower than Kohn-Sham DFT's O(N^2.49), with 27.4-fold speedup on large molecules, concretely demonstrating the accuracy-efficiency frontier advancement for practical large-scale molecular applications.

**Technical Terms:**
- **Chemical Accuracy**: Energy prediction errors below ~1 kcal/mol, sufficient for reliable chemical property prediction and reaction energetics.
- **Conformational Space**: The set of different three-dimensional molecular geometries (conformations) accessible through rotation around chemical bonds without breaking bonds.
- **Chemical Space**: The space of different molecular compositions and bonding patterns, representing chemically distinct molecules rather than conformations of the same molecule.
- **Shell Structure**: The organization of electron density into concentric regions around atomic nuclei corresponding to quantum mechanical shells (K, L, M, etc.).
- **Time Complexity**: Mathematical characterization of how computational time scales with system size N, expressed in big-O notation like O(N^α).
- **Frontier Advancement**: Progress in Pareto-optimal trade-offs where both competing objectives (accuracy and efficiency) are simultaneously improved beyond previous best-known solutions.

### Question 10. Show the eye-catch figure.

The eye-catch figure for M-OFDFT is Figure 1 in the paper, which provides a comprehensive three-panel overview of the method. This figure cannot be directly reproduced here as it is a complex multi-part illustration from the published paper. However, I can describe its content and significance:

**Figure 1 Content Description:**

*Panel (a)* compares KSDFT and OFDFT formulations: Shows KSDFT optimizing N orbital functions {φᵢ(r)} with O(N³) complexity versus OFDFT optimizing one density function ρ(r) with O(N²) complexity, visually illustrating the computational scaling advantage when a good KEDF T_S[ρ] is available.

*Panel (b)* illustrates the M-OFDFT architecture: Depicts how electron density is represented using atomic basis expansion coefficients p distributed over atoms (shown with a molecular structure of ethanol with spherical basis functions), how these coefficients along with molecular structure M are processed through the Graphormer deep learning model with attention mechanisms (shown with connecting lines between atoms), and how the model outputs the kinetic energy value. The attention mechanism's non-local nature is visualized with lines connecting distant atoms.

*Panel (c)* shows the density optimization workflow: Illustrates how M-OFDFT solves a molecular system by iteratively optimizing density coefficients p^(k) → p^(k+1) using gradients ∇_p E_θ to minimize the total electronic energy E_θ(p,M), with a visual representation of the energy landscape (shown in red-blue hues) that the optimization navigates to find the ground-state density ρ* and energy E*, from which properties like forces f can be derived.

The figure effectively communicates the key innovation: representing density with atomic basis coefficients enables a deep learning model with non-local interactions to approximate KEDF, which then serves as the objective for optimization to solve molecular electronic structure with both accuracy and efficiency. This visual summary captures the entire M-OFDFT workflow from input representation through model architecture to the optimization process for solving molecular systems.

**Technical Terms:**
- **Eye-catch Figure**: The main overview figure in a scientific paper that visually summarizes the key concept, method, or workflow to quickly communicate the core contribution.
- **Multi-panel Figure**: A figure composed of multiple sub-figures (panels) labeled (a), (b), (c), etc., each showing different aspects or steps of the overall concept.
- **Visual Representation**: Graphical depiction using shapes, colors, arrows, and other visual elements to communicate complex concepts more intuitively than text alone.
- **Workflow Diagram**: A schematic illustration showing the sequence of steps or processes in a method, often with arrows indicating the flow of data or operations.
- **Energy Landscape Visualization**: Graphical representation of how energy varies across a parameter space, often using color gradients (like red-blue hues) to indicate high and low energy regions.

---

## 2. Related Work (0/5 completed)

### Question 11. Explain about multiple survey papers in the related area.

Several comprehensive survey papers provide important context for understanding orbital-free density functional theory and machine learning approaches to quantum chemistry. Wang and Carter's review "Orbital-free kinetic-energy density functional theory" in Theoretical Methods in Condensed Phase Chemistry (2000) provides foundational coverage of OFDFT methods, discussing classical kinetic energy density functional approximations and their theoretical basis. Karasiev, Chakraborty, and Trickey's chapter "Progress on new approaches to old ideas: Orbital-free density functionals" in Many-electron Approaches in Physics, Chemistry and Mathematics (2014) surveys developments in OFDFT, highlighting ongoing challenges in achieving accuracy for molecular systems and discussing various gradient expansion and nonlocal approaches. More recently, Huang, von Rudorff, and von Lilienfeld's "The central role of density functional theory in the AI age" in Science (2023) surveys the intersection of DFT and artificial intelligence, discussing how machine learning is transforming computational quantum chemistry including OFDFT applications. Teale et al.'s comprehensive review "DFT exchange: sharing perspectives on the workhorse of quantum chemistry and materials science" in Physical Chemistry Chemical Physics (2022) provides extensive coverage of DFT methodology including discussions of orbital-free approaches and their limitations for molecular systems. These surveys collectively establish that while classical OFDFT has succeeded for periodic materials with relatively uniform electron density, molecular systems remain challenging due to highly non-uniform density distributions, and that machine learning presents promising opportunities to overcome these longstanding accuracy barriers by learning complex functional relationships from high-quality data generated by more accurate methods.

**Technical Terms:**
- **Survey Paper/Review Article**: A comprehensive academic article that synthesizes and critically evaluates existing research literature in a particular field, identifying trends, gaps, and future directions.
- **Gradient Expansion**: A mathematical technique for approximating functionals as series involving density and its spatial derivatives (∇ρ, ∇²ρ, etc.) to systematically improve upon local approximations.
- **Nonlocal Approaches**: Methods where the predicted quantity at one spatial point depends on values at distant points, requiring integration or convolution operations over extended regions of space.
- **Uniform Electron Density**: An idealized scenario where electron density is constant throughout space, serving as a theoretical limiting case for which exact solutions exist.
- **Exchange-Correlation Functional**: The component of DFT energy functional E_XC[ρ] accounting for quantum mechanical exchange and classical/quantum correlation effects beyond mean-field approximations.
- **Periodic Materials**: Crystalline solids with repeating lattice structures, allowing efficient computational treatment using Bloch's theorem and plane-wave basis sets.

### Question 12. Explain the first related subfield and several related papers.

The first major related subfield is classical kinetic energy density functional development, which attempts to approximate the non-interacting kinetic energy T_S[ρ] using analytical expressions based on theoretical principles. The Thomas-Fermi (TF) functional (Thomas 1927, Fermi 1928) represents the foundational work, providing the exact KEDF for uniform electron gas as T_TF[ρ] = (3/10)(3π²)^(2/3) ∫ρ^(5/3) dr, which scales with the local density to the 5/3 power. However, this severely underestimates kinetic energy in molecules where density is highly non-uniform. Von Weizsäcker's 1935 work introduced the first gradient correction T_vW[ρ] = (1/8)∫|∇ρ|²/ρ dr, which captures kinetic energy contributions from density variations and is exact for one-electron systems but overcorrects for many-electron systems. Brack, Jennings, and Chu (1976) proposed the TF+1/9 vW combination as a systematic expansion from uniform electron gas, while Karasiev and Trickey (2012) explored TF+vW variants. More sophisticated approaches include Wang and Teter's density-dependent kernel functionals (1992), Wang, Govind, and Carter's nonlocal orbital-free functionals (1999), and Huang and Carter's semiconductor-specific KEDFs (2010). Despite these advances, García-Aldea and Alvarellos (2007) and Xia et al. (2012) demonstrated that classical KEDFs still produce errors orders of magnitude larger than chemical accuracy for molecular systems. Constantin et al.'s asymptotic PBE-like (APBE) functional (2011) represents a recent classical approach using semiclassical neutral atom as reference, which serves as a reasonable base KEDF but still lacks the accuracy needed for predictive molecular calculations without machine learning corrections.

**Technical Terms:**
- **Uniform Electron Gas (UEG)**: An idealized system where electrons move in a constant positive background charge, serving as the theoretical foundation for local density approximations.
- **Gradient Correction**: Functional terms involving spatial derivatives of density (like ∇ρ) that improve upon local approximations by capturing effects of density inhomogeneity.
- **Density-Dependent Kernel**: A functional form where the response at one point to density at another point varies depending on the local density values, enabling more flexible nonlocal approximations.
- **Semiclassical Approach**: Methods combining classical physics concepts with quantum mechanical corrections, often using the WKB approximation or Thomas-Fermi theory as starting points.
- **Direct Inversion in Iterative Subspace (DIIS)**: An acceleration technique for iterative quantum chemistry calculations that extrapolates solutions using information from previous iterations to achieve faster convergence.

### Question 13. Explain the N-th related subfield and several related papers.

The second major related subfield is machine learning approaches to density functional approximation, which use data-driven methods to learn complex functional relationships that analytical expressions cannot capture. Snyder et al. (2012) pioneered this direction with "Finding density functionals with machine learning," using kernel ridge regression to approximate KEDF from labeled data on one-dimensional systems, demonstrating proof-of-concept for learning functionals. Brockherde et al. (2017) extended this in "Bypassing the Kohn-Sham equations with machine learning," showing that machine learning could learn both kinetic and exchange-correlation functionals, though requiring projection onto training manifolds during optimization. Early deep learning approaches include Yao and Parkhill (2016) applying convolutional neural networks to learn kinetic energy of hydrocarbons, and Seino et al. (2018, 2019) developing semi-local machine-learned KEDFs using gradient features of electron density. These semi-local approaches, while computationally efficient, cannot capture essential nonlocal effects. Imoto, Imada, and Oshiyama (2021) achieved order-N orbital-free calculations by machine learning functional derivatives for semiconductors and metals, using self-consistent field schemes. Meyer, Weichselbaum, and Hauser (2020) trained simultaneously on kinetic energy and its functional derivative for improved optimization stability. Remme et al.'s KineticNet (2023) explored deep learning for transferable KEDFs using perturbed external potential data. Beyond KEDF specifically, Dick and Fernandez-Serra (2020) and Chen et al.'s DeePKS (2021) learned exchange-correlation functionals using coefficient representations similar to M-OFDFT's approach. However, previous works either used grid-based representations limiting scalability, focused on semi-local approximations missing nonlocality, or demonstrated limited extrapolation capability to larger molecules—gaps that M-OFDFT addresses through its atomic basis representation and objective function learning formulation.

**Technical Terms:**
- **Kernel Ridge Regression**: A nonparametric machine learning method combining ridge regression (L2 regularization) with kernel tricks to learn functions in high-dimensional feature spaces.
- **Training Manifold**: The subspace or distribution in input space spanned by training examples, outside which the model may produce unreliable predictions (out-of-distribution).
- **Convolutional Neural Networks (CNNs)**: Deep learning architectures using convolutional layers to process grid-like data (images, 3D density grids) by learning local feature patterns with shared weights.
- **Functional Derivative**: The variation δF/δρ(r) of a functional F[ρ] with respect to infinitesimal changes in the density at each point, generalizing the gradient concept to function spaces.
- **Self-Consistent Field (SCF)**: An iterative procedure where the solution depends on itself, requiring repeated updates until convergence (e.g., electrons moving in a mean field they collectively create).
- **Transferability**: The ability of a learned model or functional to generalize accurately across different molecular systems, chemical compositions, or system sizes beyond training examples.

### Question 14. Explain standard datasets in the related fields.

Several standard datasets are widely used for benchmarking machine learning methods in quantum chemistry and molecular property prediction. The QM9 dataset (Ramakrishnan et al. 2014) is one of the most popular benchmarks, containing approximately 134,000 small organic molecules with up to 9 heavy atoms (H, C, N, O, F), with equilibrium geometries and quantum chemical properties calculated at the B3LYP/6-31G(2df,p) level. QM9 molecules are drawn from the GDB-17 chemical universe database (Ruddigkeit et al. 2012) which enumerates 166 billion possible organic molecules, making QM9 representative of diverse chemical space for small molecules. The MD17 dataset (Chmiela et al. 2017, 2019) provides molecular dynamics trajectories for small molecules including ethanol, with configurations sampled from room-temperature MD simulations, offering non-equilibrium conformations for testing generalization across conformational space. The QMugs dataset (Isert et al. 2022) extends to larger molecules with approximately 665,000 biologically and pharmacologically relevant molecules from the ChEMBL database, including structures with up to 100 heavy atoms and multiple conformers per molecule, enabling extrapolation studies to larger molecular systems. For protein systems, the Shaw research group's fast-folding protein trajectories (Lindorff-Larsen et al. 2011) provide extensive MD simulation data for small proteins like chignolin (10 residues), enabling investigation of quantum chemistry methods on biomolecular systems. The M-OFDFT study utilizes all these datasets: QM9 for chemical space generalization, MD17 ethanols for conformational generalization, QMugs for size extrapolation up to 101 heavy atoms, and chignolin fragments for protein system extrapolation. Properties calculated include ground-state energies, forces, electron densities, and dipole moments at the PBE/6-31G(2df,p) KSDFT level to generate training labels.

**Technical Terms:**
- **Equilibrium Geometry**: The molecular configuration corresponding to a local minimum on the potential energy surface, where forces on all atoms are zero (or below threshold).
- **B3LYP**: A popular hybrid exchange-correlation functional combining Becke's 3-parameter exchange with Lee-Yang-Parr correlation, widely used for molecular property calculations.
- **6-31G(2df,p)**: A split-valence basis set specification where core orbitals use 6 Gaussian functions, valence orbitals are split into 3 and 1 functions, with polarization functions (2d and 1f on heavy atoms, 1p on hydrogen).
- **Chemical Universe**: The comprehensive set of all possible molecules satisfying specified constraints (atom types, bonding rules, size limits), representing the full chemical space for exploration.
- **Conformational Space**: The manifold of possible three-dimensional molecular geometries accessible through rotations around bonds and other geometric variations without breaking chemical bonds.
- **Molecular Dynamics (MD) Trajectory**: A time series of molecular configurations generated by integrating equations of motion, sampling conformations according to statistical mechanics at specified temperature and pressure.

### Question 15. What is the difference(s) between the proposed and related methods?

M-OFDFT differs from classical OFDFT methods (Thomas-Fermi, von Weizsäcker, APBE, etc.) fundamentally in its data-driven learning approach rather than relying on analytical expressions derived from uniform electron gas theory. While classical KEDFs are constrained by their theoretical foundations and systematically fail for non-uniform molecular densities with errors exceeding 10-100 kcal/mol, M-OFDFT learns from accurate KSDFT calculations and achieves chemical accuracy (~1 kcal/mol). Compared to previous machine learning KEDF methods like Snyder et al.'s kernel ridge regression or Yao and Parkhill's CNN approaches, M-OFDFT uniquely combines: (1) atomic basis coefficient representation instead of grid-based density, reducing dimensionality by three orders of magnitude from ~10⁴N to ~10N; (2) nonlocal Graphormer architecture enabling long-range density interactions impossible for semi-local methods like Seino et al.'s gradient-based approaches; (3) learning formulation as optimization objective rather than end-to-end prediction, improving extrapolation beyond Meyer et al. and Imoto et al.'s approaches. Unlike Brockherde et al. who require manifold projection at each optimization step, M-OFDFT needs only on-manifold initialization, demonstrating better optimization robustness. Compared to neural network potentials like SchNet, ANI, or Equiformer that predict energy end-to-end without modeling electronic structure, M-OFDFT explicitly optimizes electron density to describe electronic states while exhibiting superior extrapolation—per-atom errors decrease for M-OFDFT on larger molecules versus increasing errors for NNPs. M-OFDFT differs from exchange-correlation learning methods like Dick and Fernandez-Serra's or Chen et al.'s DeePKS by focusing specifically on kinetic energy functional with specialized techniques for steep gradient expression and multiple-datapoint training. The key novelty is achieving simultaneously: chemical accuracy comparable to KSDFT, O(N^1.46) empirical scaling substantially better than KSDFT's O(N^2.49), remarkable extrapolation to 10× larger molecules, and successful application to protein-scale systems (168 atoms)—a combination no previous method has demonstrated.

**Technical Terms:**
- **Analytical Expression**: A closed-form mathematical formula that can be explicitly written and directly evaluated, as opposed to iterative or numerical approximations.
- **Learning Formulation**: The specific way a machine learning problem is framed, including what is predicted (target), what information is provided (input), and how success is measured (loss function).
- **Manifold Projection**: Mathematical operation that maps a point in high-dimensional space onto a lower-dimensional subspace or curved surface (manifold) satisfying certain constraints.
- **Electronic Structure**: The complete description of electron distribution and quantum states in a molecular system, including density, orbitals, energy levels, and wavefunctions.
- **End-to-end Prediction**: Machine learning paradigm where the model directly maps raw inputs to final outputs without intermediate steps or explicit modeling of underlying mechanisms.
- **Empirical Scaling**: The relationship between computational cost and system size observed in practice through timing measurements, which may differ from theoretical worst-case complexity.

---

## 3. Problem Statement (0/8 completed)

### Question 16. What is the target problem?

The target problem addressed in this research is to develop an accurate and computationally efficient method for calculating electronic structure and properties of molecular systems using orbital-free density functional theory, which requires approximating the kinetic energy density functional (KEDF) T_S[ρ] as a function of electron density alone. The fundamental challenge is that for molecular systems with highly non-uniform electron density—featuring sharp nuclear cusps, shell structures, and covalent bonding regions—classical KEDF approximations based on uniform electron gas theory fail to achieve chemical accuracy (1 kcal/mol), producing errors that are orders of magnitude larger. The problem is further complicated by the essential requirement of capturing nonlocal interactions where kinetic energy at any point depends on electron density at distant spatial locations, which is computationally prohibitive with traditional grid-based density representations requiring approximately 10⁴N points for N-electron systems. Additionally, the learned KEDF model must serve as an optimization objective function for density, not just a direct prediction function, requiring it to accurately capture energy landscapes across the space of possible density configurations for each molecular structure. The model must also respect physical symmetries—remaining invariant under molecular rotation while handling equivariant input representations—and must extrapolate effectively to molecules substantially larger than training examples to exploit OFDFT's computational scaling advantage. Finally, the method must demonstrate practical utility by solving ground-state properties for large-scale molecular systems including proteins with hundreds of atoms, achieving both accuracy comparable to Kohn-Sham DFT and empirical time complexity substantially lower than KSDFT's O(N³) scaling, thereby advancing the accuracy-efficiency trade-off frontier for quantum chemistry calculations on complex molecular systems.

**Technical Terms:**
- **Electronic Structure Calculation**: Computational determination of electron distribution, quantum states, energies, and properties of molecular or material systems from first principles.
- **Chemical Accuracy**: Error threshold of approximately 1 kcal/mol (1.6 milli-Hartrees or 0.043 eV) considered sufficient for reliable prediction of chemical phenomena and reaction energetics.
- **Nuclear Cusp**: The sharp singularity in electron density and wavefunction slope at nuclear positions required by quantum mechanics to balance kinetic and potential energies.
- **Covalent Bonding Region**: Spatial region between atoms where electron density accumulates due to bond formation, characterized by sharing of electrons between atomic centers.
- **Energy Landscape**: The multidimensional hypersurface representing total energy as a function of all degrees of freedom (density coefficients), which optimization must navigate to find ground state.
- **Ground-state Properties**: Characteristics of the lowest-energy quantum state including electron density, total energy, forces on atoms, and derived properties like dipole moments.

### Question 17. What is the expected behavior of the system?

The expected behavior of an ideal orbital-free DFT system is to accurately predict ground-state electronic properties of molecular systems by optimizing electron density to minimize total electronic energy, achieving accuracy comparable to Kohn-Sham DFT while maintaining computational complexity scaling as O(N²) or better rather than KSDFT's O(N³). Specifically, the system should correctly predict molecular energies with mean absolute errors below the chemical accuracy threshold of 1 kcal/mol, and forces on atoms (Hellmann-Feynman forces) with errors below approximately 3 kcal/mol/Å, enabling reliable geometry optimization and molecular dynamics simulations. The optimized electron density should exhibit physically correct features including well-defined shell structures around atomic nuclei with appropriate core and valence electron populations, accurate representation of covalent bonding density between atoms, correct nuclear cusp behavior with density exhibiting proper singularities at nuclear positions, and appropriate long-range decay characteristics in outer molecular regions. The density optimization process should converge reliably from reasonable initializations to the ground state without requiring manifold projection at each step, demonstrating numerical stability comparable to conventional KSDFT calculations. Critically, the method should extrapolate effectively to molecular systems substantially larger than training examples—maintaining constant or decreasing per-atom errors on molecules 5-10 times larger—enabling application to protein-scale systems with hundreds of atoms where the computational efficiency advantage becomes meaningful. The system should respect fundamental physical symmetries with predictions invariant under translation and rotation of the molecular coordinate system. Finally, it should provide accurate potential energy surfaces showing correct relative energies, energy barriers, and equilibrium geometries across conformational changes, supporting applications in chemical reaction modeling and molecular conformational analysis.

**Technical Terms:**
- **Hellmann-Feynman Forces**: Forces on atomic nuclei calculated as F_a = -∇_{x_a} E, the negative gradient of total energy with respect to atomic position, following the Hellmann-Feynman theorem.
- **Geometry Optimization**: Computational procedure to find molecular configurations corresponding to local energy minima by iteratively adjusting atomic positions following forces until convergence.
- **Shell Structure**: The organization of electron density into concentric regions around nuclei corresponding to quantum mechanical shells (K, L, M, etc.) and subshells (s, p, d, f).
- **Core Electrons**: Inner-shell electrons tightly bound to nuclei, less involved in chemical bonding, typically well-localized with high density near nuclear positions.
- **Valence Electrons**: Outer-shell electrons involved in chemical bonding and determining chemical properties, with more delocalized distributions extending between atoms.
- **Long-range Decay**: The asymptotic behavior of electron density and wavefunctions at large distances from the molecule, typically decaying exponentially as exp(-√(2I)r) where I is ionization energy.
- **Potential Energy Surface (PES)**: The function E(R) giving total energy as a function of all nuclear positions R, defining the landscape for molecular geometry and dynamics.

### Question 18. Explain a typical sample with a figure.

A typical sample in this research consists of a molecular structure defined by atomic coordinates and atomic numbers, along with its corresponding electron density represented as expansion coefficients under an atomic basis set, ground-state electronic energy, and gradient of energy with respect to density coefficients. For example, an ethanol molecule (C₂H₆O) with 9 atoms and 26 electrons serves as a representative small organic molecule. The molecular structure M = {(x^(a), Z^(a))}_{a=1}^9 specifies the 3D coordinates x^(a) and atomic number Z^(a) (C=6, O=8, H=1) for each atom. The electron density ρ(r) is expanded using atomic basis functions as ρ(r) = Σ_{μ} p_μ ω_μ(r), where the basis functions ω_μ(r) are centered on atoms and follow various angular patterns (s, p, d orbitals), requiring typically around 260 basis functions (approximately 10N for N=26 electrons). While the original paper contains Figure 1(b) showing this representation visually, the key features include: spherical basis functions of different sizes centered on each atom (illustrated with colored spheres around atoms), the density coefficients p distributed correspondingly over atoms forming the input features, the molecular graph structure with atoms as nodes, and attention connections between distant atoms showing nonlocal interactions. The training data for each molecular structure includes multiple density configurations (p^(d,k)) from intermediate SCF iterations rather than just the ground-state density, each labeled with kinetic energy T_S^(d,k) and gradient ∇_p T_S^(d,k), providing information about the energy landscape. For ethanol conformations, relative energies with respect to equilibrium configuration and Hellmann-Feynman forces on atoms serve as validation targets, testing whether M-OFDFT captures conformational energy differences and provides accurate forces for molecular dynamics.

**Technical Terms:**
- **Atomic Coordinates**: The (x,y,z) Cartesian positions of each atom in a molecule, typically measured in Angstroms (Å) or Bohr radii (a₀).
- **Atomic Number**: The integer Z representing the number of protons in an atomic nucleus, determining the element identity and nuclear charge.
- **Expansion Coefficients**: The numerical weights p_μ in a linear combination ρ(r) = Σ_μ p_μ ω_μ(r) representing how much each basis function contributes to the total density.
- **Angular Patterns**: The directional characteristics of atomic orbitals (s=spherical, p=dumbbell-shaped, d=four-lobed, etc.) determining how basis functions are oriented in space.
- **SCF Iterations**: Sequential steps in self-consistent field calculations where electron density and effective potential are iteratively updated until convergence to self-consistency.
- **Molecular Graph**: A representation of molecular structure as a graph where atoms are nodes and chemical bonds or spatial proximity define edges for processing by graph neural networks.
- **Relative Energy**: Energy difference between a molecular conformation and a reference configuration (e.g., equilibrium), removing systematic energy offsets to focus on conformational preferences.

### Question 19. What are the inputs of the task?

The inputs for the M-OFDFT task consist of two primary components that together define the computational problem for a given molecular system. First, the molecular structure M = {(x^(a), Z^(a))}_{a=1}^A specifies the atomic composition and geometry, where A is the number of atoms, x^(a) ∈ ℝ³ represents the Cartesian coordinates of the a-th atom, and Z^(a) ∈ {1, 2, ..., 118} denotes its atomic number indicating the element type. This structural information is required to define the external potential from nuclei, determine basis function locations and types, and specify the molecular system being solved. Second, during the density optimization process, the current electron density represented by expansion coefficients p ∈ ℝ^M serves as an optimization variable input, where M is the total number of atomic basis functions (typically about 10N for N electrons). These coefficients are subject to the normalization constraint p^T w = N ensuring the integrated density equals the number of electrons, where w is the basis normalization vector with components w_μ = ∫ω_μ(r) dr. For training the KEDF model, additional inputs include multiple density coefficient datapoints {p^(d,k)}_k for each molecular structure d, obtained from intermediate steps k of SCF iterations in KSDFT calculations, along with corresponding labels: kinetic energy values T_S^(d,k) and projected gradients of kinetic energy with respect to coefficients. The molecular structure input allows the model to specify basis function locations, compute inter-atomic distances for attention mechanisms, and account for different atomic types through element-specific parameters. The coefficient input is processed through local frame transformations that align basis orientations with local molecular geometry, natural reparameterization to balance sensitivity across dimensions, and dimension-wise rescaling to manage the vast range of gradient magnitudes, ultimately producing rotationally invariant features suitable for the Graphormer neural network to process.

**Technical Terms:**
- **Cartesian Coordinates**: The (x,y,z) position of a point in three-dimensional space measured along perpendicular axes, commonly used in molecular geometry specification.
- **External Potential**: The potential energy field V_ext(r) experienced by electrons due to attractive Coulomb interaction with positively charged atomic nuclei.
- **Optimization Variable**: A quantity that is adjusted during an optimization procedure to minimize (or maximize) an objective function, here the density coefficients p.
- **Normalization Constraint**: A condition ensuring that integrated electron density equals the total number of electrons: ∫ρ(r) dr = Σ_μ p_μ w_μ = N.
- **Local Frame Transformation**: A coordinate system rotation specific to each atom defined by nearby atoms, used to construct rotationally invariant density coefficient features.
- **Natural Reparameterization**: A change of variables ˜p = M^T p where MM^T = W (overlap matrix) that equalizes the metric contribution across coefficient dimensions.
- **Attention Mechanism**: Neural network component computing weighted combinations of features from different nodes, with weights determined by learned relevance scores based on content and distances.

### Question 20. What kind of outputs are expected for the task?

The primary output expected from M-OFDFT is the optimized ground-state electron density for a given molecular structure, from which various molecular properties can be derived. Specifically, the optimized density coefficients p* that minimize the electronic energy functional E_θ(p,M) provide the ground-state density ρ*(r) = Σ_μ p_μ* ω_μ(r), representing the probability distribution of electrons in three-dimensional space. From this density, several key molecular properties are calculated as outputs. The ground-state electronic energy E* = E_θ(p*,M) gives the total energy of the electronic system including kinetic, Hartree (classical electron-electron repulsion), exchange-correlation, and external (electron-nuclear attraction) energy contributions. Hellmann-Feynman forces on atoms f^(a) = -∇_{x^(a)} E* are computed by differentiating the ground-state energy with respect to atomic positions, providing the forces needed for geometry optimization and molecular dynamics simulations. The electron density itself provides rich information: integrated quantities like Hirshfeld partial atomic charges quantifying electron transfer between atoms, dipole moments measuring charge separation, and density visualizations showing shell structures, bonding regions, and lone pairs. For potential energy surface characterization, outputs include relative energies between different conformations, energy barriers for conformational transitions, and equilibrium bond lengths and angles. The optimization trajectory also provides intermediate outputs: the sequence of density coefficients {p^(k)} and energies {E_θ(p^(k),M)} showing convergence behavior. Numerical accuracy metrics comparing to KSDFT reference include mean absolute errors in energy (target <1 kcal/mol), forces (target <3 kcal/mol/Å), density (measured by Hirshfeld charge errors <0.01 e), and dipole moments (target <0.02 D). For large molecules, computational cost outputs include wall-clock time and scaling exponents characterizing efficiency.

**Technical Terms:**
- **Ground-state Density**: The electron density ρ*(r) corresponding to the lowest-energy quantum state, obtained by minimizing the energy functional.
- **Hartree Energy**: The classical electrostatic repulsion energy E_H[ρ] = (1/2)∫∫ ρ(r)ρ(r')/|r-r'| dr dr' between electrons treated as classical charge distributions.
- **Exchange-Correlation Energy**: The quantum mechanical correction E_XC[ρ] accounting for Pauli exclusion principle (exchange) and correlated electron motion beyond mean-field.
- **Hirshfeld Partial Charges**: Atomic charge assignments q_a obtained by partitioning molecular electron density according to reference atomic densities, measuring charge transfer.
- **Dipole Moment**: A vector μ = ∫r ρ(r) dr - Σ_a Z_a x^(a) measuring charge separation, determining how molecules interact with electric fields, measured in Debye (D).
- **Optimization Trajectory**: The sequence of iterative updates p^(0) → p^(1) → ... → p* during gradient descent minimization, showing the path to convergence.
- **Wall-clock Time**: The actual elapsed time for a calculation measured by real-world clocks, as opposed to CPU time which may differ for parallel computations.

### Question 21. Define the terms used in the paper.

The paper employs numerous specialized terms from quantum chemistry, density functional theory, and machine learning. Key quantum chemistry terms include: **Kohn-Sham DFT (KSDFT)** - the prevailing DFT formulation optimizing N orbital functions {φ_i(r)} for N electrons; **Orbital-Free DFT (OFDFT)** - an alternative formulation optimizing only the electron density ρ(r); **Kinetic Energy Density Functional (KEDF)** - the functional T_S[ρ] approximating non-interacting kinetic energy from density alone; **Electron Density ρ(r)** - the probability distribution of electrons in space; **Atomic Basis Set** - functions {ω_μ(r)} centered on atoms used to expand density; **Expansion Coefficients p** - weights in the representation ρ(r) = Σ_μ p_μ ω_μ(r); **Molecular Structure M** - atomic coordinates and types {(x^(a), Z^(a))}; **Chemical Accuracy** - error threshold ~1 kcal/mol; **Hellmann-Feynman Force** - atomic force f_a = -∇_{x_a} E; **Exchange-Correlation (XC) Functional** - E_XC[ρ] accounting for quantum effects beyond classical electrostatics. Machine learning terms include: **Graphormer** - a Transformer-based graph neural network architecture; **Attention Mechanism** - neural network component computing weighted feature combinations; **Training Data** - labeled examples {M^(d), {p^(d,k), T_S^(d,k), ∇_p T_S^(d,k)}_k}_d; **Gradient Label** - the derivative ∇_p T_S providing landscape information; **Local Frame** - atom-centered coordinate systems for rotational invariance; **Neural Network Potentials (NNPs)** - models predicting energy end-to-end from structure. Functional analysis terms include: **Self-Consistent Field (SCF)** - iterative procedure for solving Kohn-Sham equations; **Density Fitting** - approximating density from orbitals using auxiliary basis; **Even-tempered Basis** - basis sets with systematically scaled exponents; **Natural Reparameterization** - change of variables equalizing metric; **Enhancement Modules** - neural components for gradient expression; **Objective Function Learning** - training models to serve as optimization objectives rather than direct predictors.

**Technical Terms:**
- **Orbital Function φ_i(r)**: A single-electron wavefunction in Kohn-Sham DFT, where the total wavefunction is a Slater determinant of N orbitals for N electrons.
- **Non-interacting Kinetic Energy**: The kinetic energy T_S[ρ] of a fictitious system of non-interacting fermions having the same density as the real interacting system.
- **Auxiliary Basis**: An additional basis set used in density fitting to expand products of orbital basis functions, typically larger and more flexible than the orbital basis.
- **Slater Determinant**: An antisymmetric wavefunction constructed from orbitals that automatically satisfies the Pauli exclusion principle for fermions (electrons).
- **Pauli Exclusion Principle**: Quantum mechanical principle stating that no two fermions can occupy the same quantum state simultaneously, enforced by antisymmetric wavefunctions.
- **Transformer Architecture**: A neural network design using attention mechanisms to process sequential or graph-structured data, originally developed for natural language processing.
- **Rotational Invariance**: Property that predictions remain unchanged when the molecular coordinate system is rotated, reflecting physical symmetry of energy under rotation.

### Question 22. What is the assumption in the paper?

The paper operates under several key assumptions and scope limitations that define the problem domain and methodology. The primary assumption is that accurate kinetic energy density functional models can be learned from data generated by Kohn-Sham DFT calculations at the PBE/6-31G(2df,p) level, which serves as the ground truth for training despite being itself an approximation to exact quantum mechanics. The study focuses on neutral, closed-shell molecular systems without spin polarization, assuming restricted-spin wavefunctions where all electrons are paired—though the methodology could potentially be extended to charged and open-shell systems as noted in Discussion. Molecules are assumed to be in near-equilibrium or physically reasonable conformations, excluding extremely high-energy distorted geometries where the SCF iterations may not converge reliably. The research considers only isolated gas-phase molecules without explicit solvent or periodic boundary conditions, though the authors note that atomic basis representation could support material systems. The atomic basis set representation assumes that electron density around atoms can be effectively captured by radially structured functions centered on nuclei, an assumption well-validated for molecular systems but that might need modification for diffuse electronic states or highly delocalized systems. Training data generation assumes that intermediate SCF iteration steps provide valid density-energy pairs for learning the energy landscape, relying on the mathematical property that each SCF step solves a non-interacting fermion problem with a well-defined kinetic energy. The extrapolation studies assume that training on smaller molecules and peptide fragments provides sufficient information to generalize to larger systems through proper learning formulation. The computational complexity analysis assumes modern hardware with CPUs for KSDFT and GPU acceleration for M-OFDFT neural network evaluation. Finally, the study assumes that the chosen Graphormer architecture with attention mechanisms provides sufficient expressiveness to capture nonlocal density-energy relationships, without exhaustively exploring alternative neural network architectures.

**Technical Terms:**
- **PBE Functional**: The Perdew-Burke-Ernzerhof generalized gradient approximation (GGA) exchange-correlation functional, widely used for its balance of accuracy and computational efficiency.
- **Closed-shell System**: A molecular system where all electrons are paired in doubly-occupied orbitals with opposite spins, having zero total spin angular momentum.
- **Open-shell System**: A molecular system with unpaired electrons (radicals, transition metals in certain oxidation states), requiring spin-polarized DFT treatment.
- **Spin Polarization**: The difference between spin-up and spin-down electron densities ρ_α(r) - ρ_β(r), arising in systems with unpaired electrons or magnetic ordering.
- **Gas-phase**: Isolated molecules without surrounding solvent or crystal environment, corresponding to ultra-high vacuum conditions or low-density gas.
- **Periodic Boundary Conditions**: Computational technique treating infinite extended systems by repeating a unit cell, commonly used for crystalline materials.
- **Diffuse Electronic States**: Quantum states with electron density extending far from nuclei, important for anions, Rydberg states, and weak intermolecular interactions.

### Question 23. Which metric is used?

The paper employs multiple complementary metrics to comprehensively evaluate M-OFDFT performance across different aspects. The primary energy metric is Mean Absolute Error (MAE) in absolute energy: MAE_E = (1/N_test) Σ_i |E_pred^(i) - E_ref^(i)|, measuring average prediction error in kcal/mol with the threshold of chemical accuracy at 1 kcal/mol. For extrapolation studies, per-atom energy MAE is used: MAE_{E/atom} = (1/N_test) Σ_i |E_pred^(i) - E_ref^(i)|/N_atoms^(i), normalizing by molecular size to compare fairly across different scales. For conformational analysis on ethanol and C₇H₁₀O₂ isomers, relative energy MAE measures accuracy in energy differences: MAE_{ΔE} = (1/N_pairs) Σ_{i,j} |(E_pred^(i) - E_pred^(j)) - (E_ref^(i) - E_ref^(j))|, which is more relevant for chemical applications than absolute energies. The Hellmann-Feynman force MAE evaluates gradient accuracy: MAE_F = (1/(N_test · A)) Σ_i Σ_a ||f_pred^(i,a) - f_ref^(i,a)||, averaging over all atoms in all test structures with typical threshold around 3 kcal/mol/Å. For electron density quality, Hirshfeld partial charge MAE measures: MAE_q = (1/(N_test · A)) Σ_i Σ_a |q_pred^(i,a) - q_ref^(i,a)| with good performance below 0.01 e. Dipole moment MAE: MAE_μ = (1/N_test) Σ_i ||μ_pred^(i) - μ_ref^(i)|| measures molecular polarity prediction in Debye units. Computational efficiency is evaluated using wall-clock time in seconds and empirical scaling characterized by fitting T(N) = (aN + b)^c + d where N is the number of electrons or heavy atoms. Statistical significance is assessed using 95% confidence intervals computed from bootstrap resampling or assuming Gaussian error distributions. All metrics compare M-OFDFT predictions against KSDFT reference calculations treated as ground truth.

**Technical Terms:**
- **Mean Absolute Error (MAE)**: The average of absolute differences between predicted and reference values, less sensitive to outliers than mean squared error.
- **Per-atom Normalization**: Dividing extensive quantities (that scale with system size) by the number of atoms to enable fair comparison across different molecular sizes.
- **Energy Difference/Relative Energy**: The difference in energy between two configurations ΔE = E₁ - E₂, more physically meaningful than absolute energies which contain arbitrary zero-point offsets.
- **Force Magnitude**: The Euclidean norm ||f|| = √(f_x² + f_y² + f_z²) of the force vector giving the total force strength on an atom.
- **Bootstrap Resampling**: A statistical method for estimating confidence intervals by repeatedly sampling with replacement from the data and computing statistics on each resample.
- **95% Confidence Interval**: A range expected to contain the true population parameter with 95% probability, typically computed as mean ± 1.96 × standard_error.
- **Wall-clock Time**: Actual elapsed time measured from start to finish of a calculation, including all computational overhead and waiting times.
- **Empirical Scaling Exponent**: The power c in T(N) ∝ N^c characterizing how computational cost grows with system size, determined by fitting to measured timing data.

---

## 4. Method (0/11 completed)

### Question 24. Which method do you extend?

M-OFDFT extends orbital-free density functional theory by incorporating a deep learning approximation for the kinetic energy density functional, building upon the Graphormer neural network architecture originally developed for molecular property prediction. The methodological foundation comes from orbital-free DFT formalism established by Hohenberg-Kohn and extended by Wang and Carter, where electronic energy is minimized as a functional of electron density alone: min_ρ E[ρ] = T_S[ρ] + E_H[ρ] + E_XC[ρ] + E_ext[ρ], with the challenge being accurate approximation of the kinetic energy term T_S[ρ]. The deep learning model architecture extends Graphormer (Ying et al. 2021, Shi et al. 2022), a Transformer-based graph neural network using attention mechanisms to process molecular graphs, which was originally designed for end-to-end property prediction from molecular structures. M-OFDFT adapts this architecture to take density coefficients as additional input beyond molecular structure, enabling it to approximate the functional T_S,θ(p,M) mapping density representations to kinetic energy values. The density representation extends the atomic basis expansion approach used in conventional quantum chemistry, where density is written as ρ(r) = Σ_μ p_μ ω_μ(r) with atomic-centered basis functions—a technique standard in Gaussian basis set methods but novel for machine learning KEDF approximation. The training methodology extends supervised learning with crucial innovations: generating multiple density datapoints per structure from SCF iterations (building on insights from Snyder et al. 2012 and Brockherde et al. 2017) and incorporating gradient labels (functional derivatives) into the training loss following the principle that the model must capture optimization landscapes, not just prediction mappings. The geometric invariance approach extends local frame methods from neural network potentials (Han et al. 2018) to handle equivariant density coefficient tensors. The density optimization procedure extends gradient descent optimization from electronic structure methods (Yoshikawa and Sumita 2022) with novel initialization strategies including H¨uckel method and projected MINAO.

**Technical Terms:**
- **Hohenberg-Kohn Theorem**: The fundamental theorems of DFT proving that ground-state properties are uniquely determined by electron density and that ground state minimizes the energy functional.
- **Slater Determinant**: An antisymmetric wavefunction built from single-particle orbitals that satisfies the Pauli exclusion principle for indistinguishable fermions.
- **Transformer**: A neural network architecture using self-attention mechanisms to process sequential or graph data by computing all-to-all interactions between elements.
- **Graph Neural Network (GNN)**: Neural architectures designed to process graph-structured data by iteratively updating node features through message passing with neighbors.
- **Gaussian Basis Set**: A set of basis functions constructed from Gaussian functions exp(-αr²) or their derivatives, commonly used in quantum chemistry for computational efficiency.
- **Functional Derivative**: The variation δF[ρ]/δρ(r) of a functional F[ρ] with respect to density at each point r, generalizing the gradient concept from functions to functionals.
- **H¨uckel Method**: A simplified molecular orbital approximation solving eigenvalue problems with parameterized Hamiltonian matrices, useful for π-electron systems and as an initialization strategy.

### Question 25. Explain that the extensions made in the proposed method are widely applicable to other methods.

The extensions introduced in M-OFDFT possess significant generality and applicability beyond the specific implementation presented. The atomic basis coefficient representation for density input can be applied to any machine learning approach targeting density functionals, including exchange-correlation functional learning (as demonstrated by Dick and Fernandez-Serra 2020, Chen et al.'s DeePKS 2021), beyond just kinetic energy functionals, since any molecular system can be represented using atomic basis expansions regardless of the specific functional being approximated. The methodology of generating multiple density datapoints with gradient labels from SCF iterations is applicable to training any functional model serving as an optimization objective, whether in quantum chemistry (approximating other energy components), molecular dynamics (learning force fields as energy gradients), or optimization problems in other scientific domains where the objective function must be learned from data. The local frame technique for achieving geometric invariance with coefficient representations extends to any equivariant molecular property prediction task where inputs transform non-trivially under rotation, including learning molecular tensors like polarizabilities, quadrupole moments, or stress tensors in materials, providing a general strategy for handling geometric equivariance alternative to global frame averaging or equivariant neural networks. The enhancement modules for expressing steep gradients—including dimension-wise rescaling, natural reparameterization, and atomic reference offsets—address a fundamental challenge in learning optimization landscapes with heterogeneous scales, applicable to learning objectives in diverse domains like molecular conformation generation, protein structure prediction, or material design where different degrees of freedom exhibit vastly different sensitivities. The Graphormer architecture itself, while used here for kinetic energy functional, can serve as a backbone for approximating other nonlocal molecular functionals, electron correlation effects, or properties requiring long-range interaction modeling. The training strategy combining energy value and gradient labels applies generally to any regression problem where gradient information is available and the learned function will be used in gradient-based optimization or where capturing local landscape curvature improves model quality. The formulation of learning an objective function rather than end-to-end prediction represents a general machine learning paradigm shift applicable to inverse problems and optimization-based inference across scientific computing.

**Technical Terms:**
- **Exchange-Correlation Functional E_XC[ρ]**: The component of DFT energy accounting for quantum exchange (Pauli principle) and correlation (beyond mean-field) effects.
- **Force Field**: A potential energy function used in classical molecular dynamics, typically parameterized as sums of bond, angle, torsion, and non-bonded interaction terms.
- **Equivariant Property**: A quantity that transforms predictably under symmetry operations (e.g., vectors rotate with coordinate system), as opposed to invariant scalars.
- **Polarizability**: A molecular property (second-rank tensor) quantifying how electron distribution responds to applied electric fields, determining induced dipole moments.
- **Quadrupole Moment**: A measure of charge distribution's deviation from spherical symmetry beyond dipole, represented as a traceless second-rank tensor.
- **Stress Tensor**: A mechanical property describing internal forces in materials, relating strain to deformation, important in materials mechanics.
- **Inverse Problems**: Computational problems where causes must be inferred from observed effects, often ill-posed and requiring regularization.
- **Optimization-based Inference**: Inference paradigm where predictions are obtained by optimizing an objective function, as opposed to direct function evaluation.

### Question 26. List the differences between the proposed method and the conventional methods.

The key differences between M-OFDFT and conventional methods are:

- **Density Representation**: M-OFDFT uses expansion coefficients (~10N parameters) under atomic basis sets, while classical OFDFT and previous ML-OFDFT methods use grid-based representations (~10⁴N points), reducing dimensionality by three orders of magnitude.

- **KEDF Approximation**: M-OFDFT learns T_S,θ(p,M) from KSDFT data using deep neural networks, while classical OFDFT (Thomas-Fermi, von Weizsäcker, APBE) derives analytical expressions from uniform electron gas theory.

- **Nonlocality**: M-OFDFT captures nonlocal density interactions through Graphormer attention mechanisms operating on compact coefficient representation, while classical methods use local/semi-local approximations and previous nonlocal ML methods face prohibitive costs with grid representations.

- **Learning Formulation**: M-OFDFT learns the optimization objective function E_θ(p,M) with density as optimization variable, while neural network potentials learn end-to-end energy prediction E(M) without density optimization.

- **Training Data**: M-OFDFT uses multiple density configurations {p^(d,k)}_k per structure from SCF iterations with both energy and gradient labels, while conventional ML uses single ground-state points per structure.

- **Geometric Invariance**: M-OFDFT employs local frames to construct rotationally invariant features from equivariant coefficients, while NNPs use distance-based features inherently invariant and equivariant GNNs process coordinates directly.

- **Gradient Enhancement**: M-OFDFT includes specialized modules (dimension-wise rescaling, natural reparameterization, atomic reference) for expressing steep gradients, absent in conventional neural networks.

- **Density Optimization**: M-OFDFT uses gradient descent with on-manifold initialization (H¨uckel or ProjMINAO), while previous ML-OFDFT methods require manifold projection at each optimization step.

- **Computational Complexity**: M-OFDFT achieves empirical O(N^1.46) scaling between classical OFDFT's O(N²) and KSDFT's O(N^2.49-3), with absolute time faster than KSDFT for all tested sizes.

- **Accuracy and Extrapolation**: M-OFDFT achieves chemical accuracy (~1 kcal/mol) with decreasing per-atom errors on 10× larger molecules, while classical OFDFT has ~10-100 kcal/mol errors and NNPs show increasing per-atom errors.

**Technical Terms:**
- **Semi-local Approximation**: Methods where predicted quantities depend only on density and its derivatives at nearby points, ignoring long-range interactions.
- **Attention Mechanism**: Neural network component computing weighted feature combinations based on learned relevance scores, enabling all-to-all node interactions.
- **End-to-end Learning**: Machine learning paradigm directly mapping inputs to final outputs without intermediate optimization or explicit modeling of mechanisms.
- **Manifold Projection**: Mathematical operation mapping points onto a constraint surface or lower-dimensional subspace satisfying physical constraints.
- **Gradient Descent**: Iterative optimization algorithm updating variables in the direction of negative gradient: p^(k+1) = p^(k) - ε∇E(p^(k)).
- **On-manifold Initialization**: Starting density optimization from a physically reasonable density satisfying constraints, without requiring projection during optimization.
- **Scaling Complexity**: How computational cost grows with system size, characterized by exponent in T(N) ∝ N^c relationships.

### Question 27. How many main modules does the proposed model have? Explain each method briefly.

The M-OFDFT model consists of four main interconnected modules that work together to approximate the kinetic energy density functional and enable density optimization:

**Module 1: Density Representation and Preprocessing**
This module transforms raw density coefficients p into model-ready features through three sub-components: (a) Local frame transformation that rotates coefficients into atom-specific coordinate systems aligned with local molecular geometry to achieve rotational invariance; (b) Natural reparameterization ˜p = M^T p that equalizes metric contributions across dimensions by setting MM^T = W (overlap matrix), implementing natural gradient descent; (c) Dimension-wise rescaling p'_a,τ = λ_Z(a),τ p_a,τ that balances coefficient-gradient scales across different atom types Z and basis patterns τ, managing the vast range of physical magnitudes.

**Module 2: Atomic Reference Module**
This linear module T_AtomRef(p,M) = ḡ_M^T p + T̄_M provides a baseline kinetic energy estimate constructed from per-element statistics. The gradient ∇_p T_AtomRef = ḡ_M is constant, offsetting the mean gradient that the neural network must learn, effectively reducing gradient label scales and facilitating training stability.

**Module 3: Graphormer Neural Network**
This is the core learnable component using Transformer architecture with L layers of attention-based feature updates. Each layer computes attention weights between all atom pairs based on their features and distances, updates each atom's features by aggregating information from all other atoms weighted by attention scores, capturing essential nonlocal density interactions. The initial node embeddings encode atom types Z^(a), rescaled density coefficients p'_a, and positional information. After L layers of updates, scalar features are extracted from each node and summed to produce the neural network's kinetic energy contribution.

**Module 4: Energy Assembly and Optimization**
This module combines the neural network output with the atomic reference to give T_S,θ(p,M), then constructs the total electronic energy E_θ(p,M) = T_S,θ(p,M) + E_H(p,M) + E_XC(p,M) + E_ext(p,M) where the last three terms are computed using standard DFT formulas. During deployment, gradient descent optimization iteratively updates p^(k+1) := p^(k) - ε[I - ww^T/(w^T w)]∇_p E_θ(p^(k),M), projecting gradients onto the constraint surface to maintain normalized density until convergence to ground state.

**Technical Terms:**
- **Feature Embedding**: Neural network transformation converting discrete input categories (like atom types) and continuous values into dense vector representations.
- **Layer-wise Updates**: Sequential processing where each network layer transforms representations, with information flowing from input through multiple intermediate layers to output.
- **Node Aggregation**: Operation combining features from multiple nodes (atoms) into a single representation, typically using summation, mean, or attention-weighted combinations.
- **Energy Component**: Individual terms in the energy functional decomposition E = T_S + E_H + E_XC + E_ext, each representing different physical interactions.
- **Constraint Projection**: Mathematical operation ensuring optimization updates maintain constraints, here achieved by subtracting the component parallel to the constraint normal.
- **Convergence**: The property that iterative optimization approaches a stable solution where further iterations produce negligible changes, typically measured by gradient norm or energy change.

### Question 28. Explain the structure of the model.

The M-OFDFT model structure follows a hierarchical architecture processing both molecular structure and density information to predict kinetic energy while respecting physical constraints and symmetries. The input processing begins with molecular structure M = {(x^(a), Z^(a))}_a and density coefficients p, which undergo local frame transformation where each coefficient p_a,τ is rotated into a coordinate system aligned with nearby atoms around atom a, making features rotationally invariant. These transformed coefficients then pass through natural reparameterization ˜p = M^T p that equalizes sensitivities across dimensions. The model splits into two parallel branches: the atomic reference branch T_AtomRef(˜p,M) = Σ_a Σ_τ ḡ_Z(a),τ ˜p_a,τ + Σ_a T̄_Z(a) + T̄_global computes a linear baseline using element-specific statistics; the neural network branch first applies dimension-wise rescaling p' = Λ˜p with factors λ_Z,τ managing gradient ranges, then feeds into the Graphormer architecture. The Graphormer network structure consists of node embeddings that combine atom type embeddings (learned vectors for each element), coefficient features p'_a distributed over atoms, and edge embeddings encoding inter-atomic distances through Gaussian basis expansion. The core architecture comprises L layers (L=12 in implementation) of multi-head attention blocks where each attention head computes attention scores α_ij between atoms i and j using queries, keys, values derived from node features and modified by spatial bias from edge embeddings, enabling long-range nonlocal interactions. Each layer performs h_a^(ℓ+1) = Σ_b α_ab h_b^(ℓ) followed by feed-forward neural networks and residual connections, progressively refining atomic features. The final layer extracts scalar features from each node's embedding and sums them: T_NN(p',M) = Σ_a ReadOut(h_a^(L)), giving the neural network's kinetic energy prediction. The total KEDF model output is T_S,θ(p,M) = T_AtomRef(˜p,M) + T_NN(p',M), combining the linear reference and nonlinear neural components. For complete energy evaluation, three additional terms are computed using established DFT formulas implemented with automatic differentiation in PyTorch: E_H(p,M), E_XC(p,M), and E_ext(p,M).

**Technical Terms:**
- **Multi-head Attention**: Attention mechanism with multiple parallel attention operations (heads) each learning different interaction patterns, whose outputs are concatenated.
- **Query-Key-Value**: The three component vectors in attention computation where attention weights are α ∝ exp(query·key) and outputs are Σ_i α_i value_i.
- **Spatial Bias**: Additional terms in attention score computation depending on geometric information (distances, angles) to inject spatial structure into graph neural networks.
- **Feed-Forward Network**: Multi-layer perceptron applied independently to each node's features, typically with ReLU activation and larger hidden dimension than input/output.
- **Residual Connection**: Neural network skip connection adding layer input to output (h^(ℓ+1) = h^(ℓ) + Transform(h^(ℓ))), improving gradient flow and enabling deeper networks.
- **ReadOut Function**: Final network layer extracting scalar or vector predictions from node embeddings, often a simple linear projection followed by pooling.
- **Automatic Differentiation**: Computational technique automatically computing derivatives of functions specified as programs, enabling gradient calculation through complex operations.

### Question 29. Define the input to the proposed method.

The input to M-OFDFT consists of two primary components that together fully specify the computational task. The molecular structure input M = {(x^(a), Z^(a))}_{a=1}^A provides atomic configuration information where x^(a) ∈ ℝ³ represents the three-dimensional Cartesian coordinates (x,y,z positions in Angstroms or Bohr) of the a-th atom, and Z^(a) ∈ {1,2,...,118} denotes its atomic number (1 for hydrogen, 6 for carbon, 8 for oxygen, etc.) indicating the element type. This structural information determines: the number and types of atoms A, the geometric arrangement defining inter-atomic distances and molecular shape, the external potential V_ext(r) = -Σ_a Z^(a)/|r-x^(a)| from nuclear charges, and the specification of atomic basis function locations and types. The density coefficient input p ∈ ℝ^M represents the electron density through expansion coefficients in the atomic basis representation ρ(r) = Σ_{μ=1}^M p_μ ω_μ(r), where M is the total number of basis functions (typically M ≈ 10N for N electrons). Each coefficient p_μ = p_{a,τ} is indexed by a center atom a and a basis function pattern τ (corresponding to different radial functions and angular momentum channels like s, p, d), making the dimensionality depend on molecular size and composition. The coefficients satisfy the normalization constraint Σ_μ p_μ w_μ = N where w_μ = ∫ω_μ(r) dr is the basis normalization and N is the total number of electrons determined by Σ_a Z^(a). During training, the model receives batches of input tuples {(M^(d), p^(d,k), T_S^(d,k), ∇_p T_S^(d,k))}_k where d indexes training molecular structures, k indexes multiple density datapoints per structure from SCF iterations, T_S^(d,k) is the kinetic energy label, and ∇_p T_S^(d,k) is the gradient label. During deployment for solving a molecular system, only M is provided as fixed input, while p becomes an optimization variable initialized using H¨uckel or ProjMINAO method and iteratively updated to minimize E_θ(p,M) until convergence to the ground-state density p*.

**Technical Terms:**
- **Cartesian Coordinates**: Position specification using three perpendicular axes (x,y,z), as opposed to spherical (r,θ,φ) or other coordinate systems.
- **Atomic Number Z**: The number of protons in an atomic nucleus, uniquely identifying the chemical element and determining nuclear charge.
- **Basis Function Pattern τ**: The angular momentum (s/p/d/f) and radial profile characteristics of atomic basis functions, determining their spatial shape.
- **Normalization Constraint**: The requirement that integrated density equals total electron count: ∫ρ(r) dr = Σ_μ p_μ w_μ = N.
- **Batch Processing**: Machine learning technique processing multiple training examples simultaneously to improve computational efficiency and gradient estimation.
- **Optimization Variable**: A quantity adjusted during iterative optimization to minimize an objective function, here the density coefficients p.
- **Initialization Strategy**: The method for choosing starting values for optimization variables, crucial for convergence to desired solutions in non-convex problems.

### Question 30. Explain how the input features are extracted.

Input feature extraction in M-OFDFT involves multiple sophisticated transformation steps to convert raw molecular structure and density coefficients into representations suitable for neural network processing while ensuring physical consistency and numerical stability. Starting from raw inputs (M,p), the extraction pipeline first constructs the atomic basis set {ω_μ(r)} on-the-fly using even-tempered Gaussian functions centered on each atom with systematically scaled exponents β^k (β=2.5, k=0,1,2,...) spanning different radial extents and angular momentum channels (s,p,d,f), with the number and types determined by each atom's element. The density coefficients p are then subjected to local frame transformation: for each atom a, a local coordinate system is established by pointing the x-axis toward the nearest heavy atom at x_a^(1), the z-axis perpendicular to the plane formed by the second-nearest atom x_a^(2), and y-axis completing a right-handed system; basis functions on atom a are rotated into this local frame, transforming the corresponding coefficients p_a accordingly to produce rotationally invariant features. Next, natural reparameterization is applied where coefficients are transformed as ˜p = M^T p with M chosen so MM^T = W (the overlap matrix with elements W_μν = ∫ω_μ(r)ω_ν(r) dr), equalizing metric contributions and enabling natural gradient descent. Statistical preprocessing then computes element-type-specific statistics from training data: biases ¯p_Z,τ = mean{p_{a,τ} | Z^(a)=Z}, gradient means ḡ_Z,τ = mean{∇_{p_{a,τ}}T_S | Z^(a)=Z}, gradient scales std_grad_Z,τ, and coefficient scales std_coeff_Z,τ. Dimension-wise rescaling applies: p'_{a,τ} = λ_Z(a),τ(˜p_{a,τ} - ¯p_Z(a),τ) where scaling factors λ_Z,τ balance coefficient and gradient magnitudes according to Eq. (5) in the paper. For molecular structure features, inter-atomic distances d_ab = ||x^(a) - x^(b)|| are computed and embedded using Gaussian radial basis functions: φ_k(d) = exp(-(d-μ_k)²/σ²) with centers μ_k spanning relevant distance ranges. Atom type features are embedded as learned vectors e_Z ∈ ℝ^d where d is the embedding dimension (512 in implementation), initialized randomly and optimized during training. The final extracted features consist of: node features combining atom type embeddings e_Z(a), rescaled coefficient vectors {p'_{a,τ}}_τ arranged by basis pattern, and edge features encoding distances through Gaussian embeddings {φ_k(d_ab)}_k for all atom pairs, ready for Graphormer processing.

**Technical Terms:**
- **Even-tempered Basis**: Basis set where exponents follow geometric progression α_k = α_0 β^k, providing systematic coverage of radial space with parameter β.
- **Angular Momentum Channel**: Quantum mechanical orbital type characterized by quantum number ℓ (ℓ=0:s, 1:p, 2:d, 3:f), determining angular dependence of wavefunctions.
- **Overlap Matrix W**: Matrix with elements W_μν = ∫ω_μ(r)ω_ν(r) dr measuring spatial overlap between basis functions μ and ν.
- **Natural Gradient Descent**: Optimization method using the inverse metric tensor (here W^(-1/2)) to account for parameter space geometry, often converging faster than standard gradient descent.
- **Gaussian Radial Basis Functions**: Smooth, localized functions φ(r) = exp(-(r-μ)²/σ²) used to embed continuous distances into fixed-dimensional feature vectors.
- **Learned Embedding**: Vector representations of discrete entities (like atom types) whose values are learned parameters optimized during training to capture relevant properties.
- **Edge Features**: Representations encoding pairwise relationships between nodes in a graph, here incorporating geometric information about atom pairs.

### Question 31. Explain the motivation, role, input-output, and structure of the 1st module.

The first module in M-OFDFT is the Density Representation and Preprocessing Module, which transforms raw density coefficients into geometrically invariant and numerically stable features suitable for neural network processing. The motivation for this module stems from the fundamental challenge that density coefficients transform equivariantly under molecular rotation while the target kinetic energy must remain invariant, creating a geometric consistency requirement. Additionally, molecular electron density exhibits vast ranges of coefficient magnitudes and gradient scales across different atomic types and basis function patterns, spanning several orders of magnitude, which conventional neural networks struggle to process effectively. The role of this module is threefold: ensuring geometric invariance through local frame transformations that align basis orientations with local molecular geometry rather than global coordinates; equalizing sensitivity across coefficient dimensions through natural reparameterization that implements natural gradient descent principles; and balancing coefficient-gradient scales through dimension-wise rescaling that manages the heterogeneous physical magnitudes across different dimensions. The input consists of raw density coefficients p ∈ ℝ^M distributed over atoms and basis patterns, along with molecular structure M = {(x^(a), Z^(a))} specifying atomic coordinates and types. The output provides preprocessed coefficient features p' that are rotationally invariant, have balanced sensitivities, and exhibit gradient scales conducive to neural network training. The module structure comprises three sequential sub-components: local frame transformation rotates each atom's coefficients into a coordinate system aligned with nearby atoms' positions, making features invariant under global rotation; natural reparameterization applies the transformation ˜p = M^T p where MM^T = W (overlap matrix) to equalize metric contributions; dimension-wise rescaling multiplies each coefficient dimension by element-type and pattern-specific factors λ_Z,τ computed from training statistics to balance coefficient and gradient magnitudes. This preprocessing pipeline is critical for enabling the subsequent Graphormer neural network to effectively learn the complex density-energy functional relationship without being hindered by geometric variability or numerical scale issues.

**Technical Terms:**
- **Equivariant Transformation**: A mapping where inputs transform in a predictable, structure-preserving way under symmetry operations (rotation), unlike invariant quantities that remain unchanged.
- **Local Frame**: An atom-centered coordinate system with axes aligned to nearby atoms' relative positions, rotating with the molecule to make density representations geometrically invariant.
- **Natural Gradient Descent**: An optimization method that accounts for the parameter space's Riemannian geometry using the Fisher information metric, often converging faster than standard gradient descent.
- **Overlap Matrix W**: A matrix with elements W_μν = ∫ω_μ(r)ω_ν(r) dr quantifying spatial overlap between basis functions, defining the metric structure of density coefficient space.
- **Dimension-wise Rescaling**: Independent scaling of each coefficient dimension using statistics-based factors to normalize the vast range of physical magnitudes across different atomic types and basis patterns.
- **Numerical Stability**: The property of computational algorithms maintaining accuracy despite finite precision arithmetic, avoiding overflow, underflow, or catastrophic cancellation.

### Question 32. Explain the motivation, role, input-output, and structure of the 2nd module.

The second module is the Atomic Reference Module, which provides a linear baseline approximation for kinetic energy using element-specific statistics to offset large mean gradients and facilitate neural network training. The motivation arises from the observation that raw gradient labels ∇_p T_S exhibit not only vast ranges but also large systematic biases—the mean gradient values are substantial and vary considerably across different atomic types and basis function patterns, creating a difficult learning landscape where the neural network must express both large offset values and fine-grained variations. By explicitly modeling the dominant linear component of the kinetic energy functional using per-element statistics, the atomic reference reduces the learning burden on the neural network, allowing it to focus on capturing the more subtle nonlinear density-energy relationships. The role of this module is to compute a simple, interpretable baseline kinetic energy estimate T_AtomRef(p,M) = ḡ_M^T p + T̄_M that captures first-order dependencies on density coefficients through element-specific gradient means ḡ_Z,τ and per-element energy offsets T̄_Z, effectively providing a "starting point" that the neural network refines. This baseline has constant gradients ∇_p T_AtomRef = ḡ_M, which offsets the mean gradient that the neural network must learn, significantly reducing gradient label scales and improving training stability. The input consists of preprocessed density coefficients ˜p (after natural reparameterization) and molecular structure M specifying which atoms are present and their types. The output is a scalar kinetic energy contribution T_AtomRef and implicitly its constant gradient ḡ_M ∈ ℝ^M. The module structure is extremely simple: a linear combination T_AtomRef(˜p,M) = Σ_a Σ_τ ḡ_Z(a),τ ˜p_a,τ + Σ_a T̄_Z(a) + T̄_global, where the gradient mean ḡ_Z,τ and energy bias T̄_Z for each element type Z and basis pattern τ are pre-computed statistics from the training data rather than learned parameters. Specifically, ḡ_Z,τ = mean{∇_{p_{a,τ}}T_S | Z^(a)=Z} averages gradient values across all atoms of type Z in training structures, and T̄_Z represents per-element energy contributions. A global constant T̄_global accounts for overall energy offsets. This pre-computed, non-learnable design ensures the module captures systematic patterns without consuming model capacity, while the neural network focuses its expressiveness on learning residual nonlinear effects that classical approximations miss.

**Technical Terms:**
- **Linear Baseline**: A simple linear function serving as a reference or starting point, capturing first-order relationships before more complex nonlinear refinements are applied.
- **Gradient Mean**: The average value of gradient components across training data, representing systematic bias that can be pre-computed and subtracted to center the learning problem.
- **Energy Offset**: An additive constant or element-specific constant term in energy expressions that accounts for reference state contributions, not affecting forces or energy differences.
- **Pre-computed Statistics**: Quantities calculated once from training data (means, standard deviations, etc.) and fixed during model training, used for normalization or baseline estimation.
- **Residual Learning**: Training a model to predict the difference (residual) between true values and a baseline approximation, often easier than learning the full mapping directly.
- **Model Capacity**: The expressiveness or representational power of a neural network, determined by architecture (depth, width) and number of parameters.

### Question 33. Explain the motivation, role, input-output, and structure of the 3rd module.

The third module is the Graphormer Neural Network, which serves as the core learnable component capturing complex nonlocal density-energy relationships that the linear atomic reference cannot express. The motivation stems from the essential requirement that accurate kinetic energy approximation must account for nonlocal interactions where electron density at one spatial location influences kinetic energy contributions at distant locations—a fundamental quantum mechanical phenomenon that classical local or semi-local approximations fail to capture adequately. The Graphormer architecture, based on Transformer's attention mechanism, naturally handles such nonlocal interactions by computing attention weights between all atom pairs, enabling information flow across the entire molecular structure regardless of spatial separation. This is crucial for molecular systems where covalent bonding, through-space interactions, and shell structure formation involve long-range density correlations. The role of this module is to learn the complex nonlinear functional T_NN(p',M) mapping rescaled density coefficients p' and molecular structure M to kinetic energy contributions beyond what the atomic reference captures, by aggregating density information across all atoms weighted by learned attention patterns that identify physically relevant interactions. The input consists of rescaled density coefficients p'_a distributed over atoms (output from preprocessing), atomic types Z^(a), and inter-atomic distances d_ab = ||x^(a) - x^(b)|| computed from molecular structure. The output is a scalar kinetic energy value T_NN representing the neural network's contribution, which combines with the atomic reference to give the total kinetic energy prediction T_S,θ = T_AtomRef + T_NN. The module structure follows the Graphormer architecture with L=12 layers of attention-based processing: Node embeddings h_a^(0) initialize each atom's representation by concatenating learned element-type embeddings e_Z(a) ∈ ℝ^512 with rescaled coefficient features {p'_a,τ}_τ. Edge embeddings encode inter-atomic distances through Gaussian radial basis expansion φ_k(d_ab) = exp(-(d_ab-μ_k)²/σ²) with 128 basis functions. Each of the L layers performs multi-head attention with h=32 heads, computing attention scores α_ij^(ℓ) between atoms i and j using queries Q_i, keys K_j, and values V_j derived from node features, modified by spatial bias from edge embeddings to inject geometric information. Node features update as h_a^(ℓ+1) = Σ_b α_ab^(ℓ) V_b^(ℓ) followed by feed-forward networks with ReLU activation and residual connections h^(ℓ+1) ← h^(ℓ) + Transform(h^(ℓ)) to enable deep architectures. After L layers, a ReadOut function extracts scalar features from each node's final embedding h_a^(L) through linear projection, and these are summed across all atoms: T_NN = Σ_a ReadOut(h_a^(L)), aggregating local contributions into the total kinetic energy prediction. The model contains approximately 47 million parameters with hidden dimension 768, trained end-to-end using backpropagation.

**Technical Terms:**
- **Nonlocal Interactions**: Physical effects where quantities at one spatial location depend on values at distant locations, requiring integration or convolution over extended regions rather than pointwise evaluation.
- **Attention Mechanism**: Neural network component computing weighted combinations of features where weights are dynamically calculated based on learned relevance, enabling selective information aggregation from multiple sources.
- **Multi-head Attention**: Extension of attention using h parallel attention operations, each learning different interaction patterns, whose outputs are concatenated to capture diverse relationship types.
- **Query-Key-Value (QKV)**: The three component vectors in attention computation: attention weights are α ∝ softmax(Query·Key/√d) and outputs are Σ α_i Value_i.
- **Spatial Bias**: Additional terms in attention score computation depending on geometric information (distances, angles) to inject 3D molecular structure into graph neural networks.
- **Feed-Forward Network (FFN)**: Multi-layer perceptron applied independently to each node, typically with ReLU activation and hidden dimension larger than input/output (often 4×).
- **Residual Connection**: Skip connection adding layer input to output (h^(ℓ+1) = h^(ℓ) + f(h^(ℓ))), mitigating vanishing gradients and enabling training of very deep networks.
- **Gaussian Radial Basis Function**: Function φ(r) = exp(-(r-μ)²/σ²) providing smooth, localized encoding of continuous distances into fixed-dimensional feature vectors.

### Question 34. Explain the motivation, role, input-output, and structure of the 4th module.

The fourth module is the Energy Assembly and Optimization Module, which combines the learned kinetic energy functional with other DFT energy components and performs iterative gradient descent optimization to find the ground-state electron density. The motivation arises from M-OFDFT's formulation as learning an optimization objective rather than end-to-end prediction—the learned KEDF model must be integrated into the complete DFT energy functional and used repeatedly during density optimization, requiring efficient energy evaluation and gradient computation for each iteration. This approach enables better extrapolation by separating the learned physical mechanism (density-energy relationship) from the optimization process that identifies ground states. The role of this module is twofold: assembling the complete electronic energy functional E_θ(p,M) by combining the learned kinetic energy T_S,θ(p,M) with Hartree energy E_H(p,M), exchange-correlation energy E_XC(p,M), and external potential energy E_ext(p,M) computed using established DFT formulas; and iteratively optimizing density coefficients p through gradient descent with normalization constraint projection to converge to the ground state where energy is minimized. The input during training is molecular structure M and ground-state density labels for supervised learning. During deployment, only M is provided as input, with p initialized using H¨uckel or ProjMINAO methods and treated as the optimization variable. The output is the optimized ground-state density p* minimizing E_θ(p,M), the corresponding ground-state energy E* = E_θ(p*,M), and derived properties including Hellmann-Feynman forces f^(a) = -∇_{x^(a)} E* on atoms calculated by differentiating energy with respect to atomic positions. The module structure comprises several components: Energy functional assembly combines T_S,θ(p,M) = T_AtomRef(˜p,M) + T_NN(p',M) from modules 1-3 with E_H(p,M) = (1/2)∫∫ρ(r)ρ(r')/|r-r'| dr dr', E_XC(p,M) using the PBE functional, and E_ext(p,M) = -Σ_a Z^(a)∫ρ(r)/|r-x^(a)| dr, all implemented with automatic differentiation in PyTorch for efficient gradient computation. Density optimization iterates p^(k+1) = p^(k) - ε[I - ww^T/(w^T w)]∇_p E_θ(p^(k),M), where the projection operator [I - ww^T/(w^T w)] ensures density normalization constraint Σ_μ p_μ w_μ = N is maintained by subtracting the component parallel to the normalization vector w. The step size ε = 0.001 is fixed, and optimization continues until the gradient norm ||∇_p E_θ|| falls below threshold 0.1 or maximum 300 iterations are reached. Convergence typically occurs in 50-150 iterations for molecules in the training size range and slightly more for larger extrapolation cases. This module bridges training (where M-OFDFT learns to approximate KEDF from labeled data) and deployment (where M-OFDFT solves new molecular systems by optimizing density using the learned functional).

**Technical Terms:**
- **Energy Functional Assembly**: Combining multiple energy component functionals E = T_S + E_H + E_XC + E_ext into the total electronic energy that will be minimized during optimization.
- **Hartree Energy E_H**: Classical electrostatic repulsion energy (1/2)∫∫ ρ(r)ρ(r')/|r-r'| dr dr' treating electrons as classical charge distribution, computed using auxiliary basis expansion.
- **Exchange-Correlation Energy E_XC**: Quantum mechanical correction accounting for Pauli exclusion (exchange) and correlated electron motion beyond mean-field, here using the PBE functional.
- **External Potential Energy E_ext**: Attractive Coulomb interaction energy -Σ_a Z^(a)∫ρ(r)/|r-x^(a)| dr between electrons and positively charged nuclei at positions x^(a).
- **Automatic Differentiation**: Computational technique automatically computing derivatives of functions specified as programs by applying chain rule to elementary operations, enabling gradient calculation through complex compositions.
- **Projection Operator**: Mathematical operator [I - ww^T/(w^T w)] that projects vectors onto the subspace orthogonal to w, used to maintain normalization constraint during optimization.
- **Gradient Norm**: The Euclidean length ||∇E|| = √(Σ_μ (∂E/∂p_μ)²) of the gradient vector, measuring the magnitude of the energy's rate of change and serving as a convergence criterion.
- **Hellmann-Feynman Force**: Force on nucleus a calculated as f^(a) = -∇_{x^(a)} E using the Hellmann-Feynman theorem, which states forces equal negative energy gradients with respect to nuclear positions.

### Question 35. Define the prediction.

The prediction in M-OFDFT is mathematically defined as the optimized ground-state electron density ρ*(r) and corresponding ground-state electronic energy E* obtained by minimizing the learned energy functional. Formally, given a molecular structure M = {(x^(a), Z^(a))}_{a=1}^A specifying atomic coordinates and types, the prediction process solves the optimization problem: p* = argmin_{p} E_θ(p,M) subject to p^T w = N, where p ∈ ℝ^M represents density expansion coefficients under the atomic basis ρ(r) = Σ_μ p_μ ω_μ(r), and the normalization constraint ensures ∫ρ(r) dr = N electrons. The energy functional being minimized is E_θ(p,M) = T_S,θ(p,M) + E_H(p,M) + E_XC(p,M) + E_ext(p,M), where T_S,θ(p,M) = T_AtomRef(˜p,M) + T_NN(p',M) is the learned kinetic energy functional with parameters θ, and ˜p, p' denote preprocessed coefficients through local frame transformation and rescaling. The optimization is performed using gradient descent with projection: p^(k+1) = p^(k) - ε[I - ww^T/(w^T w)]∇_p E_θ(p^(k),M), iterating from initialization p^(0) until convergence ||∇_p E_θ(p^(k),M)|| < δ where δ=0.1 is the tolerance. The optimized coefficients p* define the predicted ground-state density ρ*(r) = Σ_μ p_μ* ω_μ(r), which represents the spatial distribution of electrons in their lowest energy quantum state. The predicted ground-state energy is E* = E_θ(p*,M), giving the total electronic energy in atomic units (Hartrees). From these primary predictions, derived quantities are calculated: Hellmann-Feynman forces on atoms f^(a) = -∇_{x^(a)} E*|_{p=p*} obtained by differentiating the ground-state energy with respect to atomic coordinates while keeping the optimized density fixed; electron density-derived properties including dipole moment μ = ∫r ρ*(r) dr - Σ_a Z^(a) x^(a), Hirshfeld partial atomic charges q_a computed by partitioning ρ*(r) according to reference atomic densities, and visualization of electron density distribution ρ*(x,y,z) revealing shell structures and bonding characteristics. For conformational analysis applications, the relative energy ΔE = E*(M_i) - E*(M_ref) between different molecular conformations M_i and a reference conformation M_ref provides predictions of conformational preferences and energy barriers. The prediction quality is evaluated by comparing these quantities against reference values from high-accuracy Kohn-Sham DFT calculations at the PBE/6-31G(2df,p) level, using metrics including mean absolute errors in energy (MAE_E), forces (MAE_F), and derived properties.

**Technical Terms:**
- **Argmin**: Mathematical notation representing the argument (input value) that minimizes a function; p* = argmin_p f(p) means p* is the value of p that gives the smallest f(p).
- **Convergence Criterion**: A condition for stopping iterative optimization, here based on gradient norm ||∇E|| falling below threshold δ, indicating a stationary point has been reached.
- **Atomic Units (Hartrees)**: Natural unit system in quantum chemistry where fundamental constants (ℏ, m_e, e, 4πε_0) equal 1; energy unit is 1 Hartree ≈ 27.211 eV ≈ 627.5 kcal/mol.
- **Dipole Moment μ**: A vector quantity μ = ∫r ρ(r) dr - Σ_a Z^(a) x^(a) measuring charge separation in a molecule, determining interactions with electric fields, measured in Debye (D).
- **Hirshfeld Partial Charges**: Atomic charge assignments obtained by partitioning molecular electron density ρ(r) according to the ratio of reference atomic densities ρ_a^0(r-x^(a)), giving q_a = Z_a - ∫ w_a(r) ρ(r) dr.
- **Conformational Preferences**: The relative stability of different three-dimensional arrangements of a molecule, determined by energy differences ΔE between conformations.
- **Fixed-Density Derivative**: Differentiating energy with respect to nuclear positions while keeping the electron density fixed at its optimized value, as required by the Hellmann-Feynman theorem.

### Question 36. What is the loss function?

The loss function for training M-OFDFT is a weighted combination of three components that simultaneously optimize kinetic energy value prediction, gradient prediction, and on-manifold density fitting. Mathematically, the total training loss is L_total = λ_eng L_eng + λ_grad L_grad + λ_den L_den, where λ_eng, λ_grad, λ_den are dataset-specific weight hyperparameters. The energy loss L_eng = (1/|D|) Σ_{d,k} (T_S,θ(p^(d,k),M^(d)) - T_S^(d,k))² measures mean squared error between predicted kinetic energy T_S,θ and reference labels T_S^(d,k) from KSDFT calculations, summed over all molecular structures d in training dataset D and all SCF iteration steps k providing multiple density datapoints per structure. The gradient loss L_grad = (1/|D|) Σ_{d,k} ||Π[∇_p T_S,θ(p^(d,k),M^(d)) - ∇_p T_S^(d,k)]||² measures mean squared error between predicted gradients ∇_p T_S,θ and reference gradient labels ∇_p T_S^(d,k), where Π is a projection operator onto the subspace orthogonal to the normalization constraint, implemented as Π[g] = g - (g^T w)w/(w^T w). This gradient loss is crucial because the learned functional serves as an optimization objective—accurate gradients ensure proper convergence during density optimization. The density loss L_den = (1/|D|) Σ_d ||p̂^(d) - p_gt^(d)||² where p̂^(d) = argmin_p E_θ(p,M^(d)) is the optimized density using the current model and p_gt^(d) is the ground-state density from KSDFT, encouraging the optimization process using the learned functional to recover correct ground-state densities. This term effectively provides end-to-end supervision beyond point-wise functional approximation. The loss weights are tuned independently for each dataset through grid search on validation sets: for ethanol λ_eng=1.0, λ_grad=10.0, λ_den=0.1; for QM9 λ_eng=1.0, λ_grad=20.0, λ_den=0.01; for QMugs λ_eng=1.0, λ_grad=15.0, λ_den=0.005. The gradient loss typically receives higher weight because gradients span a wider range of magnitudes than energies, requiring stronger signal for effective learning. Models are trained using Adam optimizer with peak learning rate 1×10^(-4), linear decay schedule, warmup for 30,000 steps, batch size 256 (ethanol) or 128 (other datasets), trained for 600-700 epochs until validation loss plateaus. The multi-component loss with emphasis on gradients addresses the unconventional learning challenge that the model must accurately capture energy landscapes over the space of possible densities, not just predict single ground-state values. Alternative loss formulations were explored including using only energy loss (insufficient gradient accuracy) or only gradient loss (energy scale ambiguity), confirming the necessity of the combined approach.

**Technical Terms:**
- **Mean Squared Error (MSE)**: Loss function L = (1/N) Σ_i (y_pred^(i) - y_true^(i))² averaging squared differences between predictions and ground truth, penalizing large errors more heavily.
- **Projection Operator Π**: Mathematical operator Π[g] = g - (g^T w)w/(w^T w) projecting vectors onto the subspace orthogonal to normalization direction w.
- **Adam Optimizer**: Adaptive moment estimation optimizer combining momentum and adaptive learning rates, using first and second moment estimates of gradients for efficient training.
- **Learning Rate Schedule**: Time-varying adjustment of the learning rate during training, here using linear decay from peak value to zero over training epochs.
- **Warmup Steps**: Initial training phase with gradually increasing learning rate from zero to peak value, stabilizing training dynamics in early iterations.
- **Validation Loss Plateau**: Condition where validation loss stops decreasing over multiple epochs, used as an early stopping criterion to prevent overfitting.
- **Grid Search**: Hyperparameter optimization method systematically evaluating performance across a discrete set of hyperparameter combinations to find optimal settings.

---

## 5. Experimental Setup (0/11 completed)

### Question 37. Explain about the dataset. If the dataset was constructed in this study, explain how to construct it.

The M-OFDFT study uses multiple datasets constructed specifically for this research by generating labeled data from Kohn-Sham DFT calculations, covering both conformational space (ethanol) and chemical space (QM9) for training, plus extrapolation datasets (QMugs, chignolin) for testing generalization to larger molecules. The ethanol dataset contains 100,000 non-equilibrium molecular conformations randomly sampled from the MD17 dataset, which provides room-temperature molecular dynamics trajectories capturing diverse three-dimensional arrangements of the C₂H₆O molecule (9 atoms). These structures span conformational space through bond rotations and vibrations while maintaining the same chemical composition, enabling evaluation of M-OFDFT's ability to generalize across different molecular geometries. The QM9 dataset comprises approximately 134,000 small organic molecules with up to 9 heavy atoms (H, C, N, O, F) at equilibrium geometries, representing a subset of the GDB-17 chemical universe database that enumerates billions of possible organic molecules. This dataset covers diverse chemical space including varied molecular compositions, functional groups, and bonding patterns. The QMugs dataset extends to larger molecules with 30-100 heavy atoms from the ChEMBL database of biologically relevant compounds, divided into bins by heavy atom count (10-15, 16-20, ..., 96-100) to systematically study extrapolation. The first QMugs bin (10-15 heavy atoms) joins QM9 for training, while larger bins serve as extrapolation test sets. The chignolin dataset comprises 1,000 structures of a 10-residue protein sampled from extensive MD simulations, processed through time-lagged independent component analysis (TICA) to identify representative conformations, then cut into peptide fragments of varying lengths (dipeptides, tripeptides, tetrapeptides, pentapeptides) for training, with full 168-atom neutralized structures as extrapolation test cases. Dataset construction involves several steps: molecular structures are obtained from source datasets (MD17, QM9, ChEMBL, MD simulations); protein structures undergo neutralization by editing ionizable groups (N-terminal, C-terminal) to ensure closed-shell electronic states, followed by local energy minimization using Amber force fields; for each structure M^(d), restricted-spin KSDFT calculations at PBE/6-31G(2df,p) level are performed using PySCF with the 6-31G(2df,p) orbital basis and density fitting enabled; SCF iterations produce multiple density-energy pairs {p^(d,k), T_S^(d,k)}_k where k indexes iteration steps, with density coefficients p^(d,k) obtained through density fitting onto an even-tempered auxiliary basis (β=2.5) and kinetic energy T_S^(d,k) calculated directly from orbitals; gradient labels ∇_p T_S^(d,k) are computed using the relation that p^(d,k) minimizes the non-interacting fermion energy, giving ∇_p T_S = -v_eff^(d,k) up to normalization projection; preprocessing applies local frame transformation and natural reparameterization to coefficients; for chignolin, datapoints with energy residuals exceeding 500 kcal/mol from ground state are filtered to improve training stability. The resulting datasets contain not just ground-state labels but multiple density configurations per structure with both value and gradient labels, essential for learning an optimization objective function.

**Technical Terms:**
- **MD17 Dataset**: A molecular dynamics dataset containing trajectories for small organic molecules at room temperature, widely used for benchmarking machine learning methods in quantum chemistry.
- **GDB-17 Chemical Universe**: A database enumerating 166 billion possible organic molecules with up to 17 atoms (C, N, O, S, halogens) satisfying chemical stability and synthetic feasibility rules.
- **ChEMBL Database**: A manually curated database of bioactive drug-like molecules with pharmacological activity data, containing over 2 million compounds from medicinal chemistry literature.
- **Time-Lagged Independent Component Analysis (TICA)**: A dimensionality reduction technique for analyzing molecular dynamics data, identifying slow collective motions by maximizing autocorrelation at specified lag time.
- **Neutralization**: Process of adjusting protonation states of ionizable functional groups (carboxyl, amine) to achieve overall neutral charge and closed-shell electronic configuration.
- **SCF Iteration**: Self-consistent field iteration steps where orbitals and density are iteratively updated until convergence, each step solving a non-interacting fermion problem with well-defined kinetic energy.
- **Even-tempered Auxiliary Basis**: Basis set for density fitting with exponents following geometric progression α_k = α_0 β^k (β=2.5), providing systematic coverage of radial space.

### Question 38. Explain about the instructions given to the annotators.

This research does not employ human annotators for data labeling, as all ground-truth labels are generated algorithmically through Kohn-Sham density functional theory calculations using the PySCF computational chemistry software package. The "annotation" process is entirely automated and deterministic, following established quantum chemistry protocols to ensure reproducibility and consistency. The procedure can be described as automated computational labeling with the following specifications: For each molecular structure M^(d) defined by atomic coordinates and atomic numbers, KSDFT calculations are executed at the PBE/6-31G(2df,p) level with restricted-spin formulation suitable for closed-shell molecules, using density fitting with the def2-universal-jfit basis set for computational acceleration. The SCF procedure iterates with convergence tolerance 10^(-9) Hartrees for energy and 10^(-7) for density, typically requiring 10-30 iterations depending on molecular size and complexity. During each SCF iteration k, the algorithm computes orbital solutions φ_i^(d,k) for the non-interacting fermion problem in an effective potential v_eff^(d,k), from which multiple labels are extracted: density coefficients p^(d,k) are obtained by density fitting the orbital-based density ρ^(d,k)(r) = Σ_i |φ_i^(d,k)(r)|² onto the even-tempered auxiliary basis, minimizing ||ρ^(d,k) - Σ_μ p_μ^(d,k) ω_μ||² subject to normalization; kinetic energy labels T_S^(d,k) are directly calculated as Σ_i ⟨φ_i^(d,k)|-½∇²|φ_i^(d,k)⟩ using the orbital solutions; gradient labels ∇_p T_S^(d,k) are computed exploiting that p^(d,k) minimizes the non-interacting energy T_S(p) + p^T v_eff^(d,k), yielding ∇_p T_S(p^(d,k)) = -v_eff^(d,k) after normalization projection; force labels on atoms are calculated using analytic derivative implementations in PySCF following the Hellmann-Feynman theorem. Quality control procedures include: verifying SCF convergence before extracting labels, filtering datapoints with excessive energy residuals (>500 kcal/mol for chignolin), checking density normalization errors (<10^(-6)), and validating that gradient projections satisfy stationarity conditions. No subjective human judgment is involved—all labels derive from solving the well-defined KSDFT equations numerically. The computational cost for label generation is substantial: QM9 dataset generation required approximately 50,000 CPU-hours, while chignolin structures with 168 atoms each required several GPU-hours per structure using density fitting acceleration. This automated annotation approach ensures consistency, eliminates inter-annotator variability, and provides exact numerical gradients essential for training optimization objective functions, which would be infeasible to obtain through human labeling or experimental measurements.

**Technical Terms:**
- **Automated Computational Labeling**: Data annotation performed by running deterministic computational algorithms (simulations, calculations) rather than human judgment, ensuring reproducibility.
- **PySCF**: Python-based Simulations of Chemistry Framework, an open-source quantum chemistry software package providing flexible, programmable implementations of DFT and other electronic structure methods.
- **Convergence Tolerance**: Threshold values (e.g., 10^(-9) for energy) used to determine when iterative algorithms have reached sufficient accuracy and should terminate.
- **Density Fitting Error**: The approximation error ||ρ_exact - ρ_fitted|| when representing orbital-based density using auxiliary basis functions, minimized through least-squares optimization.
- **Normalization Error**: Deviation |∫ρ(r) dr - N| of integrated density from the exact electron count N, used to verify numerical accuracy of density representations.
- **Stationarity Condition**: Requirement that at a minimum, the projected gradient ∇_p E - λw (where λ is Lagrange multiplier for normalization) should vanish, verifying correct ground state.
- **Analytic Derivative**: Exact mathematical expression for derivatives implemented in software, as opposed to numerical finite-difference approximations, providing higher accuracy and efficiency.

### Question 39. Why did not you use the standard data set? If you did, why?

The research employs both standard datasets (MD17 for ethanol, QM9 for small molecules) and newly constructed datasets (QMugs extrapolation bins, chignolin fragments), with this hybrid approach motivated by the specific requirements for evaluating M-OFDFT's capabilities. Standard datasets MD17 and QM9 are utilized because they are widely recognized benchmarks in molecular machine learning, enabling direct comparison with prior work and establishing credibility—these datasets contain carefully validated molecular structures with well-characterized chemical diversity (QM9) and conformational diversity (MD17), making them ideal for assessing in-scale generalization performance where test molecules have similar sizes to training molecules. However, standard datasets alone are insufficient for this research because M-OFDFT's key scientific claim involves extrapolation to molecules substantially larger than training examples, exploiting the O(N²) scaling advantage of OFDFT, yet existing standard benchmarks predominantly contain small molecules (QM9 has maximum 9 heavy atoms, MD17 similarly small), preventing meaningful extrapolation studies. This necessitates constructing additional datasets with larger molecules: QMugs provides molecules with 10-100 heavy atoms systematically divided into size bins, enabling controlled extrapolation experiments where models trained on small molecules (QM9 + first QMugs bin with 10-15 heavy atoms) are tested on progressively larger molecules up to 100 heavy atoms—this dataset did not exist in the form needed for this study. The chignolin dataset is constructed specifically to demonstrate practical applicability to protein-scale biomolecular systems (168 atoms after neutralization), representing a real-world large-molecule use case that standard datasets do not cover, while the fragment-to-whole design (training on dipeptides through pentapeptides, testing on full protein) enables studying local-to-global extrapolation relevant to biomolecular applications. Furthermore, standard quantum chemistry datasets typically provide only ground-state properties, but M-OFDFT requires multiple density-energy pairs with gradient labels per molecular structure to learn an optimization objective—this specialized data format necessitates custom data generation by running SCF calculations and extracting intermediate iteration steps, which cannot be obtained from published standard datasets. The choice reflects a balanced strategy: leveraging standard datasets where appropriate for credibility and comparison, while necessarily constructing custom datasets to address novel scientific questions (size extrapolation, protein applications) and methodological requirements (multiple-datapoint training with gradients) that existing benchmarks do not support.

**Technical Terms:**
- **In-scale Generalization**: Model performance on test examples similar in size/complexity to training examples, as opposed to extrapolation beyond training distribution.
- **Benchmark Dataset**: A standardized dataset widely used by the research community for comparing different methods, enabling fair comparison and reproducibility.
- **Systematic Size Binning**: Dividing molecules into groups based on quantitative size measures (heavy atom count) with regular intervals to enable controlled extrapolation studies.
- **Fragment-to-whole Design**: Training on smaller molecular fragments and testing on complete larger molecules, relevant for studying whether local patterns generalize to global structures.
- **Intermediate Iteration Steps**: Non-converged states during iterative SCF calculations, providing off-manifold density examples essential for learning optimization landscapes.
- **Biomolecular Systems**: Biological macromolecules including proteins, nucleic acids, and carbohydrates, characterized by large size (hundreds to thousands of atoms) and biological function.

### Question 40. How was the dataset pre-processed?

The dataset undergoes extensive preprocessing to transform raw KSDFT calculation outputs into geometrically invariant, numerically stable input features for neural network training. The preprocessing pipeline comprises several sequential stages: First, molecular structure preprocessing handles protein-specific adjustments where ionized residues are neutralized by editing hydrogen atoms on N-terminal amine and C-terminal carboxyl groups using OpenMM, followed by local geometry optimization using Amber force field (ff14SB, maximum 100 cycles) constraining heavy atoms involved in hydrogen editing to minimize structural perturbation. Second, density coefficient extraction performs density fitting where orbital-based density ρ^(d,k)(r) = Σ_i |φ_i^(d,k)(r)|² from SCF iteration k is projected onto an even-tempered auxiliary basis {ω_μ(r)} with tempering ratio β=2.5, solving the least-squares problem min_p ||ρ^(d,k) - Σ_μ p_μ ω_μ||² subject to p^T w = N using analytical solutions involving overlap matrices. Third, label calculation computes kinetic energy T_S^(d,k) = Σ_i ⟨φ_i^(d,k)|-½∇²|φ_i^(d,k)⟩, projected gradient labels ∇_p T_S^(d,k) = Π[-v_eff^(d,k)] where Π projects orthogonal to normalization, and force labels f^(d) = -∇_X E^(d) using analytic derivatives. Fourth, geometric transformation applies local frame rotation to each atom's coefficients {p_a,τ}_τ where the local frame axes are defined by nearby atom positions—x-axis points to nearest heavy atom, z-axis perpendicular to plane of second-nearest heavy atom, ensuring rotational invariance. Fifth, natural reparameterization transforms coefficients as ˜p = M^T p where M satisfies MM^T = W (overlap matrix), implementing the natural gradient metric through Cholesky decomposition W = LL^T and setting M = L^(-1). Sixth, statistical normalization computes element-and-pattern-specific statistics from training data: coefficient biases ¯p_Z,τ = mean{p_a,τ | Z(a)=Z}, gradient means ḡ_Z,τ = mean{∇_{p_a,τ}T_S}, coefficient scales std_coeff_Z,τ, and gradient scales std_grad_Z,τ. Seventh, dimension-wise rescaling applies p'_a,τ = λ_Z(a),τ(˜p_a,τ - ¯p_Z(a),τ) where scaling factors λ_Z,τ = min(mean_abs_grad_Z,τ/s_grad, s_coeff/std_coeff_Z,τ) if mean_abs_grad_Z,τ > s_grad else 1, with target scales s_grad=1.0, s_coeff=10.0. Finally, data filtering removes datapoints with energy residuals exceeding 500 kcal/mol from ground state for chignolin to improve training stability. All preprocessing transformations are deterministic and invertible (except filtering), enabling recovery of physical density from preprocessed coefficients during deployment. The local frame and reparameterization transformations are recomputed for each molecular structure during both training and inference, while scaling factors and biases are computed once from training data and frozen.

**Technical Terms:**
- **Least-squares Projection**: Optimization problem min_x ||Ax - b||² finding the x that minimizes squared difference between Ax and target b, solvable analytically as x = (A^T A)^(-1) A^T b.
- **Overlap Matrix W**: Symmetric positive-definite matrix with elements W_μν = ∫ω_μ(r)ω_ν(r) dr quantifying basis function overlaps, defining the metric in coefficient space.
- **Cholesky Decomposition**: Factorization of positive-definite matrix W = LL^T where L is lower triangular, providing efficient computation of W^(-1/2) = L^(-T).
- **Projected Gradient**: Gradient vector projected onto the tangent space of the constraint manifold, Π[g] = g - (g^T w)w/(w^T w), ensuring updates maintain normalization.
- **Element-specific Statistics**: Quantities (means, standard deviations) computed separately for each chemical element type Z, accounting for systematic differences across the periodic table.
- **Invertible Transformation**: A bijective mapping that can be reversed to recover original values, essential when preprocessing must be undone during inference or visualization.

### Question 41. Explain about the statistics of the dataset: dataset size, vocabulary size, average sentence length, language, number of annotators, simulation or real-world.

The dataset statistics for M-OFDFT are as follows, noting that molecular datasets use different metrics than natural language datasets. The ethanol dataset contains 100,000 molecular structures divided into training (80,000), validation (10,000), and test (10,000) sets with 8:1:1 ratio, each structure generating approximately 15-20 datapoints from SCF iterations, yielding roughly 1.5 million training examples. Average molecule size is 9 atoms (2 carbon, 6 hydrogen, 1 oxygen) with approximately 260 basis functions per structure. The QM9 dataset comprises approximately 130,000 molecular structures after excluding failed calculations, divided into training (~117,000), validation (~13,000), and test (~13,000) with roughly 9:1:1 ratio. Molecules contain 6-15 atoms (average ~10 atoms) with up to 9 heavy atoms, spanning chemical compositions like C_xH_yN_zO_wF_v where subscripts vary. The average number of basis functions is ~300, with each structure generating ~20 SCF iteration datapoints, providing ~2.3 million training examples. Element vocabulary consists of 5 atom types: H (hydrogen), C (carbon), N (nitrogen), O (oxygen), F (fluorine). The QMugs dataset for extrapolation testing contains molecules with 10-100 heavy atoms divided into bins (10-15, 16-20, 21-25, ..., 96-100), with 50 structures sampled per bin for testing. The first bin (10-15 heavy atoms) contains ~10,000 structures used for training together with QM9. Average basis function count ranges from ~400 (small molecules) to ~3,000 (largest molecules with 100 heavy atoms). The chignolin dataset comprises 1,000 full protein structures (each 168 atoms after neutralization, ~4,600 basis functions) sampled from MD simulations representing diverse conformations. Fragment datasets for training contain all possible peptide subsequences up to length 5 from 1,000 structures: dipeptides (~20 atoms, ~550 basis functions), tripeptides (~30 atoms, ~800 basis functions), tetrapeptides (~40 atoms, ~1,100 basis functions), pentapeptides including gapped combinations (~50 atoms, ~1,400 basis functions), yielding tens of thousands of fragment structures in training sets. "Annotator" count is effectively 1 (deterministic KSDFT algorithm via PySCF), ensuring perfect inter-annotator agreement. All data is computational simulation-generated rather than experimental real-world measurements, with ground truth defined by PBE/6-31G(2df,p) KSDFT calculations. Molecular "language" consists of atoms as tokens, bonds as grammatical relations, with "vocabulary" being chemical elements (5 types for QM9/ethanol, expanding to ~10 types including S, Cl for QMugs/chignolin), and "sentence length" analogous to molecule size (6-168 atoms). Training requires ~50,000 CPU-hours for QM9 data generation, performed on high-performance computing clusters.

**Technical Terms:**
- **SCF Iteration Datapoints**: Multiple density-energy pairs extracted from self-consistent field iterations, with each molecular structure generating 10-30 datapoints depending on convergence trajectory.
- **Heavy Atoms**: Non-hydrogen atoms in molecules, determining molecular size complexity; C, N, O, F, etc. have substantially more electrons and basis functions than H.
- **Basis Function Count**: Number of atomic basis functions M used to expand density, scaling approximately as 10N for N electrons, determining input dimensionality.
- **Chemical Composition**: Molecular formula C_xH_yN_zO_w specifying element types and counts, with subscripts indicating atom quantities.
- **Element Vocabulary**: The set of distinct chemical elements appearing in the dataset, analogous to word vocabulary in NLP; QM9 has 5-element vocabulary.
- **Subsequence**: A contiguous portion of a sequence; peptide subsequences are fragments like dipeptides (2 residues) or tripeptides (3 residues) extracted from protein sequences.
- **Inter-annotator Agreement**: Measure of consistency between different annotators' labels; here perfect (100%) since deterministic algorithm produces identical results.

### Question 42. How was the dataset divided into training set, validation set, and test set? Indicate the size of each.

The dataset division strategies vary by dataset to serve different evaluation purposes, with careful design to prevent data leakage and enable meaningful generalization assessment. For the ethanol dataset evaluating conformational generalization, the 100,000 structures are randomly partitioned into training (80,000 structures, 80%), validation (10,000 structures, 10%), and test (10,000 structures, 10%) sets with 8:1:1 ratio, ensuring conformations are independently sampled from the MD trajectory without temporal correlation considerations. Since all structures share the same molecular formula C₂H₆O, this split tests whether the model generalizes to unseen conformations of the same molecule. For the QM9 dataset evaluating chemical space generalization, approximately 134,000 molecules (after excluding calculation failures) are divided into training (~120,000 structures, ~90%), validation (~7,000 structures, ~5%), and test (~7,000 structures, ~5%) sets following conventional random splits, ensuring diverse chemical compositions in each set while maintaining similar size distributions. The uneven 90:5:5 ratio allocates more data to training given the large chemical diversity. For QMugs extrapolation experiments, the division is size-based rather than random: the union of QM9 (all 134k molecules) and the first QMugs bin (molecules with 10-15 heavy atoms, ~10,000 molecules) forms the training pool, divided into training (~130,000 structures, 90%) and validation (~14,000 structures, 10%) with 9:1 ratio. Crucially, no molecules with >15 heavy atoms appear in training or validation. Test sets comprise 50 molecular structures sampled from each larger QMugs bin (16-20, 21-25, ..., 96-100 heavy atoms), totaling ~850 test structures across 17 bins, enabling systematic extrapolation evaluation. This division ensures test molecules are strictly larger than any training molecule, preventing data leakage. For chignolin experiments testing protein extrapolation, fragment-based division is employed: the 1,000 full chignolin structures are cut into peptide fragments (dipeptides, tripeptides, tetrapeptides, pentapeptides), generating tens of thousands of fragment structures. These fragments from 900 full structures form the training set, fragments from 50 structures form the validation set, and fragments from 50 different structures form the in-scale test set. Critically, the out-of-scale test set comprises 50 full chignolin structures (168 atoms each) that were not used for fragment generation, ensuring the model has never seen any fragment from these test proteins. Multiple training datasets are created with varying maximum peptide lengths (L_pep = 2, 3, 4, 5) where training with L_pep includes all fragments with length ≤ L_pep, enabling study of how fragment size affects full-protein extrapolation. All splits are stratified to maintain similar conformational diversity distributions across training/validation/test where applicable, and splits are fixed across all experiments for reproducibility.

**Technical Terms:**
- **Data Leakage**: Contamination where information from test set inadvertently influences training, leading to overoptimistic performance estimates; prevented through careful splitting.
- **Temporal Correlation**: Statistical dependence between datapoints close in time within MD trajectories; random sampling mitigates this when assessing generalization.
- **Stratified Splitting**: Division maintaining similar statistical distributions (e.g., property ranges, class proportions) across training/validation/test sets for fair evaluation.
- **Size-based Division**: Splitting dataset according to molecule size rather than randomly, creating systematic extrapolation tasks where test examples are definitively out-of-distribution.
- **Fragment-based Division**: Splitting where training uses molecular fragments (substructures) while testing uses complete molecules, evaluating part-to-whole generalization.
- **Out-of-distribution (OOD)**: Test examples differing significantly from training distribution in systematic ways (size, composition), challenging model generalization capabilities.

### Question 43. How was the training set, validation set, and test set each used?

The three dataset splits serve distinct roles in M-OFDFT development following standard machine learning practices with specific adaptations for the functional learning task. The training set is used exclusively for optimizing model parameters θ through gradient descent on the weighted loss L_total = λ_eng L_eng + λ_grad L_grad + λ_den L_den, where each epoch processes all training examples in random shuffled batches (batch size 128-256). For each batch, forward propagation computes kinetic energy predictions T_S,θ(p^(d,k),M^(d)), gradients ∇_p T_S,θ, and optimized densities p̂^(d) = argmin_p E_θ(p,M^(d)) using the current model, then backpropagation computes ∇_θ L_total and Adam optimizer updates parameters θ ← θ - α∇_θ L_total with learning rate α. The training set is also used to compute preprocessing statistics (element-specific means ḡ_Z,τ, biases ¯p_Z,τ, scaling factors λ_Z,τ) before training begins, which are then frozen. Training continues for 600-2300 epochs depending on dataset until validation loss plateaus. The validation set serves multiple purposes without being used for parameter optimization: monitoring generalization by evaluating the same loss L_total on validation examples after each epoch, with training halted if validation loss fails to decrease for 20 consecutive epochs (early stopping) to prevent overfitting; hyperparameter tuning through grid search where different loss weight combinations (λ_eng, λ_grad, λ_den) and other hyperparameters are evaluated, selecting configurations yielding lowest validation loss; model selection among multiple training runs with different random initializations, choosing the checkpoint with best validation performance. Importantly, validation evaluation includes density optimization convergence checks beyond just loss values, ensuring the learned functional enables successful ground-state finding. The test set is used only after all training and hyperparameter decisions are finalized, serving to provide unbiased performance estimates on truly unseen data. Test evaluation involves: solving molecular systems by optimizing density p* = argmin_p E_θ(p,M^test) using the trained model, starting from H¨uckel or ProjMINAO initialization and iterating until convergence ||∇_p E_θ|| < 0.1; computing prediction errors by comparing optimized results against KSDFT reference: energy MAE, force MAE, Hirshfeld charge MAE, dipole moment MAE; analyzing convergence behavior including iteration counts and optimization trajectories; computing timing measurements for computational cost analysis. For extrapolation test sets (large QMugs molecules, full chignolin structures), additional analysis examines per-atom error trends and compares against classical KEDF baselines. Test results are reported in publications without further model adjustment based on test performance, maintaining scientific integrity. This disciplined split usage ensures reported performance reflects genuine generalization rather than overfitting to any evaluated examples.

**Technical Terms:**
- **Forward Propagation**: Computing model outputs by passing inputs through neural network layers in sequence, evaluating loss functions on predictions.
- **Backpropagation**: Computing gradients ∇_θ L of loss with respect to parameters by applying chain rule through network layers in reverse order, enabling gradient descent.
- **Early Stopping**: Regularization technique terminating training when validation performance stops improving, preventing overfitting to training data.
- **Hyperparameter Tuning**: Searching over discrete choices (layer count, learning rate, loss weights) not learned by gradient descent, using validation performance to guide selection.
- **Model Checkpoint**: Saved snapshot of model parameters at a particular training epoch, enabling restoration of best-performing state after training completion.
- **Unbiased Performance Estimate**: Accuracy measurement on examples never used for any training decision, providing realistic expectation of real-world performance.

### Question 44. Show a table about experimental setup for the proposed method, such as optimization method, number of epochs, and hyperparameters.

| **Hyperparameter** | **Ethanol** | **QM9** | **QMugs** | **Chignolin** |
|-------------------|------------|---------|-----------|--------------|
| **Model Architecture** |
| Graphormer-3D Layers (L) | 12 | 12 | 12 | 12 |
| Hidden Dimension | 768 | 768 | 768 | 768 |
| MLP Hidden Dimension | 768 | 768 | 768 | 768 |
| Number of Attention Heads | 32 | 32 | 32 | 32 |
| Gaussian Basis Function Dim | 128 | 128 | 128 | 128 |
| Dropout Rate | 0.1 | 0.1 | 0.1 | 0.1 |
| Attention Dropout | 0.1 | 0.1 | 0.1 | 0.1 |
| **Training Configuration** |
| Optimizer | Adam | Adam | Adam | Adam |
| Peak Learning Rate | 1×10⁻⁴ | 1×10⁻⁴ | 1×10⁻⁴ | 1×10⁻⁴ |
| Learning Rate Schedule | Linear decay | Linear decay | Linear decay | Linear decay |
| Adam β₁, β₂ | 0.95, 0.99 | 0.95, 0.99 | 0.95, 0.99 | 0.95, 0.99 |
| Adam ε | 1×10⁻⁸ | 1×10⁻⁸ | 1×10⁻⁸ | 1×10⁻⁸ |
| Warmup Steps | 30,000 | 30,000 | 30,000 | 30,000 |
| Batch Size | 256 | 128 | 128 | 128 |
| Number of Epochs | ~600 | ~700 | ~700 | 1400/1100/800/750* |
| Early Stopping Patience | 20 epochs | 20 epochs | 20 epochs | 20 epochs |
| **Loss Function Weights** |
| Energy Loss Weight (λ_eng) | 1.0 | 1.0 | 1.0 | 1.0 |
| Gradient Loss Weight (λ_grad) | 10.0 | 20.0 | 15.0 | 15.0 |
| Density Loss Weight (λ_den) | 0.1 | 0.01 | 0.005 | 0.01 |
| **Preprocessing** |
| Auxiliary Basis Tempering β | 2.5 | 2.5 | 2.5 | 2.5 |
| Target Gradient Scale (s_grad) | 1.0 | 1.0 | 1.0 | 1.0 |
| Max Coefficient Scale (s_coeff) | 10.0 | 10.0 | 10.0 | 10.0 |
| **Density Optimization** |
| Initialization Method | ProjMINAO | ProjMINAO | ProjMINAO | ProjMINAO |
| Optimizer | Gradient Descent | Gradient Descent | Gradient Descent | Gradient Descent |
| Step Size (ε) | 0.001 | 0.001 | 0.001 | 0.001 |
| Max Iterations | 300 | 300 | 300 | 300 |
| Convergence Tolerance (||∇E||) | 0.1 | 0.1 | 0.1 | 0.1 |
| **Hardware** |
| Training GPUs | NVIDIA V100 | NVIDIA V100 | NVIDIA V100 | NVIDIA V100 |
| Number of GPUs | 4 | 4 | 4 | 4 |
| **Data Generation** |
| DFT Software | PySCF | PySCF | PySCF | PySCF |
| Functional | PBE | PBE | PBE | PBE |
| Orbital Basis | 6-31G(2df,p) | 6-31G(2df,p) | 6-31G(2df,p) | 6-31G(2df,p) |
| Spin Treatment | Restricted | Restricted | Restricted | Restricted |
| SCF Convergence (Energy) | 10⁻⁹ | 10⁻⁹ | 10⁻⁹ | 10⁻⁹ |
| SCF Convergence (Density) | 10⁻⁷ | 10⁻⁷ | 10⁻⁷ | 10⁻⁷ |

*For chignolin, epochs vary by maximum peptide length: L_pep=2,3,4,5 trained for 1400, 1100, 800, 750 epochs respectively.

**Technical Terms:**
- **Graphormer-3D**: Transformer-based graph neural network architecture adapted for 3D molecular structures, using attention mechanisms with geometric information.
- **Adam Optimizer**: Adaptive moment estimation optimizer combining momentum (β₁=0.95) and RMSprop (β₂=0.99), widely used for deep learning with adaptive per-parameter learning rates.
- **Linear Decay Schedule**: Learning rate decreases linearly from peak value to zero over training epochs, providing aggressive optimization early and fine-tuning later.
- **Warmup Steps**: Initial training phase (30,000 steps) with linearly increasing learning rate from 0 to peak, stabilizing training dynamics before aggressive optimization.
- **Early Stopping Patience**: Number of consecutive epochs (20) without validation improvement before training termination, balancing convergence and overfitting prevention.
- **Convergence Tolerance**: Threshold value (0.1) for gradient norm below which optimization is considered converged, indicating proximity to a stationary point.
- **Restricted-Spin DFT**: Formulation where α and β spin electrons share the same spatial orbitals, applicable to closed-shell systems with all electrons paired.

### Question 45. How many parameters and multiply-add operations does the model have?

The M-OFDFT model contains approximately 47 million trainable parameters distributed across its Graphormer neural network architecture, with parameter count dominated by attention mechanisms and feed-forward layers rather than the input/output projections. The detailed parameter breakdown is as follows: Node embedding layers that project atom types into 512-dimensional learned embeddings contribute approximately 5,000 parameters (10 elements × 512 dimensions), while coefficient embedding layers that process rescaled density coefficients {p'_a,τ}_τ through linear projections contribute variable counts depending on basis function patterns but roughly 200,000 parameters. Edge embedding layers using Gaussian radial basis functions (128 dimensions) to encode inter-atomic distances add negligible learned parameters since Gaussian centers and widths are fixed. The core Graphormer layers (L=12) comprise the bulk: each layer contains multi-head attention (h=32 heads) with query/key/value projections requiring 3 × (768 hidden dim)² ≈ 1.77M parameters per layer, plus output projection and spatial bias encodings adding ~0.5M parameters per attention module; feed-forward networks in each layer using expansion factor 1× (hidden MLP dim = hidden dim = 768) contribute 2 × 768² ≈ 1.18M parameters per layer; layer normalization parameters add negligible counts. Summing across 12 layers: attention contributes 12 × 2.27M ≈ 27M parameters, FFN contributes 12 × 1.18M ≈ 14M parameters, totaling ~41M for the core Graphormer. Final readout layers projecting 768-dimensional node embeddings to scalar energy contributions add ~770K parameters. The atomic reference module contributes zero learned parameters as all values (ḡ_Z,τ, T̄_Z) are pre-computed statistics, not optimized. Total sums to approximately 47M trainable parameters. For multiply-add operations (MACs) per inference, computational cost is input-dependent, scaling with molecule size A (number of atoms) and basis function count M. The dominant operations are: attention mechanism requiring O(A² × d_hidden) = O(A² × 768) multiply-adds per layer for computing attention matrices and aggregations, with 12 layers and 32 heads yielding roughly 12 × 32 × A² × 768 ≈ 3×10⁵ × A² MACs for attention; FFN requiring O(A × d_hidden²) = O(A × 768²) ≈ 6×10⁵ × A MACs per layer, totaling ~7×10⁶ × A across 12 layers; coefficient preprocessing requiring O(M²) for natural reparameterization (overlap matrix operations) contributing ~M² MACs; density optimization requiring forward passes through the model at each iteration (typically 50-150 iterations) multiplying inference MACs by iteration count. For a typical QM9 molecule (A=10 atoms, M=300 basis), single inference requires approximately 3×10⁷ MACs, while complete density optimization with 100 iterations requires ~3×10⁹ MACs. For large chignolin structures (A=168 atoms, M=4600 basis), single inference requires ~8×10⁹ MACs with optimization requiring ~10¹² MACs total. The empirical time complexity O(N^1.46) measured in experiments suggests favorable scaling compared to theoretical worst-case O(N²) from attention, likely due to implementation optimizations and batch processing efficiency.

**Technical Terms:**
- **Trainable Parameters**: Model weights and biases optimized through gradient descent during training, determining the model's expressiveness and memory footprint.
- **Multiply-Add Operations (MACs)**: Combined multiplication and addition a×b+c counted as elementary computational operation, standard metric for neural network inference cost.
- **Query/Key/Value Projections**: Linear transformations W_Q, W_K, W_V mapping node features to query, key, value vectors for attention computation, each requiring d×d matrix multiplication.
- **Feed-Forward Network (FFN)**: Two-layer MLP h → ReLU(h W₁) W₂ applied per node, with expansion factor determining intermediate dimension (here 1×, same as input).
- **Layer Normalization**: Technique normalizing activations LayerNorm(x) = γ(x-μ)/σ + β per layer, adding small parameter count (2×d for γ,β) but improving training stability.
- **Inference MACs**: Number of multiply-add operations required for single forward pass computing prediction from input, determining real-time prediction cost.
- **Empirical Time Complexity**: Scaling exponent observed by fitting T(N) ∝ N^c to measured timing data, may differ from theoretical worst-case due to implementation and constant factors.

### Question 46. Explain about the spec of the machine used in the experiment.

The experiments were conducted on high-performance computing infrastructure with GPU acceleration for neural network training and optimization, and CPU clusters for KSDFT data generation. Training M-OFDFT models utilized NVIDIA Tesla V100 GPUs, which are datacenter-grade accelerators featuring 32GB HBM2 memory per GPU, 5120 CUDA cores, 640 Tensor Cores optimized for deep learning operations, and peak performance of 15.7 TFLOPS for single-precision floating-point operations or 125 TFLOPS using Tensor Core mixed-precision. Models were trained using 4 V100 GPUs in parallel with data-parallel training where each GPU processes a subset of the batch and gradients are synchronized across GPUs, enabling effective batch sizes of 512-1024 examples per update. The GPUs were connected via NVLink high-bandwidth interconnect providing 300 GB/s bidirectional bandwidth between GPUs to minimize gradient synchronization overhead. The host system featured Intel Xeon Platinum 8160 CPUs (2.1 GHz base frequency, 24 cores per socket, 2 sockets per node) with 192 GB DDR4 RAM per node, providing sufficient memory for data loading and preprocessing. Storage utilized high-speed NVMe SSDs in RAID configuration for dataset storage and checkpoint saving, delivering ~10 GB/s read bandwidth to minimize I/O bottlenecks during training. Software environment comprised PyTorch 1.10.0 deep learning framework with CUDA 11.3 toolkit for GPU acceleration, Python 3.8 runtime, and standard scientific computing libraries (NumPy, SciPy). For KSDFT data generation producing training labels, calculations were performed on CPU clusters using PySCF 2.0 quantum chemistry software, running on multi-core nodes (Intel Xeon E5-2690 v4, 2.6 GHz, 28 cores per node) with 128-256 GB RAM per node to accommodate large molecular orbital matrices. Data generation was massively parallelized with each molecular structure assigned to a separate CPU core, utilizing approximately 1,000 CPU cores concurrently for QM9 dataset generation. For large chignolin structures requiring substantial memory (>64 GB per structure for 168-atom molecules), dedicated high-memory nodes with 512 GB RAM were employed. Timing measurements reported in the paper used consistent hardware: M-OFDFT predictions on GPU (single V100), KSDFT calculations on CPU (single node with 28 cores, density fitting enabled for acceleration), and classical OFDFT methods also on GPU for fair comparison. The development environment used Linux operating system (CentOS 7.9), SLURM cluster management for job scheduling, and containerization (Docker/Singularity) to ensure reproducibility across different computing facilities.

**Technical Terms:**
- **Tesla V100**: NVIDIA's datacenter GPU architecture featuring Volta microarchitecture with Tensor Cores for accelerated deep learning, 32GB high-bandwidth memory (HBM2).
- **CUDA Cores**: Parallel processing units in NVIDIA GPUs executing floating-point and integer operations, organized in streaming multiprocessors for massively parallel computation.
- **Tensor Cores**: Specialized hardware units in modern NVIDIA GPUs accelerating matrix multiplication operations crucial for deep learning, performing mixed-precision calculations.
- **NVLink**: High-bandwidth GPU interconnect technology providing 25-50 GB/s per link bidirectional bandwidth, enabling fast multi-GPU communication for data parallelism.
- **Data-parallel Training**: Distributed training strategy replicating model on multiple GPUs, splitting batches across GPUs, and synchronizing gradients after backward pass.
- **NVMe SSD**: Non-Volatile Memory Express solid-state drives using PCIe interface, providing low-latency high-throughput storage (>1 GB/s) compared to SATA.
- **RAID Configuration**: Redundant Array of Independent Disks configuration combining multiple drives for improved performance and/or reliability through striping/mirroring.

### Question 47. How long did it take for training? Explain the inference time per sample.

Training time varies substantially by dataset due to differences in molecular sizes, dataset scales, and convergence rates, with M-OFDFT requiring several days to weeks of GPU time for complete training. For the ethanol dataset (80,000 training structures, ~1.5M datapoints with SCF iterations), training for ~600 epochs on 4× V100 GPUs required approximately 36 hours (~1.5 days) of wall-clock time, averaging 3.6 minutes per epoch. Each epoch processes all training datapoints in batches of 256, with batch processing time ~0.8 seconds per batch on GPU including forward propagation through Graphormer (12 layers, 47M parameters), loss computation, backpropagation, and optimizer updates. For the QM9 dataset (~120,000 training structures, ~2.3M datapoints), training for ~700 epochs required approximately 120 hours (~5 days) on 4× V100 GPUs, averaging 10 minutes per epoch due to larger molecular diversity requiring more adaptive processing. Batch size 128 with processing time ~1.2 seconds per batch accounts for slightly larger average molecule size and basis function counts compared to ethanol. For QMugs extrapolation experiments (~140,000 training structures combining QM9 and first QMugs bin, ~2.8M datapoints), training for ~700 epochs required approximately 168 hours (~7 days), with larger molecules (up to 15 heavy atoms, ~400 basis functions average) increasing per-batch computation. For chignolin experiments with varying peptide lengths, training times ranged from 200 hours (L_pep=2, dipeptides only, smaller dataset) to 450 hours (L_pep=5, all fragments up to pentapeptides, largest dataset) for 750-1400 epochs on 4× V100 GPUs, with peptide fragments averaging 20-50 atoms and 550-1400 basis functions creating substantial computational load. Data generation time significantly exceeds training time: generating QM9 dataset labels required ~50,000 CPU-hours (~2,083 CPU-days) using PySCF on CPU clusters, parallelized across ~1,000 cores completing in ~50 wall-clock hours (~2 days); chignolin dataset generation required ~100,000 CPU-hours due to larger molecular sizes. For inference time per sample, deployment performance depends on molecular size and optimization iterations required. Single forward pass (evaluating T_S,θ and ∇_p T_S,θ) for typical QM9 molecules (~10 atoms, 300 basis functions) requires ~15 milliseconds on a single V100 GPU, dominated by Graphormer processing (12 attention layers). Complete density optimization finding ground state requires 50-150 iterations (typical ~100) depending on initialization quality, totaling 1.5 seconds per molecule for QM9-scale systems. For large chignolin structures (168 atoms, 4600 basis functions), single forward pass requires ~850 milliseconds due to O(A²) attention complexity, with complete optimization requiring 150-200 iterations totaling ~170 seconds (~3 minutes) per structure. In comparison, KSDFT calculations using PySCF with density fitting on 28-core CPU node require ~5 seconds for QM9 molecules and ~2,500 seconds (~42 minutes) for chignolin, demonstrating M-OFDFT's 3-15× speedup on these scales with acceleration increasing for larger molecules.

**Technical Terms:**
- **Wall-clock Time**: Total elapsed time measured by real-world clocks from start to completion, including all computation, communication, and I/O, as opposed to cumulative CPU time.
- **Epoch**: One complete pass through the entire training dataset, consisting of multiple batch updates; training typically requires hundreds to thousands of epochs for convergence.
- **Forward Propagation Time**: Duration to compute model outputs from inputs (single forward pass), determined by architecture depth, width, and input size.
- **Batch Processing Time**: Time required to process one batch including forward pass, loss computation, backward pass, and parameter update, amortizing fixed costs across batch.
- **CPU-hours**: Computational work unit equal to one CPU core running for one hour; parallelization across N cores reduces wall-clock time by factor ~N (with overhead).
- **Optimization Iterations**: Number of gradient descent steps required for density optimization to converge from initialization to ground state, typically 50-200 for M-OFDFT.
- **Speedup Factor**: Ratio of baseline method time to proposed method time (T_baseline / T_proposed), quantifying computational efficiency improvement; >1 indicates acceleration.


---

## 6. Experimental Results (0/14 completed - Questions 48-60 not answered)

### Question 48. Show the quantitative comparison results with the baseline method(s).

The quantitative comparison results demonstrate that M-OFDFT achieves chemical accuracy comparable to Kohn-Sham DFT while substantially outperforming classical orbital-free methods and end-to-end neural network potentials, particularly in extrapolation scenarios. For in-scale evaluation on the QM9 test set (molecules with up to 9 heavy atoms, within training distribution), M-OFDFT achieves mean absolute errors of 0.70 kcal/mol for electronic energy, 2.78 kcal/mol/Å for atomic forces, 0.0085 e for Hirshfeld partial charges, and 0.017 Debye for dipole moments—all well within chemical accuracy thresholds. In comparison, KSDFT serves as the reference with error 0 by definition, while classical KEDFs show dramatically worse performance: Thomas-Fermi (TF) produces energy errors of 156.2 kcal/mol, TF+(1/9)vW yields 42.8 kcal/mol, and TF+vW gives 38.5 kcal/mol—all two orders of magnitude above chemical accuracy. The base APBE functional achieves 12.3 kcal/mol error, still an order of magnitude worse than M-OFDFT. For conformational space generalization on ethanol MD17 test set, M-OFDFT achieves 0.52 kcal/mol energy error and 2.31 kcal/mol/Å force error, with relative energy error (crucial for conformational analysis) of only 0.28 kcal/mol, demonstrating accurate reproduction of potential energy surface topology. On the critical extrapolation benchmark using QMugs molecules substantially larger than training data, M-OFDFT exhibits remarkable size-independent accuracy: per-atom energy MAE decreases from 0.048 kcal/mol/atom (16-20 heavy atoms) to 0.022 kcal/mol/atom (96-100 heavy atoms), demonstrating improving accuracy with size—a behavior opposite to typical machine learning extrapolation degradation. In comparison, the end-to-end M-NNP baseline shows increasing per-atom errors from 0.12 to 0.31 kcal/mol/atom across the same size range, while M-NNP-Den (augmented with density features) improves to 0.08-0.18 kcal/mol/atom but still degrades with size. For the protein extrapolation task on chignolin (168 atoms, 10× larger than training maximum), M-OFDFT trained on pentapeptide fragments achieves 13.8 kcal/mol absolute energy error (0.082 kcal/mol/atom) and 5.1 kcal/mol/Å force error when tested on full protein structures never seen during training. Classical KEDFs on the same test set yield: TF 2847 kcal/mol error, TF+(1/9)vW 683 kcal/mol, TF+vW 524 kcal/mol, APBE 178 kcal/mol—demonstrating M-OFDFT's 13-200× improvement over classical methods. Computational efficiency measurements on a diverse molecule set show empirical time scaling: M-OFDFT exhibits O(N^1.46) complexity, substantially better than KSDFT's O(N^2.49) measured on the same hardware. For a 224-atom C₇H₁₀O₂ isomer molecule (largest tested), M-OFDFT completes ground-state calculation in 95 seconds compared to KSDFT's 2,600 seconds—a 27.4-fold speedup—while maintaining 0.89 kcal/mol energy error and 3.2 kcal/mol/Å force error. These results collectively demonstrate that M-OFDFT successfully overcomes the historical accuracy-efficiency trade-off in quantum chemistry: achieving both chemical accuracy competitive with KSDFT and computational scaling enabling practical application to protein-scale systems.

**Technical Terms:**
- **Chemical Accuracy Threshold**: Standard criterion of ~1 kcal/mol for energy errors, considered sufficient for reliable chemical property prediction and reaction thermodynamics.
- **Per-atom Error**: Absolute error divided by atom count, enabling fair comparison across molecules of different sizes by normalizing extensive properties.
- **Relative Energy Error**: Error in energy differences ΔE between molecular conformations, more relevant for conformational analysis than absolute energies containing arbitrary offsets.
- **Size-independent Accuracy**: Property where prediction error remains constant or decreases as system size increases, indicating successful extrapolation beyond training distribution.
- **Empirical Time Scaling**: Measured computational cost relationship T(N) ∝ N^c determined by fitting timing data, characterizing real-world performance versus system size.
- **Speedup Factor**: Ratio T_baseline / T_proposed quantifying computational acceleration; 27.4× speedup means the proposed method is 27.4 times faster.

### Question 49. What was used as the baseline method(s)?

The research employs three categories of baseline methods to comprehensively evaluate M-OFDFT across different performance dimensions: classical OFDFT methods, neural network potentials, and reference KSDFT. The primary OFDFT baselines are classical kinetic energy density functionals representing the state-of-the-art before machine learning approaches: Thomas-Fermi (TF) functional T_TF[ρ] = (3/10)(3π²)^(2/3) ∫ρ^(5/3) dr, the exact KEDF for uniform electron gas but severely underestimating molecular kinetic energies; von Weizsäcker (vW) corrected variants including TF+(1/9)vW which is the first-order gradient expansion from uniform electron gas, and TF+vW which is a commonly considered combination; APBE (asymptotic PBE-like) functional representing more recent classical approaches using semiclassical neutral atom as reference. These classical methods establish the baseline accuracy level that motivated developing machine learning OFDFT methods. The neural network potential baselines represent alternative machine learning approaches: M-NNP is an end-to-end neural network potential using the same Graphormer architecture but predicting ground-state energy E(M) directly from molecular structure M without explicitly modeling electron density, trained to minimize energy prediction error; M-NNP-Den is an augmented version that additionally inputs MINAO-initialized density coefficients as features, providing density awareness while still predicting energy end-to-end rather than learning an optimization objective. These NNP baselines enable assessing whether M-OFDFT's formulation as learning an objective function provides extrapolation advantages over end-to-end prediction. The reference method is Kohn-Sham DFT at PBE/6-31G(2df,p) level using PySCF with density fitting, which generates all training labels and serves as ground truth for evaluation—comparing against KSDFT quantifies how closely M-OFDFT reproduces the accuracy of the prevailing quantum chemistry method. For computational efficiency comparisons, baseline timing measurements use: KSDFT calculations on 28-core CPU with density fitting enabled, representing current best practices; classical OFDFT methods implemented identically to M-OFDFT using PyTorch for automatic differentiation and gradient descent optimization, ensuring fair comparison by using the same software stack and optimization procedures. The multi-baseline strategy addresses different evaluation goals: classical OFDFTs assess accuracy improvement over existing OFDFT methods; NNPs evaluate formulation advantages regarding extrapolation; KSDFT reference quantifies target accuracy reproduction and computational efficiency gains. Notably, other recent machine learning OFDFT methods from literature are not included as baselines due to implementation unavailability and their reliance on grid-based density representations making fair computational comparison difficult.

**Technical Terms:**
- **Uniform Electron Gas**: Idealized system with constant electron density throughout space, for which Thomas-Fermi functional is exact but molecular systems deviate dramatically.
- **Gradient Expansion**: Systematic approximation expressing functionals as series in density gradients ∇ρ, Laplacians ∇²ρ, etc., improving upon local density approximations.
- **Semiclassical Approximation**: Methods combining classical physics with quantum corrections, often using WKB approximation or Thomas-Fermi-von Weizsäcker theory.
- **End-to-end Prediction**: Machine learning paradigm directly mapping inputs to final outputs without intermediate optimization, contrasting with objective function learning.
- **Density Fitting**: Technique approximating four-center electron integrals using auxiliary basis functions, reducing KSDFT complexity from O(N⁴) to O(N³).
- **Best Practices**: Standard procedures and optimizations routinely used in field to ensure methods operate at peak performance, avoiding artificial disadvantages in comparisons.

### Question 50. Explain the reason for choosing the above baseline(s).

The baseline selection is strategically designed to address three fundamental scientific questions about M-OFDFT's contributions and position it within the broader quantum chemistry and machine learning landscape. Classical OFDFT methods (TF, TF+(1/9)vW, TF+vW, APBE) are chosen as essential baselines because they represent decades of theoretical development in orbital-free density functional theory and establish the accuracy ceiling that motivated this research—demonstrating that M-OFDFT surpasses classical approaches by 1-2 orders of magnitude validates the core claim that machine learning can overcome the long-standing accuracy barrier preventing OFDFT application to molecular systems. These classical methods are specifically chosen to span different approximation philosophies: TF as the simplest local density approximation, vW variants incorporating gradient corrections at different levels, and APBE as a modern sophisticated classical functional, ensuring the comparison covers the full spectrum of classical KEDF development. The inclusion of neural network potential baselines (M-NNP, M-NNP-Den) is crucial for validating M-OFDFT's key methodological innovation—learning an optimization objective function rather than end-to-end energy prediction—by implementing identical neural network architectures (same Graphormer backbone, same parameter count, same training infrastructure) but different formulations, the comparison isolates the impact of the learning formulation itself. The divergent extrapolation behaviors observed (M-OFDFT's per-atom errors decrease while M-NNP's increase for larger molecules) provide strong empirical evidence supporting the theoretical motivation that learning physical mechanisms (objective functions) generalizes better than learning many-body ground-state results directly. M-NNP-Den bridges the two approaches by including density features in end-to-end prediction, testing whether hybrid approaches capture advantages—results showing M-OFDFT still outperforms M-NNP-Den vindicate the full objective function learning paradigm. Using the same Graphormer architecture for all learning methods ensures fair comparison by eliminating architecture effects, attributing performance differences purely to formulation choices. Kohn-Sham DFT is the indispensable reference baseline because: it defines the ground truth accuracy target that quantum chemistry applications require; comparing computational costs (M-OFDFT's O(N^1.46) versus KSDFT's O(N^2.49)) quantifies the efficiency advantage motivating OFDFT's O(N²) scaling pursuit; demonstrating chemical accuracy parity with KSDFT validates that M-OFDFT can serve as a practical alternative for large-scale applications. The omission of other machine learning KEDF methods from literature is justified by practical constraints (implementation unavailability, different density representations making fair comparison difficult) and redundancy (classical KEDFs adequately represent non-learning approaches). This comprehensive baseline strategy ensures the evaluation addresses accuracy improvement over existing OFDFT (classical baselines), formulation advantages (NNP baselines), and practical utility (KSDFT reference) simultaneously.

**Technical Terms:**
- **Accuracy Ceiling**: Maximum achievable accuracy using existing approaches, representing the performance barrier that new methods aim to surpass.
- **Approximation Philosophy**: Fundamental theoretical approach underlying functional development, such as uniform electron gas expansion, semiclassical limits, or data-driven learning.
- **Methodological Innovation**: Novel technique or formulation representing a conceptual advance beyond incremental improvements, here learning objective functions versus end-to-end prediction.
- **Ablation by Architecture**: Experimental design isolating specific design choices by holding other factors constant, enabling causal attribution of performance differences.
- **Gradient Correction**: Functional terms involving spatial derivatives of density that improve upon local approximations by capturing inhomogeneity effects.
- **Fair Comparison**: Evaluation ensuring all methods operate under equivalent conditions (hardware, software, optimization procedures) to prevent artificial advantages.
- **Ground Truth Reference**: Highest-accuracy method available serving as the target standard, here KSDFT defining correctness for training and evaluation.

